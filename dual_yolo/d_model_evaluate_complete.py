"""
åŒæ¨¡æ€YOLOæ¨¡å‹è¯„ä¼°è„šæœ¬
ä½¿ç”¨åŒ»ç”Ÿæ ‡æ³¨çš„JSONæ•°æ®è¿›è¡Œç²¾åº¦è¯„ä¼°ï¼Œè®¡ç®—IoUå’Œé«˜åº¦å·®å¼‚æŒ‡æ ‡
"""

import torch
import sys
import os
import json
import numpy as np
import cv2
import matplotlib.pyplot as plt
from pathlib import Path
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
from ultralytics import YOLO
import json

# å¯¼å…¥æ•°æ®å¢å¼ºç­–ç•¥é…ç½®
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'dual_dataset'))
from d_dataset_config import DatasetConfig

# è·å–å¢å¼ºå‚æ•°æ˜ å°„è¡¨
_config = DatasetConfig()
AUGMENTATION_STRATEGIES = _config.strategies

def get_augmentation_params(filename):
    """æ ¹æ®æ–‡ä»¶åè·å–å¢å¼ºå‚æ•°"""
    suffix = filename.split('_')[-1].replace('.npy', '')
    return AUGMENTATION_STRATEGIES.get(suffix, {})


def apply_rotation_to_points(points, angle, img_size=(1504, 1504)):
    """å¯¹ç‚¹ä½åº”ç”¨æ—‹è½¬å˜æ¢"""
    if angle == 0:
        return points
    
    center = (img_size[0] // 2, img_size[1] // 2)
    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)
    
    # è½¬æ¢ä¸ºé½æ¬¡åæ ‡
    points_array = np.array(points, dtype=np.float32)
    ones = np.ones((points_array.shape[0], 1), dtype=np.float32)
    homogeneous_points = np.hstack([points_array, ones])
    
    # åº”ç”¨æ—‹è½¬
    rotated_homogeneous = rotation_matrix.dot(homogeneous_points.T).T
    
    # è£å‰ªåˆ°å›¾åƒè¾¹ç•Œ
    rotated_points = []
    for pt in rotated_homogeneous:
        x = max(0, min(img_size[0] - 1, pt[0]))
        y = max(0, min(img_size[1] - 1, pt[1]))
        rotated_points.append([x, y])
    
    return np.array(rotated_points, dtype=np.float32)  # ä¿æŒæµ®ç‚¹ç²¾åº¦


def calculate_iou(mask1, mask2):
    """è®¡ç®—IoU"""
    m1, m2 = mask1 > 0, mask2 > 0
    intersection = np.logical_and(m1, m2).sum()
    union = np.logical_or(m1, m2).sum()
    return intersection / union if union > 0 else 1.0


def sort_points_by_angle(points):
    """æŒ‰æè§’æ’åºç‚¹ä½"""
    center = np.mean(points, axis=0)
    angles = np.arctan2(points[:, 1] - center[1], points[:, 0] - center[0])
    return points[np.argsort(angles)]


def find_json_annotation(npy_filename, json_dirs=['./data/rawdata/class1/', './data/rawdata/class2/']):
    """æŸ¥æ‰¾JSONæ ‡æ³¨æ–‡ä»¶"""
    parts = npy_filename.split('_')
    if len(parts) < 4:
        return None
    
    json_prefix = '_'.join(parts[:3]) + '_'
    
    for class_dir in json_dirs:
        if not os.path.exists(class_dir):
            continue
        for json_filename in os.listdir(class_dir):
            if json_filename.endswith('.json') and json_filename.startswith(json_prefix.replace('_', '_', 2)):
                try:
                    with open(class_dir + json_filename, 'r') as f:
                        return json.load(f)
                except Exception:
                    continue
    return None


def extract_annotation_points(json_data):
    """æå–åŒ»ç”Ÿæ ‡æ³¨ç‚¹ä½ - ä¿æŒå‘åå…¼å®¹ï¼Œé»˜è®¤æå–Class 1"""
    return extract_annotation_points_multiclass(json_data, class_id=1)


def extract_annotation_points_multiclass(json_data, class_id):
    """æå–æŒ‡å®šç±»åˆ«çš„åŒ»ç”Ÿæ ‡æ³¨ç‚¹ä½
    
    Args:
        json_data: JSONæ ‡æ³¨æ•°æ®
        class_id: ç±»åˆ«ID (0=è¡€æ¸…å±‚, 1=ç™½è†œå±‚, 2=è¡€æµ†å±‚)
    
    Returns:
        np.array: æ’åºåçš„æ ‡æ³¨ç‚¹ä½ï¼Œå¦‚æœç‚¹æ•°ä¸è¶³è¿”å›None
    """
    true_points = []
    shapes = json_data.get("shapes", [])
    
    if class_id == 0:
        # Class 0 (è¡€æ¸…å±‚): ä½¿ç”¨ç‚¹ä½ 0,1,2,3
        point_indices = [0, 1, 2, 3]
        min_points = 4
    elif class_id == 1:
        # Class 1 (ç™½è†œå±‚): ä½¿ç”¨ç‚¹ä½ 2,3,4,5 - ä¿æŒç°æœ‰é€»è¾‘
        point_indices = [2, 3, 4, 5]
        min_points = 4
    elif class_id == 2:
        # Class 2 (è¡€æµ†å±‚): ä½¿ç”¨ç‚¹ä½ 4,5,6
        point_indices = [4, 5, 6]
        min_points = 3
    else:
        return None
    
    # æå–æŒ‡å®šç´¢å¼•çš„ç‚¹ä½
    for idx in point_indices:
        if idx < len(shapes):
            x, y = shapes[idx]["points"][0]
            # åº”ç”¨åæ ‡å˜æ¢
            x, y = int((x - 800) * 1504/1216), int(y - 250)
            true_points.append([x, y])
    
    # æ£€æŸ¥ç‚¹æ•°æ˜¯å¦ä¸¥æ ¼åŒ¹é…
    if len(true_points) == min_points:
        return sort_points_by_angle(np.array(true_points, dtype=np.int32))
    else:
        return None


def visualize_results(annotated_image, all_class_data, save_path=None):
    """å¯è§†åŒ–å¤šç±»åˆ«ç»“æœ
    
    Args:
        annotated_image: æ ‡æ³¨å›¾åƒ
        all_class_data: æ‰€æœ‰ç±»åˆ«æ•°æ®å­—å…¸ {class_id: {'pred_points': points, 'true_points': points}}
        save_path: ä¿å­˜è·¯å¾„
    """
    # ç¡®ä¿å›¾åƒæ ¼å¼æ­£ç¡®
    if not isinstance(annotated_image, np.ndarray):
        annotated_image = np.array(annotated_image)
    if annotated_image.dtype != np.uint8:
        annotated_image = annotated_image.astype(np.uint8)
    annotated_image = np.ascontiguousarray(annotated_image)
    
    # é¢„æµ‹ç‚¹é¢œè‰²æ–¹æ¡ˆ (BGRæ ¼å¼)
    pred_colors = {
        0: (0, 255, 255),    # Class 0: é»„è‰²åå­—
        1: (0, 255, 0),      # Class 1: ç»¿è‰²åå­— (ä¿æŒç°æœ‰)
        2: (255, 0, 0)       # Class 2: è“è‰²åå­—
    }
    
    # å…ˆç»˜åˆ¶æ‰€æœ‰çœŸå®æ ‡æ³¨ç‚¹ (çº¢è‰²åœ†ç‚¹)
    for class_id, class_data in all_class_data.items():
        true_points = class_data.get('true_points')
        if true_points is not None:
            for point in true_points:
                cv2.circle(annotated_image, tuple(map(int, point)), 5, (0, 0, 255), -1)
    
    # å†æŒ‰ç±»åˆ«ç»˜åˆ¶é¢„æµ‹ç»“æœ
    for class_id, class_data in all_class_data.items():
        pred_points = class_data.get('pred_points')
        if pred_points is not None:
            pred_color = pred_colors.get(class_id, (0, 255, 0))  # é»˜è®¤ç»¿è‰²
            
            # ç»˜åˆ¶é¢„æµ‹çº¿è¿æ¥ (ç™½è‰²ç»†çº¿)
            pred_line = np.array(pred_points, dtype=np.int32).reshape((-1, 1, 2))
            cv2.polylines(annotated_image, [pred_line], True, (255, 255, 255), 1, lineType=cv2.LINE_AA)
            
            # ç»˜åˆ¶é¢„æµ‹ç‚¹åå­—
            for point in pred_points:
                x, y = int(point[0]), int(point[1])
                cv2.line(annotated_image, (x-2, y), (x+2, y), pred_color, 2)
                cv2.line(annotated_image, (x, y-2), (x, y+2), pred_color, 2)
    
    # ä¿å­˜å›¾åƒ
    if save_path:
        rgb_image = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)
        plt.imsave(save_path, rgb_image)


def visualize_results_single(annotated_image, pred_points=None, true_points=None, save_path=None, class_id=1):
    """å•ç±»åˆ«å¯è§†åŒ–ç»“æœ - ä¿æŒå‘åå…¼å®¹"""
    all_class_data = {}
    if true_points is not None or pred_points is not None:
        all_class_data[class_id] = {
            'true_points': true_points,
            'pred_points': pred_points
        }
    visualize_results(annotated_image, all_class_data, save_path)


def evaluate_dual_yolo_model(fusion_name, debug=False, include_augmented=True, 
                            evaluate_classes=[1], conf_threshold=0.5):
    """ä¸»è¯„ä¼°å‡½æ•°
    
    Args:
        fusion_name: èåˆç­–ç•¥åç§°
        debug: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼
        include_augmented: æ˜¯å¦åŒ…å«å¢å¼ºæ•°æ®
        evaluate_classes: è¦è¯„ä¼°çš„ç±»åˆ«åˆ—è¡¨ï¼Œé»˜è®¤[1]ä¿æŒå‘åå…¼å®¹
        conf_threshold: YOLOæ¨ç†ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œé»˜è®¤0.5ï¼Œç›´æ¥åœ¨æ¨¡å‹æ¨ç†æ—¶åº”ç”¨è¿‡æ»¤
    """
    # é…ç½®å‚æ•°
    project_root = Path(__file__).parent.parent
    if fusion_name:
        model_yaml = project_root / 'dual_yolo' / 'models' / f'yolo11x-dseg-{fusion_name}.yaml'
        if fusion_name == 'crossattn-30epoch':
            model_yaml = project_root / 'dual_yolo' / 'models' / f'yolo11x-dseg-crossattn.yaml'
        model_pt = project_root / 'dual_yolo' / 'runs' / 'segment' / f'dual_modal_train_{fusion_name}' / 'weights' / 'best.pt'
    else:
        model_yaml = project_root / 'dual_yolo' / 'models' / f'yolo11x-seg.yaml'
        model_pt = project_root / 'dual_yolo' / 'weights' / 'yolo11x-seg-blue.pt'
    
    dataset_path = project_root / 'datasets' / 'Dual-Modal-1504-500-1-6ch'
    test_images = dataset_path / 'test' / 'images'
    
    eval_results_name = 'evaluation_results_aug' if include_augmented else 'evaluation_results'
    for class_id in evaluate_classes:
        eval_results_name += f'_{class_id}'

    eval_results_dir = project_root / 'dual_yolo' / eval_results_name / f'{fusion_name}'
    os.makedirs(eval_results_dir, exist_ok=True)
    
    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
    if debug:
        print(f"ğŸ” è°ƒè¯•ä¿¡æ¯ï¼š")
        print(f"  æ¨¡å‹é…ç½®æ–‡ä»¶: {model_yaml} ({'âœ…å­˜åœ¨' if model_yaml.exists() else 'âŒä¸å­˜åœ¨'})")
        print(f"  æ¨¡å‹æƒé‡æ–‡ä»¶: {model_pt} ({'âœ…å­˜åœ¨' if model_pt.exists() else 'âŒä¸å­˜åœ¨'})")
        print(f"  æµ‹è¯•å›¾åƒç›®å½•: {test_images} ({'âœ…å­˜åœ¨' if test_images.exists() else 'âŒä¸å­˜åœ¨'})")
    
    try:
        model = YOLO(model_yaml).load(model_pt)
        if debug:
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # è·å–æµ‹è¯•æ–‡ä»¶åˆ—è¡¨
    if include_augmented:
        # åŒ…å«æ‰€æœ‰å¢å¼ºæ•°æ® _0-8
        npy_files = sorted([f for f in os.listdir(test_images) 
                           if f.endswith('.npy') and f.split('_')[-1].replace('.npy', '') in AUGMENTATION_STRATEGIES])
        # print(f"è¯„ä¼°å›¾åƒæ•°é‡: {len(npy_files)} (åŒ…å«å¢å¼ºæ•°æ®)")
    else:
        # åªè¯„ä¼°åŸå§‹å›¾åƒ _0
        npy_files = sorted([f for f in os.listdir(test_images) 
                           if f.endswith('_0.npy')])
        # print(f"è¯„ä¼°å›¾åƒæ•°é‡: {len(npy_files)} (ä»…åŸå§‹æ•°æ®)")
    
    # è¯„ä¼°æŒ‡æ ‡ - åˆ†ç»„ç»Ÿè®¡
    metrics = {
        'original': {
            'iou_list': [],
            'height_upper_diff': [],
            'height_lower_diff': [],
            'height_upper_diff_percent': [],
            'height_lower_diff_percent': [],
            'detected_count': 0,
            'total_count': 0
        },
        'augmented': {
            'iou_list': [],
            'height_upper_diff': [],
            'height_lower_diff': [],
            'height_upper_diff_percent': [],
            'height_lower_diff_percent': [],
            'detected_count': 0,
            'total_count': 0
        }
    }
    
    # é€ä¸ªè¯„ä¼°å›¾åƒ
    for npy_file in tqdm(npy_files, desc="è¯„ä¼°è¿›åº¦"):
        # ç¡®å®šæ˜¯åŸå§‹æ•°æ®è¿˜æ˜¯å¢å¼ºæ•°æ®
        suffix = npy_file.split('_')[-1].replace('.npy', '')
        is_original = (suffix == '0')
        
        # Process for augmented group (always runs)
        metrics['augmented']['total_count'] += 1
        success_aug = process_single_image(npy_file, test_images, model, eval_results_dir, metrics, 'augmented', 
                                        evaluate_classes, conf_threshold)
        if success_aug:
            metrics['augmented']['detected_count'] += 1

        # Process for original group (only for _0 files)
        if is_original:
            metrics['original']['total_count'] += 1
            # We need to re-process for the 'original' metrics group, even if it's redundant,
            # to ensure metrics are stored in the correct dictionary key.
            success_orig = process_single_image(npy_file, test_images, model, eval_results_dir, metrics, 'original', 
                                         evaluate_classes, conf_threshold)
            if success_orig:
                metrics['original']['detected_count'] += 1
    
    # æ‰“å°å’Œä¿å­˜ç»“æœ
    print_evaluation_results(metrics, include_augmented, fusion_name)
    
    # ä¿å­˜metricsæ•°æ®åˆ°JSONæ–‡ä»¶
    save_metrics_to_file(metrics, eval_results_dir, fusion_name)
    
    if any(metrics[key]["iou_list"] for key in metrics):  # åªæœ‰åœ¨æœ‰æ£€æµ‹ç»“æœæ—¶æ‰ç”Ÿæˆå›¾è¡¨
        generate_evaluation_chart(metrics, eval_results_dir, fusion_name)
    
    print(f"\nè¯„ä¼°å®Œæˆï¼ç»“æœä¿å­˜åœ¨ {eval_results_dir}")


def process_single_image(npy_file, test_images_dir, model, results_dir, metrics, group_key, 
                        evaluate_classes=[1], conf_threshold=0.5):
    """å¤„ç†å•ä¸ªå›¾åƒè¯„ä¼°
    
    Args:
        evaluate_classes: è¦è¯„ä¼°çš„ç±»åˆ«åˆ—è¡¨
        conf_threshold: ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œç›´æ¥åœ¨æ¨¡å‹æ¨ç†æ—¶åº”ç”¨
    """
    try:
        # åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®
        dual_tensor = np.load(test_images_dir / npy_file)
        if dual_tensor.shape[-1] == 6:
            dual_tensor = dual_tensor.transpose(2, 0, 1)
        
        # å‡†å¤‡å¯è§†åŒ–å›¾åƒ, [0, 1, 2]åŸå§‹RGBé€šé“é¡ºåº, [2, 1, 0]BGRé€šé“é¡ºåº
        blue_channels = dual_tensor[:3, :, :][[2, 1, 0], :, :]  # BGRé€šé“é¡ºåº
        blue_image = blue_channels.transpose(1, 2, 0)
        annotated_image = np.clip(blue_image * 255 if blue_image.max() <= 1.0 else blue_image, 0, 255).astype(np.uint8)
        
        # å‡†å¤‡æ¨¡å‹è¾“å…¥
        model_tensor = dual_tensor / 255.0 if dual_tensor.max() > 1.0 else dual_tensor
        model_input = torch.from_numpy(model_tensor).unsqueeze(0).float()
        
        # è·å–å¢å¼ºå‚æ•°
        augmentation_params = get_augmentation_params(npy_file)
        rotation_angle = augmentation_params.get('rotation', 0)
        
        # è·å–æ ‡æ³¨æ•°æ®
        json_data = find_json_annotation(npy_file)
        if not json_data:
            return False
        
        # æå–åŸå§‹æ ‡æ³¨ç‚¹
        original_true_points = extract_annotation_points(json_data)
        if original_true_points is None:
            return False
        
        # ä¸ºå¯è§†åŒ–å‡†å¤‡æ—‹è½¬åçš„æ ‡æ³¨ç‚¹
        rotated_true_points = apply_rotation_to_points(original_true_points, rotation_angle)
        
        # æ¨¡å‹æ¨ç† - ç›´æ¥åœ¨æ¨ç†æ—¶è®¾ç½®ç½®ä¿¡åº¦é˜ˆå€¼
        device = "cuda:0" if torch.cuda.is_available() else "cpu"
        results = model(model_input, imgsz=1504,
                        device=device, verbose=False, conf=conf_threshold)
        
        # æ£€æŸ¥æ¨¡å‹è¾“å‡ºå¹¶æŒ‰ç±»åˆ«åˆ†ç»„ï¼ˆç½®ä¿¡åº¦å·²åœ¨æ¨ç†æ—¶è¿‡æ»¤ï¼‰
        detected_classes = {}
        if hasattr(results[0], 'boxes') and results[0].boxes is not None:
            if len(results[0].boxes) > 0:
                all_classes = results[0].boxes.cls.cpu().numpy()
                
                # æŒ‰ç±»åˆ«åˆ†ç»„æ£€æµ‹ç»“æœ
                for i, cls_id in enumerate(all_classes):
                    if int(cls_id) in evaluate_classes:
                        if int(cls_id) not in detected_classes:
                            detected_classes[int(cls_id)] = []
                        detected_classes[int(cls_id)].append(i)
        
        base_filename = npy_file.replace('.npy', '')
        # --- æ–°çš„ç²¾ç¡®æ£€æµ‹è®¡æ•°é€»è¾‘ ---
        is_successful_detection = True
        # æ£€æŸ¥æ¯ä¸ªæœŸæœ›çš„ç±»åˆ«æ˜¯å¦éƒ½åªè¢«æ£€æµ‹åˆ°äº†ä¸€æ¬¡
        for class_id_to_check in evaluate_classes:
            if detected_classes.get(class_id_to_check) is None or len(detected_classes[class_id_to_check]) != 1:
                is_successful_detection = False
                break # åªè¦æœ‰ä¸€ä¸ªç±»åˆ«ä¸æ»¡è¶³æ¡ä»¶ï¼Œå°±åˆ¤å®šä¸ºå¤±è´¥

        # --- å¯è§†åŒ–å’ŒæŒ‡æ ‡è®¡ç®—é€»è¾‘ ---
        all_class_data = {}
        
        # æ­¥éª¤1: å§‹ç»ˆå‡†å¤‡å¯è§†åŒ–æ•°æ®ï¼Œæ— è®ºæ£€æµ‹æ˜¯å¦æˆåŠŸ
        # è¿™æ ·æˆ‘ä»¬æ€»èƒ½çœ‹åˆ°æ¨¡å‹åˆ°åº•é¢„æµ‹äº†ä»€ä¹ˆ
        for class_id in evaluate_classes:
            original_true_points_class = extract_annotation_points_multiclass(json_data, class_id)
            if original_true_points_class is None:
                continue
            
            rotated_true_points_class = apply_rotation_to_points(original_true_points_class, rotation_angle)
            
            pred_points_for_vis = None
            if class_id in detected_classes:
                # å³ä½¿æ£€æµ‹å¤±è´¥ï¼ˆæ¯”å¦‚æ£€æµ‹åˆ°2ä¸ªï¼‰ï¼Œæˆ‘ä»¬ä¾ç„¶æå–ç‚¹ä½ç”¨äºå¯è§†åŒ–
                detections = detected_classes[class_id]
                pred_points_for_vis = extract_prediction_points(results[0], detections)

            all_class_data[class_id] = {
                'true_points': rotated_true_points_class,
                'pred_points': pred_points_for_vis
            }

        # æ­¥éª¤2: ä»…åœ¨æ£€æµ‹æˆåŠŸæ—¶æ‰è®¡ç®—æŒ‡æ ‡
        if is_successful_detection:
            for class_id in evaluate_classes:
                # æˆ‘ä»¬çŸ¥é“æ¯ä¸ªç±»åˆ«è‚¯å®šéƒ½åœ¨detected_classesé‡Œï¼Œå¹¶ä¸”åªæœ‰ä¸€ä¸ªå®ä¾‹
                detections = detected_classes[class_id]
                pred_points = extract_prediction_points(results[0], detections)
                pred_mask = get_prediction_mask(results[0], detections)
                original_true_points_class = extract_annotation_points_multiclass(json_data, class_id)

                if original_true_points_class is not None:
                    calculate_metrics_multiclass(
                        original_true_points_class, pred_points, pred_mask, 
                        rotation_angle, metrics, group_key, class_id
                    )

        # æ­¥éª¤3: æ ¹æ®æˆåŠŸä¸å¦å†³å®šæ–‡ä»¶åå¹¶è¿›è¡Œå¯è§†åŒ–
        if is_successful_detection:
            save_path = results_dir / f'{base_filename}_evaluation.jpg'
        else:
            save_path = results_dir / f'{base_filename}_no_detection.jpg'
        
        # æ‰§è¡Œå¯è§†åŒ–
        if len(evaluate_classes) == 1:
            class_id = evaluate_classes[0]
            class_data = all_class_data.get(class_id, {})
            visualize_results_single(annotated_image, 
                                   class_data.get('pred_points'), 
                                   class_data.get('true_points'), 
                                   save_path, class_id)
        else:
            visualize_results(annotated_image, all_class_data, save_path)
        
        return is_successful_detection
            
    except Exception as e:
        print(f"å¤„ç†å¤±è´¥ {npy_file}: {e}")
        return False


def extract_prediction_points(result, bloodzone_detections):
    """æå–é¢„æµ‹ç‚¹ä½"""
    points_list = []
    for i in bloodzone_detections:
        points = result[i].masks.xyn[0]
        points[:, 0] *= result.orig_shape[1]
        points[:, 1] *= result.orig_shape[0]
        points_list.append(points)
    return np.vstack(points_list).astype(np.float32)  # ä¿æŒfloat32ç²¾åº¦


def get_prediction_mask(result, bloodzone_detections):
    """è·å–é¢„æµ‹æ©ç """
    mask = result[bloodzone_detections[0]].masks.data.cpu().numpy()[0]
    return cv2.resize(mask, (1504, 1504), interpolation=cv2.INTER_NEAREST)


def calculate_metrics_with_rotation(original_true_points, pred_points, pred_mask, rotation_angle, metrics, group_key):
    """ä½¿ç”¨åå‘æ—‹è½¬æ–¹æ³•è®¡ç®—è¯„ä¼°æŒ‡æ ‡ - ä¿æŒå‘åå…¼å®¹ï¼Œé»˜è®¤ä¸ºClass 1"""
    calculate_metrics_multiclass(original_true_points, pred_points, pred_mask, rotation_angle, 
                                metrics, group_key, class_id=1)


def calculate_metrics_multiclass(original_true_points, pred_points, pred_mask, rotation_angle, 
                                metrics, group_key, class_id):
    """ä½¿ç”¨åå‘æ—‹è½¬æ–¹æ³•è®¡ç®—å¤šç±»åˆ«è¯„ä¼°æŒ‡æ ‡
    
    Args:
        original_true_points: åŸå§‹æ ‡æ³¨ç‚¹ä½
        pred_points: é¢„æµ‹ç‚¹ä½
        pred_mask: é¢„æµ‹æ©ç ï¼ˆæš‚æ—¶ä¿ç•™ï¼Œç”¨äºIoUè®¡ç®—ï¼‰
        rotation_angle: æ—‹è½¬è§’åº¦
        metrics: æŒ‡æ ‡å­—å…¸
        group_key: åˆ†ç»„é”®å
        class_id: ç±»åˆ«ID
    """
    # å°†é¢„æµ‹ç‚¹åå‘æ—‹è½¬åˆ°åŸå§‹åæ ‡ç³»
    pred_points_original = apply_rotation_to_points(pred_points, -rotation_angle)
    
    # åœ¨åŸå§‹åæ ‡ç³»ä¸­è®¡ç®—é«˜åº¦å·®å¼‚
    pred_heights = np.sort(pred_points_original[:, 1])
    true_heights = np.sort(original_true_points[:, 1])
    
    # æ ¹æ®ç±»åˆ«é‡‡ç”¨ä¸åŒçš„é«˜åº¦è®¡ç®—ç­–ç•¥
    if class_id == 0:
        # Class 0 (è¡€æ¸…å±‚): 4ä¸ªç‚¹ï¼Œå–å‰å2ä¸ªå‡å€¼
        pred_upper, pred_lower = np.mean(pred_heights[:2]), np.mean(pred_heights[-2:])
        true_upper, true_lower = np.mean(true_heights[:2]), np.mean(true_heights[-2:])
    elif class_id == 1:
        # Class 1 (ç™½è†œå±‚): 4ä¸ªç‚¹ï¼Œé¢„æµ‹ç‚¹æ•°é‡å¤šï¼ˆåå‡ ä¸ªï¼‰ï¼Œå–å‰å2ä¸ªå‡å€¼ï¼›çœŸå®ç‚¹åªæœ‰4ä¸ªï¼Œå–é¦–æœ«1ä¸ª
        pred_upper, pred_lower = np.mean(pred_heights[:2]), np.mean(pred_heights[-2:])
        true_upper, true_lower = np.mean(true_heights[:2]), np.mean(true_heights[-2:])
    elif class_id == 2:
        # Class 2 (è¡€æµ†å±‚): 3-4ä¸ªç‚¹ï¼Œå–å‰å2ä¸ªå‡å€¼
        pred_upper, pred_lower = np.mean(pred_heights[:2]), np.mean(pred_heights[-2:])
        true_upper, true_lower = np.mean(true_heights[:2]), np.mean(true_heights[-1:])
    else:
        return  # æœªçŸ¥ç±»åˆ«ï¼Œè·³è¿‡
    
    # è®¡ç®—IoUï¼šåªå¯¹Class 0å’ŒClass 1è®¡ç®—ï¼ŒClass 2è·³è¿‡ï¼ˆä¸‰è§’å½¢IoUæ— æ„ä¹‰ï¼‰
    if class_id in [0, 1]:
        true_mask = np.zeros((1504, 1504), dtype=np.uint8)
        cv2.fillPoly(true_mask, [original_true_points.astype(np.int32)], 255)
        
        pred_mask_original = np.zeros((1504, 1504), dtype=np.uint8)
        cv2.fillPoly(pred_mask_original, [pred_points_original.astype(np.int32)], 255)
        
        iou = calculate_iou(true_mask, pred_mask_original)
        
        # è®°å½•IoUåˆ°å¯¹åº”åˆ†ç»„
        metrics[group_key]['iou_list'].append(iou)
    metrics[group_key]['height_upper_diff'].append(abs(true_upper - pred_upper))
    metrics[group_key]['height_lower_diff'].append(abs(true_lower - pred_lower))
    metrics[group_key]['height_upper_diff_percent'].append(abs((true_upper - pred_upper) / true_upper))
    metrics[group_key]['height_lower_diff_percent'].append(abs((true_lower - pred_lower) / true_lower))


def calculate_metrics(true_points, pred_points, pred_mask, metrics):
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡ - ä¿ç•™ç”¨äºå‘åå…¼å®¹"""
    # è®¡ç®—é«˜åº¦
    pred_heights = np.sort(pred_points[:, 1])
    true_heights = np.sort(true_points[:, 1])
    pred_upper, pred_lower = np.mean(pred_heights[:2]), np.mean(pred_heights[-2:])
    true_upper, true_lower = np.mean(true_heights[:1]), np.mean(true_heights[-1:])
    
    # è®¡ç®—IoU
    true_mask = np.zeros((1504, 1504), dtype=np.uint8)
    cv2.fillPoly(true_mask, [true_points], 255)
    iou = calculate_iou(true_mask, pred_mask)
    
    # è®°å½•æŒ‡æ ‡
    metrics['iou_list'].append(iou)
    metrics['height_upper_diff'].append(abs(true_upper - pred_upper))
    metrics['height_lower_diff'].append(abs(true_lower - pred_lower))
    metrics['height_upper_diff_percent'].append(abs((true_upper - pred_upper) / true_upper))
    metrics['height_lower_diff_percent'].append(abs((true_lower - pred_lower) / true_lower))


def print_evaluation_results(metrics, include_augmented, fusion_name):
    """æ‰“å°åˆ†ç»„è¯„ä¼°ç»“æœ"""
    print(f'\n=== {fusion_name}è¯„ä¼°ç»“æœ ===')
    
    # æ‰“å°åŸå§‹æ•°æ®ç»“æœ
    original_metrics = metrics['original']
    if original_metrics['total_count'] > 0:
        print(f'\nã€åŸå§‹æ•°æ®ã€‘ (å…±{original_metrics["total_count"]}å¼ å›¾åƒ)')
        detection_rate = (original_metrics['detected_count'] / original_metrics['total_count']) * 100
        print(f'  æ£€æµ‹ç‡: {detection_rate:.2f}%')
        
        if original_metrics["iou_list"]:
            print(f'  å¹³å‡IoU: {np.mean(original_metrics["iou_list"]):.4f}')
            print(f'  ä¸Šè¡¨é¢å·®å¼‚: {np.mean(original_metrics["height_upper_diff"]):.2f} åƒç´  ({np.mean(original_metrics["height_upper_diff_percent"])*100:.2f}%)')
            print(f'  ä¸‹è¡¨é¢å·®å¼‚: {np.mean(original_metrics["height_lower_diff"]):.2f} åƒç´  ({np.mean(original_metrics["height_lower_diff_percent"])*100:.2f}%)')
        else:
            print('  æ²¡æœ‰æˆåŠŸæ£€æµ‹åˆ°ä»»ä½•ç›®æ ‡')
    
    # æ‰“å°å¢å¼ºæ•°æ®ç»“æœ
    if include_augmented:
        augmented_metrics = metrics['augmented']
        if augmented_metrics['total_count'] > 0:
            print(f'\nã€å¢å¼ºæ•°æ®é›†ã€‘ (å…±{augmented_metrics["total_count"]}å¼ å›¾åƒï¼ŒåŒ…å«åŸå§‹æ•°æ®)')
            detection_rate = (augmented_metrics['detected_count'] / augmented_metrics['total_count']) * 100
            print(f'  æ£€æµ‹ç‡: {detection_rate:.2f}%')
            
            if augmented_metrics["iou_list"]:
                print(f'  å¹³å‡IoU: {np.mean(augmented_metrics["iou_list"]):.4f}')
                print(f'  ä¸Šè¡¨é¢å·®å¼‚: {np.mean(augmented_metrics["height_upper_diff"]):.2f} åƒç´  ({np.mean(augmented_metrics["height_upper_diff_percent"])*100:.2f}%)')
                print(f'  ä¸‹è¡¨é¢å·®å¼‚: {np.mean(augmented_metrics["height_lower_diff"]):.2f} åƒç´  ({np.mean(augmented_metrics["height_lower_diff_percent"])*100:.2f}%)')
            else:
                print('  æ²¡æœ‰æˆåŠŸæ£€æµ‹åˆ°ä»»ä½•ç›®æ ‡')
    
    # æ•…éšœæ’é™¤æç¤º
    if not any(metrics[key]["iou_list"] for key in metrics):
        print('\nå»ºè®®æ£€æŸ¥ï¼š')
        print('- æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æ­£ç¡®')
        print('- æ•°æ®æ ¼å¼æ˜¯å¦åŒ¹é…')
        print('- JSONæ ‡æ³¨æ–‡ä»¶æ˜¯å¦æ‰¾åˆ°')
        print('- æ¨¡å‹è¾“å…¥æ•°æ®èŒƒå›´å’Œæ ¼å¼')


def save_metrics_to_file(metrics, save_dir, fusion_name):
    """ä¿å­˜metricsæ•°æ®åˆ°JSONæ–‡ä»¶"""
    # è®¡ç®—ç»Ÿè®¡æ•°æ®
    metrics_summary = {}
    
    for group_key in ['original', 'augmented']:
        group_metrics = metrics[group_key]
        if group_metrics['total_count'] > 0:
            detection_rate = group_metrics['detected_count'] / group_metrics['total_count']
            
            if group_metrics['iou_list']:
                iou_mean = np.mean(group_metrics['iou_list'])
                iou_std = np.std(group_metrics['iou_list'])
                upper_diff_mean = np.mean(group_metrics['height_upper_diff'])
                upper_diff_std = np.std(group_metrics['height_upper_diff'])
                lower_diff_mean = np.mean(group_metrics['height_lower_diff'])
                lower_diff_std = np.std(group_metrics['height_lower_diff'])
                upper_diff_percent_mean = np.mean(group_metrics['height_upper_diff_percent'])
                lower_diff_percent_mean = np.mean(group_metrics['height_lower_diff_percent'])
            else:
                iou_mean = iou_std = 0
                upper_diff_mean = upper_diff_std = 0
                lower_diff_mean = lower_diff_std = 0
                upper_diff_percent_mean = lower_diff_percent_mean = 0
            
            metrics_summary[group_key] = {
                'total_count': group_metrics['total_count'],
                'detected_count': group_metrics['detected_count'],
                'detection_rate': detection_rate,
                'iou_mean': float(iou_mean),
                'iou_std': float(iou_std),
                'upper_diff_mean': float(upper_diff_mean),
                'upper_diff_std': float(upper_diff_std),
                'lower_diff_mean': float(lower_diff_mean), 
                'lower_diff_std': float(lower_diff_std),
                'upper_diff_percent_mean': float(upper_diff_percent_mean),
                'lower_diff_percent_mean': float(lower_diff_percent_mean),
                # ä¿å­˜åŸå§‹æ•°æ®åˆ—è¡¨
                'raw_data': {
                    'iou_list': [float(x) for x in group_metrics['iou_list']],
                    'height_upper_diff': [float(x) for x in group_metrics['height_upper_diff']],
                    'height_lower_diff': [float(x) for x in group_metrics['height_lower_diff']],
                    'height_upper_diff_percent': [float(x) for x in group_metrics['height_upper_diff_percent']],
                    'height_lower_diff_percent': [float(x) for x in group_metrics['height_lower_diff_percent']]
                }
            }
        else:
            metrics_summary[group_key] = {
                'total_count': 0,
                'detected_count': 0,
                'detection_rate': 0,
                'iou_mean': 0, 'iou_std': 0,
                'upper_diff_mean': 0, 'upper_diff_std': 0,
                'lower_diff_mean': 0, 'lower_diff_std': 0,
                'upper_diff_percent_mean': 0, 'lower_diff_percent_mean': 0,
                'raw_data': {'iou_list': [], 'height_upper_diff': [], 'height_lower_diff': [], 
                           'height_upper_diff_percent': [], 'height_lower_diff_percent': []}
            }
    
    # ä¿å­˜åˆ°JSONæ–‡ä»¶
    metrics_file = save_dir / f'metrics_{fusion_name}.json'
    with open(metrics_file, 'w') as f:
        json.dump(metrics_summary, f, indent=2)
    
    print(f"Metricsæ•°æ®å·²ä¿å­˜åˆ°: {metrics_file}")


def generate_evaluation_chart(metrics, save_dir, fusion_name):
    """ç”Ÿæˆåˆå¹¶çš„è¯„ä¼°å¯¹æ¯”å›¾è¡¨"""
    fig, ax1 = plt.subplots(figsize=(12, 8))
    
    # å‡†å¤‡æ•°æ®
    original_metrics = metrics['original']
    augmented_metrics = metrics['augmented']
    
    # æå–æ•°æ® (åŒ…æ‹¬æ ‡å‡†å·®)
    original_data = []
    augmented_data = []
    original_stds = []
    augmented_stds = []
    
    for key in ['original', 'augmented']:
        group_metrics = metrics[key]
        if group_metrics['total_count'] > 0:
            detection_rate = group_metrics['detected_count'] / group_metrics['total_count']
            if group_metrics['iou_list']:
                iou_mean = np.mean(group_metrics['iou_list'])
                iou_std = np.std(group_metrics['iou_list'])
                upper_diff = np.mean(group_metrics['height_upper_diff'])
                upper_diff_std = np.std(group_metrics['height_upper_diff'])
                lower_diff = np.mean(group_metrics['height_lower_diff'])
                lower_diff_std = np.std(group_metrics['height_lower_diff'])
            else:
                iou_mean = iou_std = 0
                upper_diff = upper_diff_std = 0
                lower_diff = lower_diff_std = 0
        else:
            detection_rate = 0
            iou_mean = iou_std = 0
            upper_diff = upper_diff_std = 0
            lower_diff = lower_diff_std = 0
        
        if key == 'original':
            original_data = [detection_rate, iou_mean, upper_diff, lower_diff]
            original_stds = [0, iou_std, upper_diff_std, lower_diff_std]  # æ£€æµ‹ç‡æ— æ ‡å‡†å·®
        else:
            augmented_data = [detection_rate, iou_mean, upper_diff, lower_diff]
            augmented_stds = [0, iou_std, upper_diff_std, lower_diff_std]
    
    # è®¾ç½®xè½´ä½ç½®å’Œå®½åº¦
    metrics_labels = ['Detection Rate', 'IoU', 'Upper Surface', 'Lower Surface']
    x_pos = np.arange(len(metrics_labels))
    width = 0.35
    
    # å®šä¹‰é¢œè‰² - 4ç§ä¸åŒçš„åŸºç¡€é¢œè‰²
    colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']  # è“ã€æ©™ã€ç»¿ã€çº¢
    
    # åˆ†åˆ«å¤„ç†æ¯”ä¾‹æŒ‡æ ‡å’Œåƒç´ å·®å¼‚æŒ‡æ ‡
    
    # è®¾ç½®å·¦ä¾§yè½´ (æ¯”ä¾‹ç±»æŒ‡æ ‡)
    ax1.set_ylabel('Ratio', fontsize=12)
    ax1.set_ylim([0, 1.01])
    ax1.tick_params(axis='y', labelcolor='black')
    
    # åˆ›å»ºå³ä¾§yè½´ (åƒç´ å·®å¼‚æŒ‡æ ‡)
    ax2 = ax1.twinx()
    ax2.set_ylabel('Pixel Difference', fontsize=12)
    ax2.set_ylim([0, 12.01])  # è®¾ç½®å³ä¾§yè½´èŒƒå›´ä¸º0-6
    ax2.tick_params(axis='y', labelcolor='black')
    
    # ç»˜åˆ¶æ‰€æœ‰4ä¸ªæŒ‡æ ‡çš„æŸ±çŠ¶å›¾ (åŒ…å«æ ‡å‡†å·®)
    for i, (metric_name, color) in enumerate(zip(metrics_labels, colors)):
        x_position = x_pos[i]
        
        if i < 2:  # Detection Rate å’Œ IoU ä½¿ç”¨å·¦ä¾§yè½´
            # Originalæ•°æ®ï¼šä¸é€æ˜
            ax1.bar(x_position - width/2, original_data[i], width, 
                   yerr=original_stds[i] if original_stds[i] > 0 else None,
                   color=color, alpha=0.9, label='Original' if i == 0 else "",
                   capsize=3)
            # Augmentedæ•°æ®ï¼šé€æ˜
            ax1.bar(x_position + width/2, augmented_data[i], width, 
                   yerr=augmented_stds[i] if augmented_stds[i] > 0 else None,
                   color=color, alpha=0.6, label='Augmented' if i == 0 else "",
                   capsize=3)
        else:  # Upper Surface å’Œ Lower Surface ä½¿ç”¨å³ä¾§yè½´
            # Originalæ•°æ®ï¼šä¸é€æ˜
            ax2.bar(x_position - width/2, original_data[i], width, 
                   yerr=original_stds[i] if original_stds[i] > 0 else None,
                   color=color, alpha=0.9, capsize=3)
            # Augmentedæ•°æ®ï¼šé€æ˜
            ax2.bar(x_position + width/2, augmented_data[i], width, 
                   yerr=augmented_stds[i] if augmented_stds[i] > 0 else None,
                   color=color, alpha=0.6, capsize=3)
    
    # è®¾ç½®xè½´
    ax1.set_xticks(x_pos)
    ax1.set_xticklabels(metrics_labels, fontsize=11)
    ax1.set_xlabel('Metrics', fontsize=12)
    
    # æ·»åŠ ç½‘æ ¼
    ax1.grid(axis='y', linestyle='--', alpha=0.3)
    
    # æ·»åŠ å›¾ä¾‹
    ax1.legend(loc='upper left', fontsize=10)
    
    # è®¾ç½®æ ‡é¢˜
    plt.title(f'Dual-Modal YOLO Evaluation Results - {fusion_name}', fontsize=14, pad=20)
    
    # è°ƒæ•´å¸ƒå±€å¹¶ä¿å­˜
    plt.tight_layout()
    plt.savefig(save_dir / f'evaluation_chart_{fusion_name}.png', dpi=300, bbox_inches='tight')
    plt.close()


if __name__ == '__main__':
    fusion_names = ['crossattn-precise']
    # fusion_names = ['id', 'crossattn', 'crossattn-30epoch', 'weighted-fusion', 'concat-compress']
    for fusion_name in fusion_names:
        evaluate_dual_yolo_model(fusion_name=fusion_name, 
                             debug=True, 
                             include_augmented=True, 
                             evaluate_classes=[0, 1, 2], 
                             conf_threshold=0.70)