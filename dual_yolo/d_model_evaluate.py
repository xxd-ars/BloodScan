"""
åŒæ¨¡æ€YOLOæ¨¡å‹è¯„ä¼°è„šæœ¬
ä½¿ç”¨åŒ»ç”Ÿæ ‡æ³¨çš„JSONæ•°æ®è¿›è¡Œç²¾åº¦è¯„ä¼°ï¼Œè®¡ç®—IoUå’Œé«˜åº¦å·®å¼‚æŒ‡æ ‡
"""

import torch
import sys
import os
import json
import numpy as np
import cv2
import matplotlib.pyplot as plt
from pathlib import Path
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))
from ultralytics import YOLO


def calculate_iou(mask1, mask2):
    """è®¡ç®—IoU"""
    m1, m2 = mask1 > 0, mask2 > 0
    intersection = np.logical_and(m1, m2).sum()
    union = np.logical_or(m1, m2).sum()
    return intersection / union if union > 0 else 1.0


def sort_points_by_angle(points):
    """æŒ‰æè§’æ’åºç‚¹ä½"""
    center = np.mean(points, axis=0)
    angles = np.arctan2(points[:, 1] - center[1], points[:, 0] - center[0])
    return points[np.argsort(angles)]


def find_json_annotation(npy_filename, json_dirs=['./data/rawdata/class1/', './data/rawdata/class2/']):
    """æŸ¥æ‰¾JSONæ ‡æ³¨æ–‡ä»¶"""
    parts = npy_filename.split('_')
    if len(parts) < 4:
        return None
    
    json_prefix = '_'.join(parts[:3]) + '_'
    
    for class_dir in json_dirs:
        if not os.path.exists(class_dir):
            continue
        for json_filename in os.listdir(class_dir):
            if json_filename.endswith('.json') and json_filename.startswith(json_prefix.replace('_', '_', 2)):
                try:
                    with open(class_dir + json_filename, 'r') as f:
                        return json.load(f)
                except Exception:
                    continue
    return None


def extract_annotation_points(json_data):
    """æå–åŒ»ç”Ÿæ ‡æ³¨ç‚¹ä½"""
    true_points = []
    for id, shape in enumerate(json_data.get("shapes", [])):
        if 2 <= id <= 5:  # ç™½è†œå±‚4ä¸ªç‚¹
            x, y = shape["points"][0]
            x, y = int((x - 800) * 1504/1216), int(y - 250)
            true_points.append([x, y])
    
    return sort_points_by_angle(np.array(true_points, dtype=np.int32)) if len(true_points) >= 4 else None


def visualize_results(annotated_image, pred_points=None, true_points=None, save_path=None):
    """å¯è§†åŒ–ç»“æœ"""
    # ç¡®ä¿å›¾åƒæ ¼å¼æ­£ç¡®
    if not isinstance(annotated_image, np.ndarray):
        annotated_image = np.array(annotated_image)
    if annotated_image.dtype != np.uint8:
        annotated_image = annotated_image.astype(np.uint8)
    annotated_image = np.ascontiguousarray(annotated_image)
    
    # ç»˜åˆ¶é¢„æµ‹è½®å»“
    if pred_points is not None:
        pred_points = np.array(pred_points, dtype=np.int32).reshape((-1, 1, 2))
        cv2.polylines(annotated_image, [pred_points], True, (0, 255, 0), 5)
    
    # ç»˜åˆ¶çœŸå®ç‚¹ä½
    if true_points is not None:
        for point in true_points:
            cv2.circle(annotated_image, tuple(map(int, point)), 5, (0, 0, 255), -1)
    
    # ä¿å­˜å›¾åƒ
    if save_path:
        rgb_image = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)
        plt.imsave(save_path, rgb_image)


def evaluate_dual_yolo_model(fusion_name='crossattn', debug=False):
    """ä¸»è¯„ä¼°å‡½æ•°"""
    # é…ç½®å‚æ•°
    project_root = Path(__file__).parent.parent
    model_yaml = project_root / 'dual_yolo' / 'models' / f'yolo11x-dseg-{fusion_name}.yaml'
    model_pt = project_root / 'dual_yolo' / 'runs' / 'segment' / f'dual_modal_train_{fusion_name}' / 'weights' / 'best.pt'
    
    dataset_path = project_root / 'datasets' / 'Dual-Modal-1504-500-1-6ch'
    test_images = dataset_path / 'test' / 'images'
    
    eval_results_dir = project_root / 'dual_yolo' / 'evaluation_results' / f'{fusion_name}'
    eval_results_dir.mkdir(exist_ok=True)
    
    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
    if debug:
        print(f"ğŸ” è°ƒè¯•ä¿¡æ¯ï¼š")
        print(f"  æ¨¡å‹é…ç½®æ–‡ä»¶: {model_yaml} ({'âœ…å­˜åœ¨' if model_yaml.exists() else 'âŒä¸å­˜åœ¨'})")
        print(f"  æ¨¡å‹æƒé‡æ–‡ä»¶: {model_pt} ({'âœ…å­˜åœ¨' if model_pt.exists() else 'âŒä¸å­˜åœ¨'})")
        print(f"  æµ‹è¯•å›¾åƒç›®å½•: {test_images} ({'âœ…å­˜åœ¨' if test_images.exists() else 'âŒä¸å­˜åœ¨'})")
    
    # åŠ è½½æ¨¡å‹
    print("åŠ è½½åŒæ¨¡æ€YOLOæ¨¡å‹...")
    try:
        model = YOLO(model_yaml).load(model_pt)
        if debug:
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # è·å–æµ‹è¯•æ–‡ä»¶åˆ—è¡¨ï¼ˆåªè¯„ä¼°_0åç¼€çš„åŸå§‹å›¾åƒï¼‰
    npy_files = sorted([f for f in os.listdir(test_images) 
                       if f.endswith('_0.npy')])
    
    print(f"è¯„ä¼°å›¾åƒæ•°é‡: {len(npy_files)}")
    
    # è¯„ä¼°æŒ‡æ ‡
    metrics = {
        'iou_list': [],
        'height_upper_diff': [],
        'height_lower_diff': [],
        'height_upper_diff_percent': [],
        'height_lower_diff_percent': [],
        'detected_count': 0
    }
    
    # é€ä¸ªè¯„ä¼°å›¾åƒ
    for npy_file in tqdm(npy_files, desc="è¯„ä¼°è¿›åº¦"):
        success = process_single_image(npy_file, test_images, model, eval_results_dir, metrics)
        if success:
            metrics['detected_count'] += 1
    
    # æ‰“å°å’Œä¿å­˜ç»“æœ
    print_evaluation_results(metrics, len(npy_files))
    if metrics["iou_list"]:  # åªæœ‰åœ¨æœ‰æ£€æµ‹ç»“æœæ—¶æ‰ç”Ÿæˆå›¾è¡¨
        generate_evaluation_chart(metrics, len(npy_files), eval_results_dir, fusion_name)
    
    print(f"\nè¯„ä¼°å®Œæˆï¼ç»“æœä¿å­˜åœ¨ {eval_results_dir}")


def process_single_image(npy_file, test_images_dir, model, results_dir, metrics):
    """å¤„ç†å•ä¸ªå›¾åƒè¯„ä¼°"""
    try:
        # åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®
        dual_tensor = np.load(test_images_dir / npy_file)
        if dual_tensor.shape[-1] == 6:
            dual_tensor = dual_tensor.transpose(2, 0, 1)
        
        # å‡†å¤‡å¯è§†åŒ–å›¾åƒ, [0, 1, 2]åŸå§‹RGBé€šé“é¡ºåº, [2, 1, 0]BGRé€šé“é¡ºåº
        blue_channels = dual_tensor[:3, :, :][[2, 1, 0], :, :]  # BGRé€šé“é¡ºåº
        blue_image = blue_channels.transpose(1, 2, 0)
        annotated_image = np.clip(blue_image * 255 if blue_image.max() <= 1.0 else blue_image, 0, 255).astype(np.uint8)
        
        # å‡†å¤‡æ¨¡å‹è¾“å…¥
        model_tensor = dual_tensor / 255.0 if dual_tensor.max() > 1.0 else dual_tensor
        model_input = torch.from_numpy(model_tensor).unsqueeze(0).float()
        
        # è·å–æ ‡æ³¨æ•°æ®
        json_data = find_json_annotation(npy_file)
        if not json_data:
            return False
        
        true_points = extract_annotation_points(json_data)
        if true_points is None:
            return False
        
        # æ¨¡å‹æ¨ç†
        results = model(model_input, imgsz=1504, device="cuda:0", verbose=False)
        
        # æ£€æŸ¥æ¨¡å‹è¾“å‡º
        if hasattr(results[0], 'boxes') and results[0].boxes is not None:
            if len(results[0].boxes) > 0:
                all_classes = results[0].boxes.cls.cpu().numpy()
                bloodzone_detections = [i for i, cls_id in enumerate(all_classes) if cls_id == 1]
            else:
                bloodzone_detections = []
        else:
            bloodzone_detections = []
        
        base_filename = npy_file.replace('.npy', '')
        
        if bloodzone_detections:
            # æå–é¢„æµ‹ç»“æœå¹¶è®¡ç®—æŒ‡æ ‡
            pred_points = extract_prediction_points(results[0], bloodzone_detections)
            pred_mask = get_prediction_mask(results[0], bloodzone_detections)
            calculate_metrics(true_points, pred_points, pred_mask, metrics)
            
            # ä¿å­˜å¯è§†åŒ–ç»“æœ
            save_path = results_dir / f'{base_filename}_evaluation.jpg'
            visualize_results(annotated_image, pred_points, true_points, save_path)
            return True
        else:
            # æœªæ£€æµ‹åˆ°çš„æƒ…å†µ
            save_path = results_dir / f'{base_filename}_no_detection.jpg'
            visualize_results(annotated_image, None, true_points, save_path)
            return False
            
    except Exception as e:
        print(f"å¤„ç†å¤±è´¥ {npy_file}: {e}")
        return False


def extract_prediction_points(result, bloodzone_detections):
    """æå–é¢„æµ‹ç‚¹ä½"""
    points_list = []
    for i in bloodzone_detections:
        points = result[i].masks.xyn[0]
        points[:, 0] *= result.orig_shape[1]
        points[:, 1] *= result.orig_shape[0]
        points_list.append(points)
    return np.vstack(points_list).astype(np.int32)


def get_prediction_mask(result, bloodzone_detections):
    """è·å–é¢„æµ‹æ©ç """
    mask = result[bloodzone_detections[0]].masks.data.cpu().numpy()[0]
    return cv2.resize(mask, (1504, 1504), interpolation=cv2.INTER_NEAREST)


def calculate_metrics(true_points, pred_points, pred_mask, metrics):
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    # è®¡ç®—é«˜åº¦
    pred_heights = np.sort(pred_points[:, 1])
    true_heights = np.sort(true_points[:, 1])
    pred_upper, pred_lower = np.mean(pred_heights[:2]), np.mean(pred_heights[-2:])
    true_upper, true_lower = np.mean(true_heights[:1]), np.mean(true_heights[-1:])
    
    # è®¡ç®—IoU
    true_mask = np.zeros((1504, 1504), dtype=np.uint8)
    cv2.fillPoly(true_mask, [true_points], 255)
    iou = calculate_iou(true_mask, pred_mask)
    
    # è®°å½•æŒ‡æ ‡
    metrics['iou_list'].append(iou)
    metrics['height_upper_diff'].append(abs(true_upper - pred_upper))
    metrics['height_lower_diff'].append(abs(true_lower - pred_lower))
    metrics['height_upper_diff_percent'].append(abs((true_upper - pred_upper) / true_upper))
    metrics['height_lower_diff_percent'].append(abs((true_lower - pred_lower) / true_lower))


def print_evaluation_results(metrics, total_images):
    """æ‰“å°è¯„ä¼°ç»“æœ"""
    print('\n=== åŒæ¨¡æ€YOLOè¯„ä¼°ç»“æœ ===')
    print(f'æ£€æµ‹ç‡: {(metrics["detected_count"]/total_images)*100:.2f}%')
    
    if metrics["iou_list"]:  # åªæœ‰åœ¨æœ‰æ£€æµ‹ç»“æœæ—¶æ‰è®¡ç®—å‡å€¼
        print(f'å¹³å‡IoU: {np.mean(metrics["iou_list"]):.4f}')
        print(f'ä¸Šè¡¨é¢å·®å¼‚: {np.mean(metrics["height_upper_diff"]):.2f} åƒç´  ({np.mean(metrics["height_upper_diff_percent"])*100:.2f}%)')
        print(f'ä¸‹è¡¨é¢å·®å¼‚: {np.mean(metrics["height_lower_diff"]):.2f} åƒç´  ({np.mean(metrics["height_lower_diff_percent"])*100:.2f}%)')
    else:
        print('æ²¡æœ‰æˆåŠŸæ£€æµ‹åˆ°ä»»ä½•ç›®æ ‡ï¼Œæ— æ³•è®¡ç®—IoUå’Œé«˜åº¦å·®å¼‚æŒ‡æ ‡')
        print('å»ºè®®æ£€æŸ¥ï¼š')
        print('- æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æ­£ç¡®')
        print('- æ•°æ®æ ¼å¼æ˜¯å¦åŒ¹é…')
        print('- JSONæ ‡æ³¨æ–‡ä»¶æ˜¯å¦æ‰¾åˆ°')
        print('- æ¨¡å‹è¾“å…¥æ•°æ®èŒƒå›´å’Œæ ¼å¼')


def generate_evaluation_chart(metrics, total_images, save_dir, fusion_name):
    """ç”Ÿæˆè¯„ä¼°å›¾è¡¨"""
    _, ax1 = plt.subplots(figsize=(12, 8))
    x_pos = [0.5, 1.5, 2.5, 3.5]
    
    # æ£€æµ‹ç‡å’ŒIoU
    ax1.bar(x_pos[0], metrics['detected_count']/total_images, 0.4, color='C0', alpha=0.9)
    ax1.bar(x_pos[1], np.mean(metrics['iou_list']), 0.4, yerr=np.std(metrics['iou_list']), color='C1', alpha=0.9)
    ax1.set_ylabel('Ratio')
    ax1.set_ylim([0, 1])
    
    # é«˜åº¦å·®å¼‚
    ax2 = ax1.twinx()
    ax2.bar(x_pos[2], np.mean(metrics['height_upper_diff']), 0.4, yerr=np.std(metrics['height_upper_diff']), color='C2', alpha=0.9)
    ax2.bar(x_pos[3], np.mean(metrics['height_lower_diff']), 0.4, yerr=np.std(metrics['height_lower_diff']), color='C3', alpha=0.9)
    ax2.set_ylabel('Pixel Difference')
    
    plt.xticks(x_pos, ['Detection Rate', 'IoU', 'Upper Diff', 'Lower Diff'])
    plt.title('Dual-Modal YOLO Evaluation Results', fontsize=14)
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.tight_layout()
    plt.savefig(save_dir / f'evaluation_chart_{fusion_name}.png', dpi=300)
    plt.close()


if __name__ == '__main__':
    fusion_name = 'crossattn'  # 'crossattn', 'id', 'concat_compress', 'weighted_fusion'
    evaluate_dual_yolo_model(fusion_name=fusion_name, debug=True)