\documentclass[journal,twoside,web]{ieeecolor}
\usepackage{generic}
\usepackage{gensymb}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{algorithm,algorithmic}
\usepackage{hyperref}
\hypersetup{hidelinks=true}
\usepackage{textcomp}
\usepackage{multirow}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\markboth{\hskip25pc IEEE TRANSACTIONS AND JOURNALS TEMPLATE}
{Author \MakeLowercase{\textit{et al.}}: Title}
\begin{document}
\title{Dual-Illumination Unidirectional Attention YOLO for Blood Fractionation Segmentation}
\author{First A. Author, \IEEEmembership{Fellow, IEEE}, Second B. Author, and Third C. Author Jr., \IEEEmembership{Member, IEEE}
\thanks{This paragraph of the first footnote will contain the date on 
which you submitted your paper for review. It will also contain support 
information, including sponsor and financial support acknowledgment. For 
example, ``This work was supported in part by the U.S. Department of 
Commerce under Grant 123456.'' }
\thanks{The next few paragraphs should contain 
the authors' current affiliations, including current address and e-mail. For 
example, First A. Author is with the National Institute of Standards and 
Technology, Boulder, CO 80305 USA (e-mail: author@boulder.nist.gov). }
\thanks{Second B. Author Jr. was with Rice University, Houston, TX 77005 USA. He is 
now with the Department of Physics, Colorado State University, Fort Collins, 
CO 80523 USA (e-mail: author@lamar.colostate.edu).}
\thanks{Third C. Author is with 
the Electrical Engineering Department, University of Colorado, Boulder, CO 
80309 USA, on leave from the National Research Institute for Metals, 
Tsukuba, Japan (e-mail: author@nrim.go.jp).}}

\maketitle

\begin{abstract}
These instructions give you guidelines for preparing papers for 
IEEE Transactions and Journals. Use this document as a template if you are 
using \LaTeX. Otherwise, use this document as an 
instruction set. The electronic file of your paper will be formatted further 
at IEEE. Paper titles should be written in uppercase and lowercase letters, 
not all uppercase. Avoid writing long formulas with subscripts in the title; 
short formulas that identify the elements are fine (e.g., "Nd--Fe--B"). Do 
not write ``(Invited)'' in the title. Full names of authors are preferred in 
the author field, but are not required. Put a space between authors' 
initials. The abstract must be a concise yet comprehensive reflection of 
what is in your article. In particular, the abstract must be self-contained, 
without abbreviations, footnotes, or references. It should be a microcosm of 
the full article. The abstract must be between 150--250 words. Be sure that 
you adhere to these limits; otherwise, you will need to edit your abstract 
accordingly. The abstract must be written as one paragraph, and should not 
contain displayed mathematical equations or tabular material. The abstract 
should include three or four different keywords or phrases, as this will 
help readers to find it. It is important to avoid over-repetition of such 
phrases as this can result in a page being rejected by search engines. 
Ensure that your abstract reads well and is grammatically correct.
\end{abstract}

\begin{IEEEkeywords}
Enter key words or phrases in alphabetical order, separated by commas. Using the IEEE Thesaurus can help you find the best standardized keywords to fit your article. Use the thesaurus access request form for free access to the IEEE Thesaurus: \underline{https://www.ieee.org/publications/services/thesaurus-acce}\\
\underline{ss-page.com.}
\end{IEEEkeywords}

\section{Introduction}
\label{sec:introduction}
\IEEEPARstart {B}LOOD component stratification following centrifugation is a critical process in clinical laboratory workflows. Centrifuged blood tubes exhibit distinct layers—serum or plasma, buffy coat, and erythrocyte fractions—whose accurate localization is essential for automated liquid handling and downstream assays. Precise layer identification also enables preliminary quality assessment by revealing sample integrity and physiological conditions. However, conventional stratification methods rely predominantly on manual visual inspection or basic measurement tools, which are constrained by operator subjectivity, labor intensity, and inter-operator variability. As laboratory automation advances and testing volumes increase, intelligent systems capable of accurate layer localization are needed to ensure reliable, high-throughput clinical diagnostics.

Automated blood stratification presents multiple technical challenges. First, inter-layer boundaries exhibit intrinsically low optical contrast. The buffy coat, typically 0.5–1.0 mm thick and comprising less than 1\% of blood volume, is particularly difficult to distinguish from adjacent layers under standard imaging conditions. Second, single-illumination imaging fails to capture the full range of discriminative features. White-light images provide structural context but poor buffy coat contrast, while blue-light illumination enhances buffy coat fluorescence but may obscure other boundaries. Third, cross-spectral registration is challenging due to parallax, pose variation, and temporal mismatch during dual-modality acquisition. Direct feature fusion often introduces spatial misalignment and semantic inconsistency, degrading detection accuracy.

\begin{figure}[!t]
\centering

% ==== 参数定义 ====
\newlength{\sepM}\setlength{\sepM}{10pt}      % 中间间距
\newlength{\colw}\setlength{\colw}{0.45\linewidth} % 每个子图宽度
\newlength{\imgheight}

% ==== 左图 ====
\begin{minipage}[t]{\colw}
  \centering
  \includegraphics[width=\linewidth]{img/blue_annotated.jpg}%
  \settoheight{\imgheight}{\includegraphics[width=\linewidth]{img/blue_annotated.jpg}}\\
  \vspace{3pt}
\end{minipage}
% ==== 右图 ====
\begin{minipage}[t]{\colw}
  \centering
  \includegraphics[width=\linewidth]{img/normal_annotated.jpg}\\
  \vspace{3pt}
\end{minipage}

\vspace{5pt}
\caption{\textbf{Annotated visualization of blood component stratification under white-light and blue-light illumination.}
Red markers denote manually selected layer boundary points by professional laboratory personnel: points 1–4 enclose the plasma region, points 3–6 define the buffy coat (white membrane layer), and points 5–7 correspond to the erythrocyte fraction. 
Under white-light illumination (left), inter-layer boundaries show low optical contrast, making the buffy coat difficult to distinguish. 
Under blue-light illumination (right), the buffy coat fluorescence enhances layer separability, though specular reflections on the tube surface and similar chromatic appearance between plasma and erythrocytes may introduce visual ambiguity.}
\label{fig:annotated images}
\end{figure}

Despite recent advances, existing blood stratification methods remain limited. Prior approaches have focused on one-dimensional liquid-level regression [1], which can identify single interfaces but not complex multi-layer structures. Deep networks like Inception-ResNet-V2 [2] achieve serum quality classification but provide only categorical assessment without spatial localization. Commercial systems such as PerkinElmer's JANUS Blood iQ [3] utilize dual-illumination imaging based on traditional image processing and clustering algorithm for automated layer detection, but technical details remain unpublished. In broader medical imaging, Transformer-based methods like MicFormer [4] and A2FSeg [5] have introduced cross-modal attention for CT–MRI fusion, yet these approaches target segmentation neglecting detection capabilities. Multispectral detection methods such as YOLO-Phantom [6] and MAF-YOLO combine visible and infrared imaging through simple concatenation, lacking refined spatial alignment mechanisms. Current methods thus fail to address fine-grained cross-modal alignment and complementary information extraction for stratified blood component targets.

To address these challenges, this paper proposes DIUA-YOLO (Dual-Illumination Unidirectional Attention YOLO), a dual-spectral detection framework for precise blood component stratification. Our principal contributions are:

\begin{itemize}
    \item A dual-backbone architecture that separately processes blue-light and white-light images is designed and developed, enabling multi-scale feature extraction and fusion to leverage complementary optical information. 
    \item We introduce a cross-modal attention mechanism employing unidirectional queries—where blue-light features query white-light features—combined with localized token-level attention. This design achieves effective modality alignment and information fusion while reducing computational complexity by approximately 91\% compared to bidirectional global attention. 
    \item We establish a rigorous clinical evaluation framework requiring each layer to be detected exactly once, demanding zero false positives and negatives. On our dual-illumination blood tube dataset, DIUA-YOLO achieves 98.89\% detection success rate, substantially outperforming single-modality YOLO11 baselines. 
\end{itemize}

The remainder of this paper is organized as follows. Section II details the DIUA-YOLO architecture and key technical modules. Section III describes the experimental design and evaluation methodology. Section IV presents results and performance analysis. Section V concludes and discusses future directions.


\section{Methodology}
\label{sec:methodology}

\subsection{Automated Acquisition System}

The proposed DIUA-YOLO system comprises two integrated components: a hardware acquisition platform for dual-modality image collection and a deep learning network for blood layer detection. This section describes the automated platform design and operational workflow.

\subsubsection{Mechanical Configuration and Control Architecture} As in Fig. 2, the acquisition platform employs a three-degree-of-freedom linear motion system (X-axis horizontal, Y-axis anterior-posterior, Z-axis vertical) coupled with a single-degree-of-freedom rotational gripper. The X and Y axes coordinate to traverse all positions in a standard 96-well tube rack, while the Z axis provides vertical displacement between rack height and imaging focal plane. The rotational gripper delivers controllable radial clamping force and precise angular positioning, ensuring tube stability during transport and imaging.

The motion control system adopts a layered architecture. At the communication layer, RS485 serial protocol governs linear motion stepper motors, while Modbus TCP Ethernet protocol controls rotational servo motors. The application layer encapsulates atomic motor commands into composite detection routines, with thread event synchronization coordinating precise timing between mechanical motion and image acquisition. A data management layer provides SQLite database support, recording tube barcode, type, detection results, batch number, and timestamps for full traceability.

\begin{figure}[!t]
\centerline{\includegraphics[width=\columnwidth]{img/mechanics.jpg}}
\caption{\textbf{Mechanical configuration and prototype of the automated acquisition platform.} 
Left: Three-degree-of-freedom (X–Y–Z) linear motion system coupled with a single-degree-of-freedom rotational gripper, designed for precise positioning, tube handling, and imaging. 
Right: Physical implementation of the system, including the translational-rotational manipulator, tube rack, imaging module, and control interface.}
\label{fig mechanics}
\end{figure}

\subsubsection{Dual-Illumination Imaging System} The imaging system employs a single industrial camera integrated with dual illumination sources. A blue LED array (center wavelength ~465 nm) enhances the optical contrast of the buffy coat through increased light scattering and weak intrinsic autofluorescence, while a white LED source (color temperature ~6500 K) provides structural and color information of the overall sample. A custom mechanical fixture rigidly maintains the relative geometry among the camera, light sources, and tube gripper, thereby minimizing parallax and viewpoint variation during acquisition. Illumination control is managed by a microcontroller that drives programmable LED arrays, with the host computer synchronizing illumination switching, motor actuation, and image capture via serial communication protocols.

\subsubsection{Automated Detection Workflow} A single-tube detection sequence comprises a strictly ordered set of operations: (1) The robotic arm descends along the Z-axis to the rack height, while the X-axis and the Y-axis move synchronously to position above the target test tube well; (2) The gripper closes to clamp tube, and the Z-axis ascends to lift it; (3) The X-axis translates the tube to the camera’s field of view, and the Z-axis lowers it to the imaging focal plane; (4) Once arrvied, sequential dual-illumination imaging is performed: the white-light and blue-light sources are activated in turn to capture two aligned frames, while during such process, the camera–tube geometry remains mechanically fixed; (5) The rotational executor performs a 180 \degree rotation, the same dual-illumination imaging takes place to capture the full image of the sample; (6) The acquired images are processed by the cross-spectral detection model for layer localization and analysis; (7) After imaging, the Z-axis lifts, and the X-axis returns the tube to its original position; (8) The Z-axis then descends to rack height, the gripper releases the tube, and the arm resets to its initial position. Within the motion control system, event blocking at each critical node enforces sequential execution, ensuring completion of one action before the next begins, thereby eliminating potential motion conflicts. 

Batch detection traverses all rack positions via nested loops: the outer loop controls column selection, the inner loop controls row selection. Upon completing all tubes, the system automatically homes and increments the batch counter. This design realizes a fully automated "grasp-image-assess-release" closed loop, requiring operators only to load racks and initiate commands after homing. Upon completing the final tube, the system automatically returns to the home position and increments the batch counter. This design achieves a fully automated closed-loop workflow encompassing tube loading, imaging, analysis, and return, requiring operator intervention only for rack placement and command initialization.

\begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth]{img/arch.png}
\caption{\textbf{The proposed DIUA-YOLO framework,} comprising dual-illumination input, dual-backbone feature extraction, cross-spectral feature fusion, and YOLO11 segmentation modules. 
White-light and blue-light images are independently processed through modality-specific backbones to capture complementary structural and contrast information. 
Fused multi-scale representations are decoded through neck and segmentation heads to accurately localize plasma, buffy coat, and erythrocyte layers.}
\label{fig:architecture}
\end{figure*}

\subsection{DIUA-YOLO Network Architecture}

Fig. 3 illustrates the overall architecture of the proposed DIUA-YOLO framework. The network processes dual-illumination inputs through independent feature extraction backbones, fuses multi-scale representations via cross-modal feature fusion blocks, and outputs segmentation results through YOLO segmentation heads.

\subsubsection{Dual-Backbone Feature Extraction} White-light and blue-light images enter two structurally identical but parameter-independent convolutional neural network backbones based on the YOLO11 architecture. This dual-backbone design allows each modality's feature extractor to specialize in learning modality-specific representations: the white-light backbone network excels at extracting overall hierarchical structures, three-material color gradients, and texture details, while the blue-light backbone network captures the optical properties and boundary features of the buffy coat layer layer under blue illumination.

The backbone architecture employs YOLO11's C3k2 block, an efficient Cross Stage Partial (CSP) design that reduces computational overhead while preserving feature extraction capability. In each backbone stage, the C3k2 block employs three consecutive convolutional layers with residual connections to capture hierarchical feature representations at varying semantic depths. This streamlined architecture achieves approximately 40\% parameter reduction compared to prior YOLO variants while maintaining detection accuracy, thereby enabling efficient dual-backbone training for our dual-illumination framework without excessive computational burden.

Each backbone outputs a multi-scale feature pyramid at three levels—P3, P4, and P5—corresponding to different spatial resolutions and semantic depths. The P3 layer, with higher spatial resolution, suits detection of thin-layer structures such as the buffy coat. The P5 layer, with larger receptive fields, accommodates detection of thick layers such as serum, plasma and erythrocyte. This multi-scale design effectively captures blood layer information across varying thicknesses.

Let $I_w \in \mathbb{R}^{H \times W \times 3}$ and $I_b \in \mathbb{R}^{H \times W \times 3}$ denote the white-light and blue-light input images, respectively, where $H$ and $W$ represent height and width. The dual backbones extract feature pyramids:

\begin{equation}
\{F_w^{P3}, F_w^{P4}, F_w^{P5}\} = \text{Backbone}_w(I_w)
\end{equation}
\begin{equation}
\{F_b^{P3}, F_b^{P4}, F_b^{P5}\} = \text{Backbone}_b(I_b)
\end{equation}

where each feature $F \in \mathbb{R}^{h \times w \times c}$ has spatial dimensions $(h, w)$ and channel dimension $c$. These features then feed into fusion modules at each pyramid level.

\subsubsection{Cross-Modal Attention Fusion}

To effectively integrate complementary information from dual modalities while maintaining computational efficiency, we propose a cross-modal attention mechanism employing unidirectional queries and localized spatial matching. This fusion strategy operates independently at each pyramid level $s \in \{P3, P4, P5\}$ to align and enhance blue-light features using white-light contextual information.

\textbf{Unidirectional Query Design.} Unlike conventional bidirectional attention that computes mutual interactions, our method adopts an asymmetric design: blue-light features serve as queries $Q$ to retrieve information from white-light features (keys $K$ and values $V$), while the reverse direction is omitted. Formally, for feature pair $(F_b^s, F_w^s)$ at pyramid level $s$, we compute:

\begin{equation}
Q^s = \text{Conv}(F_b^s), \quad K_s = \text{Conv}(F_w^s), \quad V^s = \text{Conv}(F_w^s)
\end{equation}

where $Q_s, K_s, V_s \in \mathbb{R}^{B \times C \times H_s \times W_s}$, with $B$ denoting batch size, $C$ channel dimension, and $(H_s, W_s)$ spatial dimensions at level $s$. This unidirectional strategy exploits the blue-light modality's advantage in buffy coat detection to guide feature fusion, reducing attention computation by approximately 50\% compared to bidirectional schemes while maintaining detection performance.

\textbf{Token-Based Spatial Partitioning.} To reduce computational complexity from $O(N^2)$ (where $N = H_s \times W_s$) to tractable levels, we partition feature maps into non-overlapping tokens of size $t \times t$ pixels. The feature map is reshaped into a token grid:

\begin{equation}
Q^s_{tok} = \text{Tokenize}(Q^s) \in \mathbb{R}^{B \times n_h \times n_w \times C \times t^2}
\end{equation}

where $\text{Tokenize}(\cdot)$ denotes the combined spatial decomposition, dimension rearrangement, and flattening operation that partitions the feature map into non-overlapping $t \times t$ tokens, $n_h = \lceil H_s / t \rceil$ and $n_w = \lceil W_s / t \rceil$ define the token grid dimensions, and $t^2$ represents the flattened spatial dimension within each token. Similar transformations apply to $K^s_{tok}$ and $V^s_{tok}$. For our implementation, token size $t$ varies across pyramid levels, e.g., $t=2$ for P3 (preserving fine spatial detail), $t=3$ for P4, and $t=4$ for P5 (accommodating larger receptive fields).

\textbf{Localized Attention Computation.} Rather than computing global attention across all tokens, each query token attends only to a local neighborhood of key tokens. For a query token at grid position $(i, j)$, we extract a $k \times k$ neighborhood of key and value tokens centered at $(i, j)$:

\begin{equation}
\begin{aligned}
\mathcal{N}_{ij} = \{&K^s_{tok}[i', j'], V^s_{tok}[i', j'] \mid \\
&|i'-i| \leq \lfloor k/2 \rfloor, |j'-j| \leq \lfloor k/2 \rfloor\}
\end{aligned}
\end{equation}

where $k$ is the neighborhood size parameter. This localization is efficiently implemented via PyTorch's \texttt{unfold} operation with padding. The attention weights are then computed as:

\begin{equation}
\alpha_{ij} = \text{softmax}\left(\frac{Q^s_{tok}[i,j] \cdot K^s_{tok}[\mathcal{N}_{ij}]^\top}{\sqrt{t^2}}\right) \in \mathbb{R}^{C \times k^2}
\end{equation}

where $\sqrt{t^2}$ is the scaling factor (corresponding to the token dimension), and $\cdot$ denotes batched matrix multiplication across channels. The enhanced token $\widetilde{Q}^s_{tok}[i,j]$ is obtained through weighted aggregation of values from the local neighborhood:

\begin{equation}
\widetilde{Q}^s_{tok}[i,j] = \sum_{(i',j') \in \mathcal{N}_{ij}} \alpha_{ij}[i',j'] \cdot V^s_{tok}[i',j'] \in \mathbb{R}^{C \times t^2}
\end{equation}

The neighborhood size $k$ is adapted per pyramid level: $k=3$ for P3, $k=5$ for P4, and $k=7$ for P5. These token size and neighborhood size hyperparameters were determined through grid search optimization on validation set, systematically evaluating candidate configurations $t \in \{2, 3, 4\}$ and $k \in \{3, 5, 7\}$ to maximize detection success rate while maintaining computational efficiency. This hierarchical design balances local precision with contextual coverage. By constraining attention to local neighborhoods, complexity reduces from $O(N^2)$ to $O(N \cdot k^2)$, achieving approximately 91\% reduction at level of P5 with input image size of $1504 \times 1504$.

\textbf{Feature Reconstruction} After computing enhanced tokens for all grid positions, we collect them into a complete token grid:

\begin{equation}
\begin{aligned}
\widetilde{Q}^s_{tok} &= \{\widetilde{Q}^s_{tok}[i,j] \mid 1 \leq i \leq n_h, 1 \leq j \leq n_w\} \\
&\quad \in \mathbb{R}^{B \times n_h \times n_w \times C \times t^2}
\end{aligned}
\end{equation}

We then reverse the tokenization process through spatial rearrangement and reshaping operations to reconstruct the feature map:

\begin{equation}
\widetilde{F}_b^s = \text{Detokenize}(\widetilde{Q}^s_{tok}) \in \mathbb{R}^{B \times C \times H_s \times W_s}
\end{equation}

where $\text{Detokenize}(\cdot)$ performs dimension permutation and reshaping to convert the token grid back to spatial layout. The final fused representation $F_{f}^s$ employs a residual connection to preserve original blue-light information:

\begin{equation}
F_{f}^s = \text{Proj}(\widetilde{F}_b^s + F_b^s)
\end{equation}

where $F_b^s$ denotes the original blue-light features and $\text{Proj}(\cdot)$ denotes a $1 \times 1$ convolution for channel projection. This residual formulation ensures that the network learns additive refinements rather than replacing the base features, facilitating gradient flow and training stability.

\subsubsection{Channel Concatenation Fusion}

As a commonly used fusion strategy, we implement channel-wise concatenation followed by dimensionality reduction. For feature pair $(F_b^s, F_w^s)$ at pyramid level $s$, this method directly concatenates along the channel dimension and applies $1 \times 1$ convolution for compression:

\begin{equation}
F_{f}^s = \text{Conv}(\text{Concat}(F_b^s, F_w^s)) \in \mathbb{R}^{B \times C \times H_s \times W_s}
\end{equation}

where the $1 \times 1$ convolutional layer reduces channel count from $2C$ back to $C$ through learned linear combinations. While computationally efficient, this approach lacks explicit modeling of cross-modal semantic correspondences and spatial alignment, limiting its ability to resolve inter-modality registration errors or extract complementary information selectively.

\subsubsection{Adaptive Weighted Fusion}

This method attempts to learn modality-specific importance through adaptive weighting. The fusion combines spatial attention, global context, and learnable temperature parameters. Features are first normalized via layer normalization, then concatenated for joint weight prediction:

\begin{equation}
\begin{split}
\tilde{F}_s = \text{Concat}(&\text{LayerNorm}(F_b^s), \\
&\text{LayerNorm}(F^w_s)) \in \mathbb{R}^{B \times 2C \times H_s \times W_s}
\end{split}
\end{equation}

Two parallel pathways predict importance weights. The spatial pathway generates pixel-wise weights through cascaded convolutions:

\begin{equation}
w^s_{sp} = \sigma(\text{Conv}(\text{Conv}_{3\times3}(\text{Conv}_{3\times3}(\tilde{F}^s))))
\end{equation}

where $\sigma$ denotes the sigmoid activation. The global pathway computes context-aware weights via adaptive average pooling (which reduces spatial dimensions to $1 \times 1$ through global averaging) followed by convolutional projections:

\begin{equation}
w^s_{gl} = \sigma(\text{Conv}(\text{Conv}(\text{GAP}(\tilde{F}^s))))
\end{equation}

where $\text{GAP}(\cdot)$ denotes global average pooling. The final weight combines both pathways with a learnable temperature $\tau$, constrained to $[0.1, 0.9]$ via clamping to prevent extreme values:

\begin{equation}
w_s = \max(0.1, \min(w_s^{sp} \cdot w_s^{gl} \cdot \tau, 0.9))
\end{equation}

The fused feature $F_{f}^s$ applies this weight with additional blue-light residual to leverage its stronger buffy coat detection capability:

\begin{equation}
F_{f}^s = (\alpha + w_s) \cdot \hat{F}_b^s + (1 - w_s) \cdot \hat{F}_w^s
\end{equation}

where $\hat{F}_b^s$ and $\hat{F}_w^s$ denote the normalized features, and $\alpha$ is a fixed coefficient emphasizing blue-light contribution.

\subsection{Implementation Details}

Our implementation extends Ultralytics YOLO through minimally invasive modifications that enable dual-backbone architectures while preserving full compatibility with existing workflows. The key principle is deep integration: dual-modality support is achieved by extending—rather than replacing—YOLO's core components, allowing users to leverage the entire ecosystem with only data format adjustments.

The model parser in \textit{ultralytics/nn/tasks.py} was extended to interpret three-section YAML configurations comprising \textit{backbone\_b}, \textit{backbone\_w}, and \textit{head}. The \textit{parse\_model} function maintains separate channel tracking lists (\textit{ch\_b}, \textit{ch\_w}) and iterates through concatenated layer definitions (\textit{backbone\_b + backbone\_w + head}), automatically inferring output channels via index-based branching. During forward propagation, \textit{\_predict\_once} routes features through modality-specific backbones using conditional logic, merging streams at designated fusion points. This architecture enables direct reuse of all existing YOLO modules (Conv, C3k2, SPPF) without modification, transforming YOLO from single-modality to dual-modality detection through parser-level extensions alone.

Fusion modules are implemented as standalone PyTorch classes in \textit{ultralytics/nn/modules/fusion.py}, each accepting feature pairs $(F_b^s, F_w^s)$ and returning fused representations. To eliminate manual hyperparameter specification, we extended \textit{parse\_model} with specialized handling logic that automatically infers channel dimensions from backbone outputs. For example, YAML specification \textit{[[4, 15], 1, CrossModalAttention, [2, 3]]} triggers retrieval of channel counts from layers 4 and 15, followed by instantiation with token size 2 and neighbor size 3. This configuration-driven design enables switching fusion strategies by modifying a single YAML line—no source code changes required.

Data flow adopts a unified 6-channel tensor format concatenating RGB from both modalities. The \textit{split\_6ch\_tensor} function separates inputs for independent backbone processing, while \textit{BaseDataset} was extended to accept \textit{channels=6} parameter. The \textit{load\_image} method automatically detects pre-concatenated NumPy arrays (\textit{.npy}) when available, falling back to paired loading from \textit{images\_b/} and \textit{images\_w/} directories otherwise. Users need only adjust dataset YAML metadata (\textit{channels: 6}) and organize data accordingly—all other YOLO functionalities (augmentation, validation, export) operate unchanged.

This modular architecture achieves deep coupling with YOLO's ecosystem: training scripts, validation routines, and export utilities require no modification. The complete implementation will be open-sourced to support multi-modal detection research.


\section{Dataset and Experimental Setup}
\label{sec:experiments}

This section describes the dataset construction, evaluation metrics, implementation details, and baseline comparisons for validating the proposed DIUA-YOLO framework.

\subsection{Dataset Construction}

\subsubsection{Data Acquisition and Annotation} We constructed a dual-illumination blood tube dataset comprising 500 tube samples acquired through the automated platform described in Section \ref{sec:methodology}. Each sample was sequentially imaged under white-light and blue-light illumination at 1504$\times$1504 pixel resolution, yielding 500 dual-modality image pairs. Consistent mechanical fixation during sequential acquisition ensured sub-pixel registration accuracy between modalities, with measured positional variance below 0.1 mm.

Annotation was performed via the Roboflow platform under expert supervision. Each image received polygon-level segmentation masks for three blood component layers: serum/plasma layer (Class 0), buffy coat (Class 1), and erythrocyte layer (Class 2). All annotations underwent dual-operator cross-validation, with discrepancies adjudicated by senior laboratory specialists.

\subsubsection{Dataset Partitioning and Augmentation} The 500 samples were partitioned into training, validation, and test sets, yielding 360, 100, and 40 samples per subset, respectively. To enhance model robustness to imaging variability, we implemented a structured augmentation pipeline generating nine variants per sample. The base variant (suffix 0) preserved original acquisition conditions. Variants 1 to 4 combined rotation ($\pm$5\degree) with either Gaussian blur ($\sigma$=1.5) or exposure adjustment (factor=0.9--1.1). Variants 5 to 8 applied three-way transformations involving rotation ($\pm$10\degree), brightness modulation (factor=0.85--1.15), and adaptive blur. These transformations address practical challenges including tube orientation variation, illumination inconsistency, and optical aberrations. Augmentation was applied uniformly to both modalities while transforming polygon annotations via corresponding geometric mappings to maintain spatial correspondence.

\subsubsection{Six-Channel Data Format} To streamline dual-modality training, we adopted a unified 6-channel tensor representation concatenating white-light RGB channels (channels 0--2) with blue-light RGB channels (channels 3--5). This format enables batch-wise data loading while preserving full spectral information from both illumination conditions. Pre-concatenated samples were serialized as NumPy arrays (.npy format), reducing disk I/O latency by approximately 40\% compared to paired loading of separate modality files. The data loader automatically handles channel decomposition during forward propagation, routing channels 0--2 and 3--5 to their respective backbone networks as described in Section \ref{sec:methodology}.

\subsection{Evaluation Metrics}

\subsubsection{Detection Success Rate (DSR)} To assess clinical applicability, we define Detection Success Rate (DSR) as the primary evaluation metric. A sample is deemed successfully detected if and only if each of the three layer classes is detected exactly once with confidence exceeding threshold $\tau$. Formally:
\begin{equation}
\text{DSR} = \frac{1}{N}\sum_{i=1}^{N}\mathbb{1}\left(\bigwedge_{c \in \{0,1,2\}} |\mathcal{D}_i^c(\tau)| = 1\right)
\end{equation}
where $N$ denotes test set cardinality, $\mathcal{D}_i^c(\tau) = \{d \mid d \in \mathcal{D}_i^c, \text{conf}(d) \geq \tau\}$ represents detections of class $c$ in sample $i$ satisfying confidence threshold $\tau$, and $\mathbb{1}(\cdot)$ denotes the indicator function. This stringent criterion mandates zero false positives (no duplicate detections) and zero false negatives (no missing layers), reflecting the operational requirement for automated systems to deliver complete layer localization in a single inference pass.

\subsubsection{Geometric Precision Metrics} Beyond detection completeness, we quantify localization accuracy through complementary geometric metrics. 

\textbf{Intersection over Union} (IoU) measures mask overlap between predicted and ground-truth segmentation regions as the ratio of intersection to union pixel counts. For each class $c$ across the test set, we compute mean IoU over all successfully detected instances:
\begin{equation}
\text{mIoU}_c = \frac{1}{|\mathcal{S}_c|} \sum_{i \in \mathcal{S}_c} \frac{|M_{\text{pred}}^{i,c} \cap M_{\text{gt}}^{i,c}|}{|M_{\text{pred}}^{i,c} \cup M_{\text{gt}}^{i,c}|}
\end{equation}
where $\mathcal{S}_c$ denotes the subset of test samples with successful detection of class $c$, $M_{\text{pred}}^{i,c}$ and $M_{\text{gt}}^{i,c}$ represent predicted and ground-truth masks for class $c$ in sample $i$, and $|\cdot|$ denotes pixel count. 

\textbf{Boundary displacement} metrics assess vertical positional error by computing absolute pixel differences between predicted and annotated upper/lower layer interfaces. For multi-vertex annotations, interface positions are defined as the mean vertical coordinate of the several most extreme vertices.

\textbf{Mean Average Precision} (mAP) quantifies detection performance across varying confidence thresholds and IoU criteria. For a given IoU threshold $\theta$, precision and recall are computed at each confidence level $t$:
\begin{equation}
\begin{aligned}
\text{Precision}(t) &= \frac{\text{TP}(t)}{\text{TP}(t) + \text{FP}(t)} \\
\text{Recall}(t) &= \frac{\text{TP}(t)}{\text{TP}(t) + \text{FN}(t)}
\end{aligned}
\end{equation}
where TP, FP, and FN denote true positives, false positives, and false negatives at threshold $t$. A predicted detection is deemed a true positive if its confidence value calculated by the model reaches $t$, its IoU with a ground-truth instance exceeds $\theta$ and the ground-truth has not been previously matched. Average Precision (AP) for class $c$ at IoU threshold $\theta$ is computed as the area under the precision-recall curve:
\begin{equation}
\text{AP}_c(\theta) = \int_{0}^{1} \text{Precision}_c(r, \theta) \, dr
\end{equation}
where $r$ denotes recall. Mean Average Precision aggregates AP across all classes: $\text{mAP}(\theta) = \frac{1}{|\mathcal{C}|} \sum_{c \in \mathcal{C}} \text{AP}_c(\theta)$. We report mAP@0.5 (IoU threshold 0.5) and mAP@0.5:0.95 (mean over IoU thresholds from 0.5 to 0.95 with step 0.05), following YOLO evaluation protocols.

These metrics collectively provide comprehensive assessment of detection completeness, localization accuracy, and segmentation quality.

\subsection{Implementation Details}

\subsubsection{Training Configuration} All models were implemented in PyTorch 2.4.1 with CUDA 12.4 support and trained on a quad-GPU workstation equipped with four NVIDIA RTX 3090 cards (24GB VRAM each). Distributed data parallel (DDP) training was employed across devices [0,1,2,3], yielding an effective batch size of 8 (2 samples per GPU). Training proceeded for at most 30 epochs at input resolution 1504$\times$1504 pixels.

We train our model using the Adam optimizer with initial learning rate $\eta_0 = 0.01$, momentum 0.937, and weight decay $5 \times 10^{-4}$. Learning rate scheduling followed applied linear decay from $\eta_0$ to $\eta_{\text{min}} = 0.0001$, preceded by 3-epoch linear warmup from 0 to $\eta_0$. Critically, automatic mixed precision (AMP) was explicitly disabled to preserve numerical stability in cross-modal attention weight computation, as float16 accumulation induced gradient instabilities during softmax normalization. This configuration demanded substantial GPU memory but ensured reliable convergence of attention mechanisms. Data loading utilized 4 parallel workers per GPU to overlap preprocessing with model computation.

\subsubsection{Model Initialization and Transfer Learning} Our training pipeline employs a hierarchical initialization strategy designed to leverage knowledge from both generic feature extraction and modality-specific fine-tuning. The complete workflow comprises three stages:

\textbf{Stage I: Single-Modality Pretraining.} We first trained two independent YOLO11x-seg models on blue-light and white-light images respectively, both initialized from COCO-pretrained weights. This stage enables each modality-specific backbone to learn discriminative features optimized for its illumination characteristics—blue-light models specialize in buffy coat fluorescence enhancement, while white-light models capture structural gradients and color transitions. Single-modality training proceeded for 30 epochs with augmentation pipelines.

\textbf{Stage II: Dual-Backbone Weight Transfer.} To initialize the dual-backbone architecture, we developed a custom weight transfer mechanism that migrates trained single-modality parameters into corresponding branches. The transfer script implements layer-wise remapping: blue-light backbone weights (layers 0--10 from single-modality model) populate the blue-light branch (layers 0--10 in dual-backbone), while white-light backbone weights similarly populate the white-light branch (layers 11--21 in dual-backbone). Segmentation head weights are transferred via index mapping to account for the expanded backbone depth. This initialization preserves modality-specific feature extractors learned during Stage I, providing superior starting points compared to random initialization or naive COCO weight replication.

\textbf{Stage III: Dual-Modality Fine-Tuning.} The dual-backbone model undergoes end-to-end fine-tuning with three alternative initialization strategies: (\textit{i}) \textit{pretrained}: loading transferred weights from Stage II for both backbones and fusion modules initialized via Xavier uniform; (\textit{ii}) \textit{freeze\_backbone}: identical to pretrained but with backbone parameters (layers 0--21) frozen during initial 5 epochs to stabilize fusion learning before joint optimization; (\textit{iii}) \textit{scratch}: training from COCO weights without Stage I/II transfer, serving as an ablation baseline. Our primary experiments employ the pretrained strategy, which empirically demonstrated the most reliable convergence across fusion variants.

\subsection{Baseline Comparisons and Ablation Studies}

To systematically validate our architectural design choices and quantify the contribution of each component, we conducted comprehensive baseline comparisons and ablation experiments along four dimensions:

\textbf{Single-Modality vs. Dual-Modality Architecture.} To validate the necessity of dual-illumination imaging and rule out confounding factors, we establish single-modality baselines as YOLO11x-seg models exclusively trained on blue-light or white-light images. 
This comparison directly quantifies whether cross-spectral fusion provides measurable performance gains over conventional single-source acquisition, particularly for challenging targets like the buffy coat layer.

\textbf{Fusion Strategy Comparison.} We systematically compare three fusion mechanisms of increasing sophistication: (\textit{i}) \textit{Channel Concatenation}: direct channel-wise concatenation followed by convolutional compression, representing minimal inter-modal interaction; (\textit{ii}) \textit{Adaptive Weighted Fusion}: learnable spatial and global attention weights for modality-specific importance estimation; (\textit{iii}) \textit{Cross-Modal Attention}: unidirectional token-level attention enabling explicit semantic correspondence modeling. All fusion variants are trained under identical conditions (pretrained initialization, 30 epochs) to isolate the impact of fusion architecture.

\textbf{Initialization Strategy Ablation.} To further distinguish the contribution of cross-spectral fusion from hierarchical transfer learning, we compare dual-modality models under two initialization regimes: (\textit{i}) \textit{from scratch}: both dual-modality and single-modality models are trained directly from COCO-pretrained weights for identical 30 epochs, without modality-specific pretraining (Stage I/II omitted for dual-modality models); (\textit{ii}) \textit{pretrained}: dual-modality models leverage transferred weights from Stage I/II single-modality training. If dual-modality models trained from scratch still outperform single-modality counterparts trained for equivalent epochs, this confirms that performance improvements originate from the dual-backbone fusion architecture rather than accumulated training iterations on backbone parameters.

All experiments employ identical training protocols (optimizer, learning rate schedule, augmentation pipelines) and evaluation procedures (DSR@$\tau$=0.25, mAP, IoU) to ensure rigorous controlled comparisons.



\section{Results}
\label{sec:results}

\begin{table*}[t]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.8}
\caption{Comparison of Our Dual-Illumination Cross-Attention Fusion Against Other Approaches Across Plasma, Buffy Coat and Erythrocyte Segmentation Tasks in Terms of Various Metrics at conf=0.25}
\label{tab:main_conf25}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c c c c c c c c}
\hline
\multicolumn{2}{c}{} & \multicolumn{4}{c}{\textbf{Plasma Segmentation}} & \multicolumn{4}{c}{\textbf{Buffy Coat Segmentation}} & \multicolumn{4}{c}{\textbf{Erythrocyte Segmentation}}\\
\cline{3-6}\cline{7-10}\cline{11-14}
Method & Dual-backbone & \textbf{DSR(\%)} $\uparrow$ & IOU $\uparrow$ & Diff$_{up}$(px) $\downarrow$ & Diff$_{low}$(px) $\downarrow$ & \textbf{DSR(\%)} $\uparrow$ & IOU $\uparrow$ & Diff$_{up}$(px) $\downarrow$ & Diff$_{low}$(px) $\downarrow$ & \textbf{DSR(\%)} $\uparrow$ & IOU $\uparrow$ & Diff$_{up}$(px) $\downarrow$ & Diff$_{low}$(px) $\downarrow$\\
\hline
Yolo11-White & $\times$ & 52.50 & 0.81$\pm$0.18 & 5.8$\pm$4.8 & 97.2$\pm$119.3 & 8.06 & 0.69$\pm$0.08 & 5.9$\pm$3.9 & 2.5$\pm$1.1 & 51.39 & 0.88$\pm$0.15 & 93.2$\pm$116.7 & \underline{3.9$\pm$6.6}\\
Yolo11-Blue & $\times$ & 24.72 & 0.94$\pm$0.03 & 9.7$\pm$6.1 & \underline{3.1$\pm$3.6} & 38.06 & 0.75$\pm$0.07 & \underline{4.0$\pm$2.3} & \textbf{1.9$\pm$1.4} & 2.22 & 0.96$\pm$0.01 & 6.4$\pm$2.6 & 6.0$\pm$3.7\\
Dual Yolo Concat & $\checkmark$ & 86.94 & 0.97$\pm$0.03 & \textbf{5.2$\pm$3.7} & 5.9$\pm$19.2 & 67.78 & \underline{0.75$\pm$0.07} & 4.1$\pm$2.7 & 2.5$\pm$1.8 & 49.72 & \textbf{0.98$\pm$0.01} & \underline{4.9$\pm$5.2} & 5.2$\pm$2.4\\
Dual Yolo Weighted & $\checkmark$ & \underline{99.72} & \textbf{0.97$\pm$0.01} & \underline{5.3$\pm$6.9} & 4.4$\pm$6.4 & \underline{75.56} & 0.73$\pm$0.07 & 4.6$\pm$3.0 & 2.7$\pm$2.3 & \underline{99.44} & \textbf{0.98$\pm$0.01} & 5.3$\pm$3.2 & 6.3$\pm$7.3\\
\hline
\textbf{Dual Yolo CrossAttn} & $\checkmark$ & \textbf{100.00} & \textbf{0.97$\pm$0.01} & 5.4$\pm$3.4 & \textbf{3.6$\pm$5.6} & \textbf{96.39} & \textbf{0.76$\pm$0.06} & \textbf{3.7$\pm$2.2} & \underline{2.4$\pm$1.8} & \textbf{100.00} & \textbf{0.98$\pm$0.01} & \underline{3.4$\pm$2.5} & \textbf{3.1$\pm$2.4}\\
\hline
\end{tabular}%
} % end resizebox
\end{table*}


\begin{table*}[t]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.8}
\caption{Comparison of Our Dual-Illumination Cross-Attention Fusion Against Other Approaches Across Plasma, Buffy Coat and Erythrocyte Segmentation Tasks in Terms of Various Metrics at conf=0.5}
\label{tab:main_conf50}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c c c c c c c c}
\hline
\multicolumn{2}{c}{} & \multicolumn{4}{c}{\textbf{Plasma Segmentation}} & \multicolumn{4}{c}{\textbf{Buffy Coat Segmentation}} & \multicolumn{4}{c}{\textbf{Erythrocyte Segmentation}}\\
\cline{3-6}\cline{7-10}\cline{11-14}
Method & Dual-backbone & \textbf{DSR(\%)} $\uparrow$ & IOU $\uparrow$ & Diff$_{up}$(px) $\downarrow$ & Diff$_{low}$(px) $\downarrow$ & \textbf{DSR(\%)} $\uparrow$ & IOU $\uparrow$ & Diff$_{up}$(px) $\downarrow$ & Diff$_{low}$(px) $\downarrow$ & \textbf{DSR(\%)} $\uparrow$ & IOU $\uparrow$ & Diff$_{up}$(px) $\downarrow$ & Diff$_{low}$(px) $\downarrow$\\
\hline
Yolo11-White & $\times$ & 56.67 & 0.81$\pm$0.18 & 5.7$\pm$4.7 & 96.7$\pm$118.7 & 4.72 & 0.72$\pm$0.06 & 5.0$\pm$3.5 & 1.5$\pm$0.8 & 47.50 & 0.89$\pm$0.14 & 80.7$\pm$111.5 & \underline{4.1$\pm$7.3}\\
Yolo11-Blue & $\times$ & 72.78 & 0.94$\pm$0.03 & 9.6$\pm$7.8 & \underline{3.8$\pm$5.6} & \underline{66.67} & 0.75$\pm$0.07 & 4.1$\pm$2.4 & \textbf{1.9$\pm$1.4} & 10.28 & 0.96$\pm$0.01 & 4.9$\pm$2.9 & 7.7$\pm$9.5\\
Dual Yolo Concat & $\checkmark$ & 93.06 & 0.96$\pm$0.04 & \underline{5.1$\pm$3.7} & 7.0$\pm$25.2 & 39.17 & \textbf{0.77$\pm$0.05} & \underline{3.8$\pm$2.8} & 2.4$\pm$1.5 & 96.67 & \textbf{0.98$\pm$0.01} & 4.7$\pm$5.1 & 5.2$\pm$2.5\\
Dual Yolo Weighted & $\checkmark$ & \underline{98.61} & \textbf{0.97$\pm$0.01} & \textbf{5.0$\pm$5.0} & 4.4$\pm$6.4 & 43.89 & 0.74$\pm$0.07 & 4.2$\pm$2.8 & 2.7$\pm$2.2 & \textbf{100.00} & \textbf{0.98$\pm$0.01} & \textbf{3.3$\pm$3.2} & 6.4$\pm$7.4\\
\hline
\textbf{Dual Yolo CrossAttn} & $\checkmark$ & \textbf{100.00} & \textbf{0.97$\pm$0.01} & 5.4$\pm$3.4 & \textbf{3.6$\pm$5.6} & \textbf{90.28} & \underline{0.77$\pm$0.06} & \textbf{3.7$\pm$2.2} & \underline{2.3$\pm$1.8} & \textbf{100.00} & \textbf{0.98$\pm$0.01} & \underline{3.4$\pm$2.5} & \textbf{3.2$\pm$2.4}\\
\hline
\end{tabular}%
}
\end{table*}


This section presents comprehensive evaluation of the DIUA-YOLO framework across multiple dimensions: quantitative performance metrics, ablation studies, qualitative visualization, and attention mechanism analysis. We evaluate our method against single-modality baselines and alternative fusion strategies under the stringent Detection Success Rate (DSR) criterion, which mandates exact detection of all three blood layers without false positives or negatives.


\subsection{Single-Modality vs. Dual-Modality Performance}

Table~\ref{tab:main_conf25} presents the primary evaluation results at confidence threshold 0.25, comparing single-modality baselines (Yolo11-White, Yolo11-Blue) against dual-modality fusion variants. The results demonstrate substantial performance gains from dual-illumination architectures while revealing complementary strengths of each modality.

Single-modality methods exhibit distinct performance characteristics reflecting their illumination-specific advantages. Yolo11-Blue achieves 24.72\% DSR for plasma segmentation with high IoU (0.94$\pm$0.03), benefiting from blue-light enhancement of layer boundaries. Conversely, its erythrocyte layer DSR remains critically low (2.22\%), as blue illumination provides limited structural information for red blood cell visualization. Yolo11-White demonstrates inverse behavior: 52.50\% plasma DSR but only 8.06\% buffy coat DSR, confirming that white-light imaging struggles with the optically thin buffy coat layer despite adequate overall structural representation.

Dual-modality architectures consistently outperform single-modality baselines across all metrics, validating our hypothesis that cross-spectral fusion addresses complementary deficiencies. Among dual-backbone methods, our Dual Yolo CrossAttn method achieves optimal performance with 100\% DSR for both plasma and erythrocyte segmentation, and 96.39\% DSR for the challenging buffy coat layer. This represents absolute improvements of +75.28\% (plasma), +96.39\% (buffy coat), and +97.78\% (erythrocyte) over the better-performing single-modality baseline for each respective class.

Geometric precision metrics corroborate detection success rates. Cross-attention fusion maintains IoU of 0.97$\pm$0.01 for plasma and 0.98$\pm$0.01 for erythrocyte layers, matching or exceeding single-modality performance while achieving substantially higher detection completeness. Boundary displacement metrics reveal sub-5-pixel accuracy: Diff$_{up}$ = 5.4$\pm$3.4 px and Diff$_{low}$ = 3.6$\pm$5.6 px for plasma; Diff$_{up}$ = 3.4$\pm$2.5 px and Diff$_{low}$ = 3.1$\pm$2.4 px for erythrocyte. For the buffy coat layer—whose thickness typically spans 20--40 pixels—achieving Diff$_{up}$ = 3.7$\pm$2.2 px demonstrates clinically acceptable localization precision.

Table~\ref{tab:main_conf50} presents results at higher confidence threshold (0.5), where the performance gap between single-modality and dual-modality methods becomes more pronounced. Cross-attention fusion maintains 100\% DSR for plasma and erythrocyte layers, while buffy coat DSR decreases marginally to 90.28\%. This threshold-dependent behavior reflects the inherent detection difficulty of thin buffy coat structures, yet dual-modality methods remain substantially more robust than single-modality alternatives.

\begin{table*}[t]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.8}
\caption{Segmentation Metrics Across Plasma, Buffy Coat and Erythrocyte Segmentation Tasks (conf=0.001, IoU@0.5)}
\label{tab:academic_metrics}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c c c c c c c c c c c c c c c}
\hline
\multicolumn{2}{c}{} & \multicolumn{6}{c}{\textbf{Plasma Segmentation}} & \multicolumn{6}{c}{\textbf{Buffy Coat Segmentation}} & \multicolumn{6}{c}{\textbf{Plasma Segmentation}}\\
\cline{3-8}\cline{9-14}\cline{15-20}
Method & Dual-backbone & mAP@[0.5:0.95] & AP@0.75 & AP@0.50 & F1 & Prec & Recall & mAP@[0.5:0.95] & AP@0.75 & AP@0.50 & F1 & Prec & Recall & mAP@[0.5:0.95] & A@0.P75 & AP@0.50 & F1 & Prec & Recall\\
\hline
Yolo11-White & $\times$ & 48.17 & 47.56 & 65.85 & 62.55 & 56.35 & 70.28 & 7.15 & 1.84 & 17.18 & 18.82 & 44.39 & 11.94 & 55.75 & 50.49 & 73.35 & 71.68 & 84.53 & 62.23\\
Yolo11-Blue & $\times$ & 87.52 & 97.33 & 96.90 & 96.20 & 94.87 & 97.57 & 53.17 & 47.35 & \underline{97.38} & \underline{90.34} & \underline{95.69} & 85.56 & 93.24 & 98.57 & \textbf{99.50} & 73.09 & 57.60 & \textbf{100.00}\\
Dual Yolo Concat & $\checkmark$ & 98.80 & 99.65 & 99.39 & 94.88 & 90.71 & 99.44 & \underline{55.22} & \textbf{57.77} & 90.86 & 84.17 & 87.79 & 80.83 & 98.52 & 98.53 & 98.49 & 91.69 & 86.09 & 98.06\\
Dual Yolo Weighted & $\checkmark$ & \underline{99.36} & \underline{99.95} & \textbf{99.50} & \textbf{99.96} & \textbf{100.00} & \underline{99.92} & 50.31 & 38.62 & 95.06 & 88.83 & 87.70 & \underline{90.00} & \textbf{99.68} & \textbf{99.50} & \textbf{99.50} & \underline{99.39} & \underline{98.78} & \textbf{100.00}\\
\hline
\textbf{Dual Yolo CrossAttn} & $\checkmark$ & \textbf{99.57} & \textbf{100.00} & \textbf{99.50} & \underline{99.63} & \underline{99.25} & \textbf{100.00} & \textbf{55.71} & \underline{54.53} & \textbf{99.50} & \textbf{98.67} & \textbf{100.00} & \textbf{97.37} & \underline{99.50} & \textbf{99.50} & \textbf{99.50} & \textbf{99.89} & \textbf{99.78} & \textbf{100.00}\\
\hline

\end{tabular}%
} % end resizebox
\end{table*}

\subsection{mAP Metrics and Segmentation Quality}

Table~\ref{tab:academic_metrics} presents traditional detection metrics evaluated at low confidence threshold (conf=0.001, IoU@0.5) to comprehensively assess segmentation quality across the precision-recall spectrum. These metrics provide complementary insights to DSR by evaluating model performance across varying confidence levels.

For plasma segmentation, Dual Yolo CrossAttn achieves mAP@[0.5:0.95] of 99.57\%, AP@0.75 of 100\%, and AP@0.50 of 99.50\%, surpassing all baselines. The F1 score of 99.63\% with precision 99.25\% and recall 100\% indicates near-perfect segmentation of this relatively straightforward class. Single-modality methods show substantial performance degradation: Yolo11-White achieves only 48.17\% mAP@[0.5:0.95], while Yolo11-Blue reaches 87.52\%, confirming the superiority of blue illumination for structural boundary detection.

Buffy coat segmentation—the most challenging task—reveals the critical advantage of cross-modal attention. Our method achieves 55.71\% mAP@[0.5:0.95], 99.50\% AP@0.50, and 98.67\% F1 score with 100\% precision and 97.37\% recall. This represents a dramatic improvement over Yolo11-White (7.15\% mAP@[0.5:0.95], 17.18\% AP@0.50) and substantial gains over Yolo11-Blue (53.17\% mAP@[0.5:0.95], 97.38\% AP@0.50). The perfect precision (100\%) combined with high recall (97.37\%) demonstrates the model's ability to reliably localize buffy coat boundaries without generating false positives—essential for automated clinical deployment.

For erythrocyte segmentation, all methods achieve high AP@0.50 ($\geq$98.49\%), reflecting the relative ease of detecting this thick, visually distinct layer. Nevertheless, Dual Yolo CrossAttn maintains the highest F1 score and joint-best recall, ensuring complete layer coverage.

Figure~\ref{fig mechanics} visualizes average performance across all methods, illustrating that dual-modality approaches consistently dominate across DSR, IoU, and academic metrics. The cross-attention method exhibits the most balanced profile, achieving near-ceiling performance across all metrics without the instability observed in weighted fusion or the modest performance plateau of channel concatenation.


\begin{figure}[!t]
\centerline{\includegraphics[width=\columnwidth]{img/metrics.png}}
\caption{Comparison of average detection and segmentation metrics across all methods. Detection sucess rate and IoU are from conf=0.25.}
\label{fig mechanics}
\end{figure}


\subsection{Fusion Strategy Comparison}

Among dual-modality architectures, fusion mechanism design critically determines performance. We compare three strategies of increasing sophistication: channel concatenation, adaptive weighted fusion, and cross-modal attention.

\textbf{Channel Concatenation} (Dual Yolo Concat) provides baseline dual-modality fusion by directly concatenating feature channels followed by convolutional compression. This approach achieves 86.94\% plasma DSR and 67.78\% buffy coat DSR at conf=0.25 (Table~\ref{tab:main_conf25}), substantially outperforming single-modality methods but falling short of more advanced fusion mechanisms. The limited performance reflects the method's inability to model explicit cross-modal correspondences or adaptively weight modality-specific contributions, relying instead on learned convolutional filters to implicitly extract complementary information.

\textbf{Adaptive Weighted Fusion} (Dual Yolo Weighted) employs learnable spatial and global attention weights to modulate modality contributions. This method achieves 99.72\% plasma DSR and 99.44\% erythrocyte DSR—approaching cross-attention performance for these classes. However, buffy coat DSR reaches only 75.56\%, revealing the method's limitations in handling optically challenging structures. Training stability also poses concerns: the weighted fusion mechanism exhibited convergence difficulties during early epochs, requiring careful training hyperparameter tuning and sometimes producing degenerate solutions where one modality dominates.

\textbf{Cross-Modal Attention} (Dual Yolo CrossAttn) implements unidirectional token-level attention with localized spatial matching. This design achieves the highest DSR across all classes: 100\% for serum/plasma, 96.39\% for buffy coat, and 100\% for erythrocyte at conf=0.25. The substantial buffy coat improvement (+20.83\% over weighted fusion, +28.61\% over concatenation) demonstrates the value of explicit semantic correspondence modeling. By allowing blue-light features to query relevant white-light context within local neighborhoods, the attention mechanism effectively resolves ambiguities in thin-layer localization while maintaining computational efficiency through token-based spatial partitioning.

Geometric precision metrics show consistent trends: cross-attention achieves the most balanced upper/lower boundary errors across all classes, with Diff$_{up}$ and Diff$_{low}$ typically within 3--6 pixels. This uniformity suggests the attention mechanism successfully integrates complementary spatial information rather than simply favoring one modality.

\begin{table*}[!t]
\caption{Ablation Experiments on the Blood Fractionation Component Segmentation Task at conf=0.25}
\label{ablation}
\centering
\setlength{\tabcolsep}{12pt}
\renewcommand{\arraystretch}{1.8}
\begin{tabular}{c c c c c c}
\hline
\multirow{2}{*}{\textbf{No.}} &
\multicolumn{3}{c}{\textbf{Settings}} &
\multirow{2}{*}{\textbf{Detection Rate(\%) $\uparrow$}} &
\multirow{2}{*}{\textbf{IOU $\uparrow$}} \\
\cline{2-4}
 & Dual-backbone & Dual-illumination fusion method & Pre-training & \\
\hline
1 & $\times$     & None                     & From Scratch      & 21.67 & 0.88$\pm$0.22 \\
2 & $\checkmark$ & Channel Concatenation    & From Scratch      & 52.40 & 0.87$\pm$0.23 \\
3 & $\checkmark$ & Channel Concatenation    & Transfer Learning & 68.15 & \underline{}{0.90$\pm$0.21} \\
4 & $\checkmark$ & Adaptive Weighted        & From Scratch      & 83.20 & 0.86$\pm$0.20 \\
5 & $\checkmark$ & Adaptive Weighted        & Transfer Learning & 91.57 & 0.89$\pm$0.17 \\
6 & $\checkmark$ & Cross Modal Attention    & From Scratch      & \underline{95.10} & 0.89$\pm$0.18 \\
7 & $\checkmark$ & Cross Modal Attention    & Transfer Learning & \textbf{98.80} & \textbf{0.90$\pm$0.14} \\
\hline
\end{tabular}
\end{table*}

\subsection{Ablation Studies}

Table~\ref{ablation} presents systematic ablation experiments evaluating three critical design factors: dual-backbone architecture, fusion mechanism, and initialization strategy. All experiments were conducted at conf=0.25 under identical training protocols with 360 test samples. Statistical significance was assessed using two-proportion Z-tests to rigorously validate performance differences.

\textbf{Dual-Backbone Necessity.} Comparing experiments 1 (single-backbone) and 2--7 (dual-backbone variants) reveals the fundamental importance of independent modality-specific feature extraction. Single-backbone architecture achieves only 21.67\% detection rate on total three classes. Introducing dual backbones with even the simplest fusion (channel concatenation, experiment 2) improves detection rate to 52.40\%, representing a statistically significant +30.73 percentage point improvement (p$<$0.001, Z=-8.54), validating the core architectural principle.

\textbf{Fusion Mechanism Progression.} Experiments 2--3 (concatenation), 4--5 (weighted fusion), and 6--7 (cross-attention) demonstrate progressive performance gains with increasingly sophisticated fusion designs. Under scratch training, cross-modal attention (experiment 6) achieves 95.10\% detection rate and 0.89$\pm$0.18 IoU, representing +42.70\% and +11.90\% absolute improvements over concatenation (p$<$0.001, Z=-13.00) and weighted fusion (p$<$0.001, Z=-5.11) respectively. These highly significant differences validate our hypothesis that explicit semantic correspondence modeling through attention mechanisms better captures cross-modal complementarity than implicit feature blending.

\textbf{Transfer Learning Impact.} Comparing scratch (even-numbered experiments) versus pretrained (odd-numbered experiments) initialization reveals consistent benefits from hierarchical weight transfer. Transfer learning improves detection rates by +15.75\% (concatenation, p$<$0.001), +8.37\% (weighted fusion, p$<$0.001), and +3.70\% (cross-attention, p$<$0.01). The diminishing marginal benefit for more sophisticated fusion methods suggests that cross-attention's architectural advantages partially compensate for initialization quality. Nevertheless, all pretrain effects remain statistically significant, and the pretrained cross-attention model (experiment 7) achieves the best overall performance: 98.80\% detection rate and 0.90$\pm$0.14 IoU, confirming the value of combining advanced fusion mechanisms with domain-adapted initialization.

\begin{figure*}[!t]
\centering
\newlength{\sepSa}\setlength{\sepSa}{0.8pt}   % 小列间距
\newlength{\sepLa}\setlength{\sepLa}{10pt}    % 大列间距（用于 2↔3 与 6↔7）
\newlength{\sepYa}\setlength{\sepYa}{3pt}     % 行间距
\newlength{\colwa}\setlength{\colwa}{\dimexpr(\linewidth - 4\sepSa - 2\sepLa)/7\relax}
\newcommand{\Img}[1]{%
  \includegraphics[width=\colwa,trim=2pt 2pt 2pt 2pt,clip]{#1}%
}
\newcommand{\Row}[7]{%
  \noindent
  \Img{#1}\hspace{\sepSa}%
  \Img{#2}\hspace{\sepLa}% 2↔3 大间距
  \Img{#3}\hspace{\sepSa}%
  \Img{#4}\hspace{\sepSa}%
  \Img{#5}\hspace{\sepSa}%
  \Img{#6}\hspace{\sepLa}% 6↔7 大间距
  \Img{#7}\par
}
\Row{evaluation/2022-03-28_103204_17_T3_2410.jpg}{evaluation/2022-03-28_103204_17_T5_2412.jpg}{evaluation/2022-03-28_103204_17_T5_2412_0_no_detection_id.jpg}{evaluation/2022-03-28_103204_17_T5_2412_0_no_detection_id_white.jpg}{evaluation/2022-03-28_103204_17_T5_2412_0_evaluation_concat.jpg}{evaluation/2022-03-28_103204_17_T5_2412_0_no_detection_cross.jpg}{evaluation/2022-03-28_103204_17_T5_2412_0_evaluation_best.jpg}\vspace{\sepYa}
\Row{evaluation/2022-03-28_143344_64_T3_2432.jpg}{evaluation/2022-03-28_143344_64_T5_2434.jpg}{evaluation/2022-03-28_143344_64_T5_2434_7_no_detection_id.jpg}{evaluation/2022-03-28_143344_64_T5_2434_7_no_detection_id_white.jpg}{evaluation/2022-03-28_143344_64_T5_2434_7_no_detection_concat.jpg}{evaluation/2022-03-28_143344_64_T5_2434_7_no_detection_cross.jpg}{evaluation/2022-03-28_143344_64_T5_2434_7_evaluation_best.jpg}\vspace{\sepYa}
\Row{evaluation/2022-04-07_141933_73_T3_2520.jpg}{evaluation/2022-04-07_141933_73_T5_2522.jpg}{evaluation/2022-04-07_141933_73_T5_2522_0_no_detection_id.jpg}{evaluation/2022-04-07_141933_73_T5_2522_0_no_detection_id_white.jpg}{evaluation/2022-04-07_141933_73_T5_2522_0_no_detection_concat.jpg}{evaluation/2022-04-07_141933_73_T5_2522_0_no_detection_cross.jpg}{evaluation/2022-04-07_141933_73_T5_2522_0_evaluation_best.jpg}\vspace{\sepYa}
\Row{evaluation/2022-04-15_084806_41_T3_2436.jpg}{evaluation/2022-04-15_084806_41_T5_2438.jpg}{evaluation/2022-04-15_084806_41_T5_2438_4_no_detection_id.jpg}{evaluation/2022-04-15_084806_41_T5_2438_4_no_detection_id_white.jpg}{evaluation/2022-04-15_084806_41_T5_2438_4_evaluation_concat.jpg}{evaluation/2022-04-15_084806_41_T5_2438_4_evaluation_cross.jpg}{evaluation/2022-04-15_084806_41_T5_2438_4_evaluation_best.jpg}
\noindent
\makebox[\colwa][c]{\small Photo}\hspace{\sepSa}%
\makebox[\colwa][c]{\small Ground Truth}\hspace{\sepLa}%
\makebox[\colwa][c]{\small Yolo11-White}\hspace{\sepSa}%
\makebox[\colwa][c]{\small Yolo11-Blue}\hspace{\sepSa}%
\makebox[\colwa][c]{\parbox{\colwa}{\centering \small Dual Yolo\\Channel Concat}}%
\makebox[\colwa][c]{\parbox{\colwa}{\centering \small Dual Yolo\\Adaptive Weighted}}%
\makebox[\colwa][c]{\parbox{\colwa}{\centering \small Dual Yolo\\Cross Attention}}%
\caption{Demonstrations of segmentation results from different models for the Blood Fractionation Component Segmentation task. In the figure, from top to bottom, the components are plasma, buffy coat, and red blood cell, respectively. From the third column, yellow, green, and blue points represent the segmented regions for each class. The regions outlined by white contours are the boundaries formed by the respective methods. From left to right, the columns display the following: original photos for the testing tube, ground truth annotated by professional blood testing personnel, Yolo11-White result, Yolo11-Blue result, Dual Yolo Channel Concatenation Fusion result, Dual Yolo Adaptive Weighted Fusion result, and Dual Yolo Cross Attention result.}
\label{fig:evaluation-grid}
\end{figure*}

\subsection{Qualitative Visualization}

Figure~\ref{fig:evaluation-grid} presents representative segmentation results across four test samples, comparing ground truth annotations against predictions from all methods. The visualizations reveal both qualitative performance differences and failure modes.

\textbf{Plasma Segmentation.} All dual-modality methods produce accurate serum/plasma boundaries closely matching ground truth clinical annotation points (rows 1--4, columns 5--7). Yolo11-White occasionally generates irregular upper boundaries due to meniscus reflection artifacts (row 1, column 3), while Yolo11-Blue exhibits slight over-segmentation extending into the buffy coat region (row 2, column 4). Cross-attention fusion produces the most consistent boundaries, effectively suppressing both artifact types through complementary information integration.

\textbf{Buffy Coat Detection.} The thin buffy coat layer (green regions) presents the greatest challenge, with substantial inter-method variation. Yolo11-White frequently fails to detect this layer entirely (rows 1--4, column 3), consistent with its 8.06\% buffy coat DSR. Yolo11-Blue performs better but occasionally produces fragmented detections or merged boundaries with adjacent layers. Concatenation and weighted fusion methods show improved consistency but exhibit thickness estimation errors—either compressing the layer too thinly or expanding into neighboring regions.

Cross-attention fusion (column 7) demonstrates superior buffy coat localization across all samples, producing contiguous masks with accurate upper and lower boundaries. The white boundary contours closely align with ground truth annotations, with typical deviations under 5 pixels—sufficient for clinical volume estimation. This consistent performance across varying buffy coat thicknesses (ranging from $\sim$15 pixels to $\sim$35 pixels) validates the attention mechanism's ability to adaptively leverage blue-light contrast enhancement while consulting white-light structural context.

\textbf{Erythrocyte Segmentation.} All methods achieve accurate erythrocyte layer detection (blue regions) in most cases, reflecting this class's high visual distinctiveness. However, single-modality methods occasionally exhibit lower boundary estimation errors when tube bottoms create optical discontinuities. Dual-modality methods consistently produce stable lower boundaries by integrating complementary depth cues.

\section{Discussion}

\subsection{Clinical Significance}
The DIUA-YOLO framework achieves 98.80\% overall detection success rate at clinical operating threshold (conf=0.25), meeting the stringent requirement for automated blood tube analysis. The sub-5-pixel boundary localization accuracy translates to volume estimation errors below 0.5\% for typical 10mL tubes—well within acceptable clinical tolerance. Most critically, the 96.39\% buffy coat detection rate represents a transformative improvement for white blood cell quantification, as this layer contains leukocytes essential for immune system assessment and disease diagnosis.

The automated platform processes a complete tube (dual-illumination acquisition, inference, result reporting) in approximately 8 seconds—enabling throughput exceeding 450 tubes per hour. This represents a 3.75$\times$ speedup over manual visual inspection and layer annotation workflows (typically 30 seconds per tube), while eliminating inter-operator variability. For high-volume clinical laboratories processing thousands of samples daily, such automation reduces greatly labor requirements, translating to substantial cost savings while improving result consistency and throughput.

Integration with the mechanical gripper system enables full workflow automation from tube loading to result reporting. The SQLite database logging provides full traceability, while the platform's modular design allows deployment in standard laboratory environments without specialized infrastructure. However, comprehensive clinical field deployment and validation remain essential next steps. Future work should conduct prospective studies in operational clinical laboratories to assess system robustness under real-world conditions, including diverse operator workflows, environmental variations, and sustained high-throughput operation.

\subsection{Understanding Cross-Modal Fusion Performance}

The substantial performance gap between fusion strategies—particularly cross-attention's superiority over concatenation and weighted fusion—can be attributed to their differing abilities to handle spatial misalignment between dual-modality inputs. Despite mechanical fixture stabilization, residual misalignment inevitably arises from parallax effects, chromatic aberration, and temporal offset during sequential illumination switching. These subtle displacements, critically affect boundary-sensitive detection tasks where layer interfaces span only a few pixels.

Channel concatenation operates under a strict pixel-to-pixel correspondence assumption, directly merging features at identical spatial coordinates. This rigid alignment makes concatenation vulnerable to misregistration: a blue-light edge feature may correspond to slightly shifted white-light context, causing the subsequent convolutional layers to receive semantically inconsistent input. Adaptive weighted fusion partially mitigates this limitation through learned spatial attention, enabling the network to suppress contributions from misaligned regions. However, its pixel-wise weighting remains constrained to fixed spatial positions, preventing active search for correct correspondences.

Cross-modal attention fundamentally addresses misalignment through its token-based neighborhood mechanism. By partitioning features into tokens and allowing each blue-light query token to attend to a local neighborhood of white-light key tokens, the attention operation performs implicit local feature alignment. When a blue-light boundary token encounters spatial offset, it can retrieve semantically matching white-light features from neighboring positions within the $k \times k$ window rather than being forced to use the strictly co-located feature. This spatial flexibility is particularly critical for thin-layer boundaries such as the buffy coat, where minor pixel misalignment can shift edge responses beyond concatenation's receptive field but remain within attention's neighborhood range. The localized search mechanism thus enables robust cross-modal correspondence even under imperfect registration, explaining the dramatic performance improvements observed in ablation studies.

\subsection{Limitations and Future Directions}

Despite strong performance, several limitations warrant further investigation. First, the current system assumes pre-centrifuged tubes with established layer stratification. Fresh whole blood or partially centrifuged samples may not exhibit clear boundaries, potentially degrading detection performance. Future work should investigate time-series imaging during centrifugation to model dynamic layer formation.

Second, the dataset comprises 90 samples covering typical clinical scenarios but may not fully represent rare pathological conditions such as severe anemia (reduced erythrocyte layer), leukemia (expanded buffy coat), or hemolysis (layer disruption). Expanding the dataset to include rare disease phenotypes and conducting clinical validation studies across diverse patient populations remains essential for regulatory approval and widespread deployment.

Third, while dual-illumination imaging demonstrates clear advantages, the sequential white-light and blue-light acquisition requires 2--3 seconds, introducing potential motion artifacts if tubes are not mechanically stabilized. Simultaneous dual-spectrum imaging using beam-splitting optics or specialized sensors could reduce acquisition time and eliminate registration errors, though at increased hardware complexity and cost.

Fourth, the high-resolution dual-modality input (1504$\times$1504 pixels, 6 channels) demands substantial GPU memory during training, limiting batch size to 2 per RTX 3090 GPU. While the cross-modal attention mechanism itself is computationally efficient relative to global attention, the large spatial dimensions necessitate careful memory management. Future work could explore input resolution reduction strategies, gradient checkpointing, or mixed-precision training to increase batch sizes and potentially improve convergence. Additionally, model compression techniques such as knowledge distillation or quantization-aware training could enable deployment on edge devices for point-of-care settings.

Fifth, current evaluation focuses on layer detection and boundary localization but does not perform cell counting or morphological analysis within the buffy coat layer. Integrating microscopic imaging or cell classification sub-networks could provide complete blood count (CBC) equivalent information, enhancing clinical utility. Preliminary experiments with SAM-based cell segmentation within detected buffy coat regions show promise but require substantial annotation effort.

Finally, the system currently operates on vacuum blood collection tubes, which maintain consistent geometry. Extending to capillary tubes, micro-centrifuge tubes, or non-standard containers would require retraining and potentially redesigning the gripper mechanism. Transfer learning experiments on these alternative tube types represent important future validation.

\subsection{Broader Impact}

Beyond blood tube analysis, the dual-illumination cross-attention architecture demonstrates general applicability to multi-modal medical imaging tasks where complementary spectral information enhances diagnostic accuracy. Potential extensions include dual-modality endoscopy (white-light + narrow-band imaging for polyp detection), dermatological imaging (visible + UV for skin lesion analysis), and histopathology (H\&E + immunofluorescence for tissue classification).

The unidirectional attention design and token-based spatial partitioning offer computational advantages for high-resolution medical images ($\geq$1500$\times$1500 pixels), where global attention becomes prohibitively expensive. By constraining attention to local neighborhoods adaptive to pyramid level, the framework scales to large images while maintaining fine-grained feature interactions—addressing a key bottleneck in medical image analysis.

From a methodological perspective, this work contributes to the broader understanding of multi-modal fusion in deep learning. The comparative evaluation of concatenation, weighted fusion, and attention mechanisms provides empirical evidence that explicit correspondence modeling through attention yields superior performance compared to implicit fusion approaches, particularly for challenging detection tasks involving thin or low-contrast structures. These insights generalize beyond medical imaging to applications such as RGB-D fusion for robotics, multi-spectral satellite imagery analysis, and sensor fusion for autonomous systems.


\section{Conclusion}
\label{sec:conclusion}
This paper presents DIUA-YOLO, a dual-illumination detection and segmentation framework addressing the critical challenge of automated blood component stratification following centrifugation. Conventional approaches relying on single-modality imaging or manual inspection fail to reliably localize the optically thin buffy coat layer while maintaining high throughput. We address these limitations through a dual-backbone architecture that independently processes white-light and blue-light images, enabling complementary feature extraction tailored to each modality's optical characteristics. The key innovation lies in our cross-modal attention mechanism, which employs unidirectional queries and token-based localized attention to achieve effective feature fusion while reducing computational complexity by approximately 91\% compared to global attention. This design fundamentally addresses spatial misalignment between modalities through implicit local feature correspondence, enabling robust boundary detection even under imperfect registration, a critical advantage over rigid pixel-wise fusion methods.

Comprehensive evaluation demonstrates that DIUA-YOLO achieves 98.80\% overall detection success rate under stringent clinical criteria requiring exact detection of all three blood layers without false positives or negatives. The framework achieves near-perfect performance for plasma and erythrocyte layers, while critically improving buffy coat detection to 96.39\%, representing transformative advancement for white blood cell localization. Statistical analysis confirms all architectural improvements are highly significant (p$<$0.001), validating the necessity of dual-backbone design, superiority of attention-based fusion, and benefits of hierarchical transfer learning. The integrated automated platform processes complete tubes in 8 seconds, delivering 3.75$\times$ speedup over manual workflows while dramatically reducing labor requirements. Sub-5-pixel boundary localization accuracy ensures volume estimation errors below 0.5\% for clinical applications.

Beyond blood tube analysis, this work contributes methodological insights demonstrating that explicit correspondence modeling through localized attention outperforms rigid fusion approaches for thin-layer detection under spatial misalignment. Primary limitations include dataset scale and restriction to pre-centrifuged tubes. Future work should expand to rare pathological conditions, explore simultaneous dual-spectrum imaging, and integrate cell-level analysis capabilities. The proposed architecture demonstrates strong potential for extension to other dual-modality medical imaging tasks, contributing to the advancement of automated diagnostic systems in personalized medicine and laboratory informatics.




\subsection{Abbreviations and Acronyms}
Define abbreviations and acronyms the first time they are used in the text, 
even after they have already been defined in the abstract. Abbreviations 
such as IEEE, SI, ac, and dc do not have to be defined. Abbreviations that 
incorporate periods should not have spaces: write ``C.N.R.S.,'' not ``C. N. 
R. S.'' Do not use abbreviations in the title unless they are unavoidable 
(for example, ``IEEE'' in the title of this article).

\subsection{Other Recommendations}
Use one space after periods and colons. Hyphenate complex modifiers: 
``zero-field-cooled magnetization.'' Avoid dangling participles, such as, 
``Using \eqref{eq}, the potential was calculated.'' [It is not clear who or what 
used \eqref{eq}.] Write instead, ``The potential was calculated by using \eqref{eq},'' or 
``Using \eqref{eq}, we calculated the potential.''

Use a zero before decimal points: ``0.25,'' not ``.25.'' Use 
``cm$^{3}$,'' not ``cc.'' Indicate sample dimensions as ``0.1 cm 
$\times $ 0.2 cm,'' not ``0.1 $\times $ 0.2 cm$^{2}$.'' The 
abbreviation for ``seconds'' is ``s,'' not ``sec.'' Use 
``Wb/m$^{2}$'' or ``webers per square meter,'' not 
``webers/m$^{2}$.'' When expressing a range of values, write ``7 to 
9'' or ``7--9,'' not ``7$\sim $9.''

A parenthetical statement at the end of a sentence is punctuated outside of 
the closing parenthesis (like this). (A parenthetical sentence is punctuated 
within the parentheses.) In American English, periods and commas are within 
quotation marks, like ``this period.'' Other punctuation is ``outside''! 
Avoid contractions; for example, write ``do not'' instead of ``don't.'' The 
serial comma is preferred: ``A, B, and C'' instead of ``A, B and C.''

If you wish, you may write in the first person singular or plural and use 
the active voice (``I observed that $\ldots$'' or ``We observed that $\ldots$'' 
instead of ``It was observed that $\ldots$''). Remember to check spelling. 


Try not to use too many typefaces in the same article. Please remember that MathJax
cannot handle nonstandard typefaces.

\subsection{Equations}
Number equations consecutively with equation numbers in parentheses flush 
with the right margin, as in \eqref{eq}. To make your equations more 
compact, you may use the solidus (~/~), the exp function, or appropriate 
exponents. Use parentheses to avoid ambiguities in denominators. Punctuate 
equations when they are part of a sentence, as in
\begin{equation}E=mc^2.\label{eq}\end{equation}

Be sure that the symbols in your equation have been defined before the 
equation appears or immediately following. Italicize symbols ($T$ might refer 
to temperature, but T is the unit tesla). Refer to ``\eqref{eq},'' not ``Eq. \eqref{eq}'' 
or ``equation \eqref{eq},'' except at the beginning of a sentence: ``Equation \eqref{eq} 
is $\ldots$ .''


\subsection{Algorithms}
Algorithms should be numbered and include a short title.
They are set off from the text with rules above and below the title and after the last line.
\begin{algorithm}[H]
\caption{Weighted Tanimoto ELM.}\label{alg:alg1}
\begin{algorithmic}
\STATE 
\STATE {\textsc{TRAIN}}$(\mathbf{X} \mathbf{T})$
\STATE \hspace{0.5cm}$ \textbf{select randomly } W \subset \mathbf{X}  $
\STATE \hspace{0.5cm}$ N_\mathbf{t} \gets | \{ i : \mathbf{t}_i = \mathbf{t} \} | $ \textbf{ for } $ \mathbf{t}= -1,+1 $
\STATE \hspace{0.5cm}$ B_i \gets \sqrt{ \textsc{max}(N_{-1},N_{+1}) / N_{\mathbf{t}_i} } $ \textbf{ for } $ i = 1,...,N $
\STATE \hspace{0.5cm}$ \hat{\mathbf{H}} \gets  B \cdot (\mathbf{X}^T\textbf{W})/( \mathbb{1}\mathbf{X} + \mathbb{1}\textbf{W} - \mathbf{X}^T\textbf{W} ) $
\STATE \hspace{0.5cm}$ \beta \gets \left ( I/C + \hat{\mathbf{H}}^T\hat{\mathbf{H}} \right )^{-1}(\hat{\mathbf{H}}^T B\cdot \mathbf{T})  $
\STATE \hspace{0.5cm}\textbf{return} $\textbf{W},  \beta $
\STATE 
\STATE {\textsc{PREDICT}}$(\mathbf{X} )$
\STATE \hspace{0.5cm}$ \mathbf{H} \gets  (\mathbf{X}^T\textbf{W} )/( \mathbb{1}\mathbf{X}  + \mathbb{1}\textbf{W}- \mathbf{X}^T\textbf{W}  ) $
\STATE \hspace{0.5cm}\textbf{return}  $\textsc{sign}( \mathbf{H} \beta )$
\end{algorithmic}
\label{alg1}
\end{algorithm}
\subsection{\LaTeX-Specific Advice}

Please use ``soft'' (e.g., \verb|\eqref{Eq}|) cross references instead
of ``hard'' references (e.g., \verb|(1)|). That will make it possible
to combine sections, add equations, or change the order of figures or
citations without having to go through the file line by line.

Please don't use the \verb|{eqnarray}| equation environment. Use
\verb|{align}| or \verb|{IEEEeqnarray}| instead. The \verb|{eqnarray}|
environment leaves unsightly spaces around relation symbols.

Please note that the \verb|{subequations}| environment in {\LaTeX}
will increment the main equation counter even when there are no
equation numbers displayed. If you forget that, you might write an
article in which the equation numbers skip from (17) to (20), causing
the copy editors to wonder if you've discovered a new method of
counting.

{\BibTeX} gets the bibliographic
data from .bib files. If you use {\BibTeX} to produce a
bibliography you must send the .bib files. 

If you assign the same label to a
subsubsection and a table, you might find that Table I has been cross
referenced as Table IV-B3. 

If you put a
\verb|\label| command before the command that updates the counter it's
supposed to be using, the label will pick up the last counter to be
cross referenced instead. In particular, a \verb|\label| command
should not go before the caption of a figure or a table.

Do not use \verb|\nonumber| inside the \verb|{array}| environment. It
will not stop equation numbers inside \verb|{array}| (there won't be
any anyway) and it might stop a wanted equation number in the
surrounding equation.

If you are submitting your paper to a colorized journal, you can use
the following two lines at the start of the article to ensure its
appearance resembles the final copy:

\smallskip\noindent
\begin{small}
\begin{tabular}{l}
\verb+\+\texttt{documentclass[journal,twoside,web]\{ieeecolor\}}\\
\verb+\+\texttt{usepackage\{\textit{Journal\_Name}\}}
\end{tabular}
\end{small}

\section{Units}
Use either SI (MKS) or CGS as primary units. (SI units are strongly 
encouraged.) English units may be used as secondary units (in parentheses). 
This applies to papers in data storage. For example, write ``15 
Gb/cm$^{2}$ (100 Gb/in$^{2})$.'' An exception is when 
English units are used as identifiers in trade, such as ``3\textonehalf-in 
disk drive.'' Avoid combining SI and CGS units, such as current in amperes 
and magnetic field in oersteds. This often leads to confusion because 
equations do not balance dimensionally. If you must use mixed units, clearly 
state the units for each quantity in an equation.

The SI unit for magnetic field strength $H$ is A/m. However, if you wish to use 
units of T, either refer to magnetic flux density $B$ or magnetic field 
strength symbolized as $\mu _{0}H$. Use the center dot to separate 
compound units, e.g., ``A$\cdot $m$^{2}$.''

\section{Some Common Mistakes}
The word ``data'' is plural, not singular. The subscript for the 
permeability of vacuum $\mu _{0}$ is zero, not a lowercase letter 
``o.'' The term for residual magnetization is ``remanence''; the adjective 
is ``remanent''; do not write ``remnance'' or ``remnant.'' Use the word 
``micrometer'' instead of ``micron.'' A graph within a graph is an 
``inset,'' not an ``insert.'' The word ``alternatively'' is preferred to the 
word ``alternately'' (unless you really mean something that alternates). Use 
the word ``whereas'' instead of ``while'' (unless you are referring to 
simultaneous events). Do not use the word ``essentially'' to mean 
``approximately'' or ``effectively.'' Do not use the word ``issue'' as a 
euphemism for ``problem.'' When compositions are not specified, separate 
chemical symbols by en-dashes; for example, ``NiMn'' indicates the 
intermetallic compound Ni$_{0.5}$Mn$_{0.5}$ whereas 
``Ni--Mn'' indicates an alloy of some composition 
Ni$_{x}$Mn$_{1-x}$.

\begin{figure}[!t]
\centerline{\includegraphics[width=\columnwidth]{fig1.png}}
\caption{Magnetization as a function of applied field.
It is good practice to explain the significance of the figure in the caption.}
\label{fig1}
\end{figure}

Be aware of the different meanings of the homophones ``affect'' (usually a 
verb) and ``effect'' (usually a noun), ``complement'' and ``compliment,'' 
``discreet'' and ``discrete,'' ``principal'' (e.g., ``principal 
investigator'') and ``principle'' (e.g., ``principle of measurement''). Do 
not confuse ``imply'' and ``infer.'' 

Prefixes such as ``non,'' ``sub,'' ``micro,'' ``multi,'' and ``ultra'' are 
not independent words; they should be joined to the words they modify, 
usually without a hyphen. There is no period after the ``et'' in the Latin 
abbreviation ``\emph{et al.}'' (it is also italicized). The abbreviation ``i.e.,'' means 
``that is,'' and the abbreviation ``e.g.,'' means ``for example'' (these 
abbreviations are not italicized).

IEEE styleguides are available at
https://journals.\discretionary{}{}{}ieeeauthorcenter.ieee.org/create-your-ieee-journal-article/\discretionary{}{}{}create-the-text-of-your-article/\discretionary{}{}{}ieee-editorial-style-manual/.

\section{Guidelines for Graphics Preparation and Submission}
\label{sec:guidelines}

\subsection{Types of Graphics}
The following list outlines the different types of graphics published in 
IEEE journals. They are categorized based on their construction, and use of 
color/shades of gray:

\subsubsection{Color/Grayscale figures}
{Figures that are meant to appear in color, or shades of black/gray. Such 
figures may include photographs, illustrations, multicolor graphs, and 
flowcharts.}

\subsubsection{Line Art figures}
{Figures that are composed of only black lines and shapes. These figures 
should have no shades or half-tones of gray, only black and white.}

\subsubsection{Author photos}
{Head and shoulders shots of authors that appear at the end of our papers. }

\subsubsection{Tables}
{Data charts which are typically black and white, but sometimes include 
color.}

\begin{table}
\caption{Units for Magnetic Properties}
\label{table}
\setlength{\tabcolsep}{3pt}
\begin{tabular}{|p{25pt}|p{75pt}|p{115pt}|}
\hline
Symbol& 
Quantity& 
Conversion from Gaussian and \par CGS EMU to SI $^{\mathrm{a}}$ \\
\hline
$\Phi $& 
magnetic flux& 
1 Mx $\to  10^{-8}$ Wb $= 10^{-8}$ V$\cdot $s \\
$B$& 
magnetic flux density, \par magnetic induction& 
1 G $\to  10^{-4}$ T $= 10^{-4}$ Wb/m$^{2}$ \\
$H$& 
magnetic field strength& 
1 Oe $\to  10^{3}/(4\pi )$ A/m \\
$m$& 
magnetic moment& 
1 erg/G $=$ 1 emu \par $\to 10^{-3}$ A$\cdot $m$^{2} = 10^{-3}$ J/T \\
$M$& 
magnetization& 
1 erg/(G$\cdot $cm$^{3}) =$ 1 emu/cm$^{3}$ \par $\to 10^{3}$ A/m \\
4$\pi M$& 
magnetization& 
1 G $\to  10^{3}/(4\pi )$ A/m \\
$\sigma $& 
specific magnetization& 
1 erg/(G$\cdot $g) $=$ 1 emu/g $\to $ 1 A$\cdot $m$^{2}$/kg \\
$j$& 
magnetic dipole \par moment& 
1 erg/G $=$ 1 emu \par $\to 4\pi \times  10^{-10}$ Wb$\cdot $m \\
$J$& 
magnetic polarization& 
1 erg/(G$\cdot $cm$^{3}) =$ 1 emu/cm$^{3}$ \par $\to 4\pi \times  10^{-4}$ T \\
$\chi , \kappa $& 
susceptibility& 
1 $\to  4\pi $ \\
$\chi_{\rho }$& 
mass susceptibility& 
1 cm$^{3}$/g $\to  4\pi \times  10^{-3}$ m$^{3}$/kg \\
$\mu $& 
permeability& 
1 $\to  4\pi \times  10^{-7}$ H/m \par $= 4\pi \times  10^{-7}$ Wb/(A$\cdot $m) \\
$\mu_{r}$& 
relative permeability& 
$\mu \to \mu_{r}$ \\
$w, W$& 
energy density& 
1 erg/cm$^{3} \to  10^{-1}$ J/m$^{3}$ \\
$N, D$& 
demagnetizing factor& 
1 $\to  1/(4\pi )$ \\
\hline
\multicolumn{3}{p{251pt}}{Vertical lines are optional in tables. Statements that serve as captions for 
the entire table do not need footnote letters. }\\
\multicolumn{3}{p{251pt}}{$^{\mathrm{a}}$Gaussian units are the same as cg emu for magnetostatics; Mx 
$=$ maxwell, G $=$ gauss, Oe $=$ oersted; Wb $=$ weber, V $=$ volt, s $=$ 
second, T $=$ tesla, m $=$ meter, A $=$ ampere, J $=$ joule, kg $=$ 
kilogram, H $=$ henry.}
\end{tabular}
\label{tab1}
\end{table}


\subsection{Multipart figures}
Figures compiled of more than one sub-figure presented side-by-side, or 
stacked. If a multipart figure is made up of multiple figure
types (one part is lineart, and another is grayscale or color) the figure 
should meet the stricter guidelines.

\subsection{File Formats For Graphics}\label{formats}
Format and save your graphics using a suitable graphics processing program 
that will allow you to create the images as PostScript (PS), Encapsulated 
PostScript (.EPS), Tagged Image File Format (.TIFF), Portable Document 
Format (.PDF), Portable Network Graphics (.PNG), or Metapost (.MPS), sizes them, and adjusts 
the resolution settings. When 
submitting your final paper, your graphics should all be submitted 
individually in one of these formats along with the manuscript.

\subsection{Sizing of Graphics}
Most charts, graphs, and tables are one column wide (3.5 inches/88 
millimeters/21 picas) or page wide (7.16 inches/181 millimeters/43 
picas). The maximum depth a graphic can be is 8.5 inches (216 millimeters/54
picas). When choosing the depth of a graphic, please allow space for a 
caption. Figures can be sized between column and page widths if the author 
chooses, however it is recommended that figures are not sized less than 
column width unless when necessary. 

There is currently one publication with column measurements that do not 
coincide with those listed above. Proceedings of the IEEE has a column 
measurement of 3.25 inches (82.5 millimeters/19.5 picas). 

The final printed size of author photographs is exactly
1 inch wide by 1.25 inches tall (25.4 millimeters$\,\times\,$31.75 millimeters/6 
picas$\,\times\,$7.5 picas). Author photos printed in editorials measure 1.59 inches 
wide by 2 inches tall (40 millimeters$\,\times\,$50 millimeters/9.5 picas$\,\times\,$12 
picas).

\subsection{Resolution }
The proper resolution of your figures will depend on the type of figure it 
is as defined in the ``Types of Figures'' section. Author photographs, 
color, and grayscale figures should be at least 300dpi. Line art, including 
tables should be a minimum of 600dpi.

\subsection{Vector Art}
In order to preserve the figures' integrity across multiple computer 
platforms, we accept files in the following formats: .EPS/.PDF/.PS. All 
fonts must be embedded or text converted to outlines in order to achieve the 
best-quality results.

\subsection{Color Space}
The term color space refers to the entire sum of colors that can be 
represented within the said medium. For our purposes, the three main color 
spaces are Grayscale, RGB (red/green/blue) and CMYK 
(cyan/magenta/yellow/black). RGB is generally used with on-screen graphics, 
whereas CMYK is used for printing purposes.

All color figures should be generated in RGB or CMYK color space. Grayscale 
images should be submitted in Grayscale color space. Line art may be 
provided in grayscale OR bitmap colorspace. Note that ``bitmap colorspace'' 
and ``bitmap file format'' are not the same thing. When bitmap color space 
is selected, .TIF/.TIFF/.PNG are the recommended file formats.

\subsection{Accepted Fonts Within Figures}
When preparing your graphics IEEE suggests that you use of one of the 
following Open Type fonts: Times New Roman, Helvetica, Arial, Cambria, and 
Symbol. If you are supplying EPS, PS, or PDF files all fonts must be 
embedded. Some fonts may only be native to your operating system; without 
the fonts embedded, parts of the graphic may be distorted or missing.

A safe option when finalizing your figures is to strip out the fonts before 
you save the files, creating ``outline'' type. This converts fonts to 
artwork what will appear uniformly on any screen.

\subsection{Using Labels Within Figures}

\subsubsection{Figure Axis labels }
Figure axis labels are often a source of confusion. Use words rather than 
symbols. As an example, write the quantity ``Magnetization,'' or 
``Magnetization M,'' not just ``M.'' Put units in parentheses. Do not label 
axes only with units. As in Fig. 1, for example, write ``Magnetization 
(A/m)'' or ``Magnetization (A$\cdot$m$^{-1}$),'' not just ``A/m.'' Do not label axes with a ratio of quantities and 
units. For example, write ``Temperature (K),'' not ``Temperature/K.'' 

Multipliers can be especially confusing. Write ``Magnetization (kA/m)'' or 
``Magnetization (10$^{3}$ A/m).'' Do not write ``Magnetization 
(A/m)$\,\times\,$1000'' because the reader would not know whether the top 
axis label in Fig. 1 meant 16000 A/m or 0.016 A/m. Figure labels should be 
legible, approximately 8 to 10 point type.

\subsubsection{Subfigure Labels in Multipart Figures and Tables}
Multipart figures should be combined and labeled before final submission. 
Labels should appear centered below each subfigure in 8 point Times New 
Roman font in the format of (a) (b) (c). 

\subsection{File Naming}
Figures (line artwork or photographs) should be named starting with the 
first 5 letters of the author's last name. The next characters in the 
filename should be the number that represents the sequential 
location of this image in your article. For example, in author 
``Anderson's'' paper, the first three figures would be named ander1.tif, 
ander2.tif, and ander3.ps.

Tables should contain only the body of the table (not the caption) and 
should be named similarly to figures, except that `.t' is inserted 
in-between the author's name and the table number. For example, author 
Anderson's first three tables would be named ander.t1.tif, ander.t2.ps, 
ander.t3.eps.

Author photographs should be named using the first five characters of the 
pictured author's last name. For example, four author photographs for a 
paper may be named: oppen.ps, moshc.tif, chen.eps, and duran.pdf.

If two authors or more have the same last name, their first initial(s) can 
be substituted for the fifth, fourth, third$\ldots$ letters of their surname 
until the degree where there is differentiation. For example, two authors 
Michael and Monica Oppenheimer's photos would be named oppmi.tif, and 
oppmo.eps.

\subsection{Referencing a Figure or Table Within Your Paper}
When referencing your figures and tables within your paper, use the 
abbreviation ``Fig.'' even at the beginning of a sentence. Do not abbreviate 
``Table.'' Tables should be numbered with Roman Numerals.

\subsection{Submitting Your Graphics}
Because IEEE will do the final formatting of your paper,
you do not need to position figures and tables at the top and bottom of each 
column. In fact, all figures, figure captions, and tables can be placed at 
the end of your paper. In addition to, or even in lieu of submitting figures 
within your final manuscript, figures should be submitted individually, 
separate from the manuscript in one of the file formats listed above in 
Section \ref{formats}. Place figure captions below the figures; place table titles 
above the tables. Please do not include captions as part of the figures, or 
put them in ``text boxes'' linked to the figures. Also, do not place borders 
around the outside of your figures.

\subsection{Color Processing/Printing in IEEE Journals}
All IEEE Transactions, Journals, and Letters allow an author to publish color figures on IEEE Xplore at no charge, and automatically convert them to grayscale for print versions. In most journals, figures and tables may alternatively be printed in color if an author chooses to do so. Please note that this service comes at an extra expense to the author. If you intend to have print color graphics, you will have the opportunity to indicate this in the Author Gateway and will be contacted by PubOps to confirm the charges. Online-only journals will have their figures appear in color, free of charge 

\section{Conclusion}
A conclusion section is not required. Although a conclusion may review the 
main points of the paper, do not replicate the abstract as the conclusion. A 
conclusion might elaborate on the importance of the work or suggest 
applications and extensions. 

\appendices

Appendixes, if needed, appear before the acknowledgment.

\section*{References and Footnotes}

\subsection{References}
References need not be cited in text. When they are, they appear on the
line, in square brackets, inside the punctuation. Multiple references are
each numbered with separate brackets. When citing a section in a book,
please give the relevant page numbers. In text, refer simply to the
reference number. Do not use ``Ref.'' or ``reference'' except at the
beginning of a sentence: ``Reference [3] shows \textellipsis .'' Please do not use
automatic endnotes in \textit{Word}, rather, type the reference list at the end of the
paper using the ``References'' style.

Reference numbers are set flush left and form a column of their own, hanging
out beyond the body of the reference. The reference numbers are on the line,
enclosed in square brackets. In all references, the given name of the author
or editor is abbreviated to the initial only and precedes the last name. Use
them all; use \textit{et al}.\ only if names are not given.
Abbreviate conference titles. When citing IEEE Transactions,
provide the issue number, page range, volume number, year, and/or month if
available. When referencing a patent, provide the day and the month of
issue, or application. References may not include all information; please
obtain and include relevant information. Do not combine references. There
must be only one reference with each number. If there is a URL included with
the print reference, it can be included at the end of the reference.

Other than books, capitalize only the first word in a paper title, except
for proper nouns and element symbols. For papers published in translation
journals, please give the English citation first, followed by the original
foreign-language citation. See the end of this document for formats and
examples of common references. For a complete discussion of references and
their formats, see the IEEE style manual at 
https://\discretionary{}{}{}journals.ieeeauthorcenter.ieee.org/\discretionary{}{}{}create-your-ieee-journal-article/\discretionary{}{}{}create-the-text-of-your-article/\discretionary{}{}{}ieee-editorial-style-manual/.


\subsection{Footnotes}
Number footnotes separately in superscript numbers.\footnote{It is recommended that footnotes be avoided (except for 
the unnumbered footnote with the receipt date on the first page). Instead, 
try to integrate the footnote information into the text.} Place the actual 
footnote at the bottom of the column in which it is cited; do not put 
footnotes in the reference list (endnotes). Use letters for table footnotes 
(see Table \ref{table}).

\section*{Submitting Your Paper for Review}           

\subsection{Review Stage Using IEEE ScholarOne Manuscripts}

Contributions to the Transactions, Journals, and Letters may be submitted electronically on IEEE ScholarOne Manuscripts. You can get help choosing the correct publication for your manuscript as well as find their corresponding peer review site using the tools listed at http://\discretionary{}{}{}www.ieee.org/\discretionary{}{}{}publications\_standards/\discretionary{}{}{}publications/\discretionary{}{}{}authors/\discretionary{}{}{}authors\_submission.html. Once you have chosen your publication and navigated to the IEEE ScholarOne Manuscripts site, you may log in with your IEEE web account. If there is none, please create a new account. After logging in, go to your Author Center and click ``Start New Submission.''

Along with other information, you will be asked to select the manuscript type from the journal's pre-determined list of options. Depending on the journal, there are various steps to the submission process; please make sure to carefully answer all of the submission questions presented to you. At the end of each step you must click ``Save and Continue''; just uploading the paper is not sufficient. After the last step, you should see a confirmation that the submission is complete. You should also receive an e-mail confirmation. For inquiries regarding the submission of your paper on IEEE ScholarOne Manuscripts, please contact oprs-support@ieee.org or call +1 732 465 5861.

IEEE ScholarOne Manuscripts will accept files for review in various formats. There is a ``Journal Home'' link on the log-in page of each IEEE ScholarOne Manuscripts site that will bring you to the journal's homepage with their detailed requirements; please check these guidelines for your particular journal before you submit.

\subsection{Final Stage Using IEEE ScholarOne Manuscripts}
Upon acceptance, you will receive an email with specific instructions
regarding the submission of your final files. To avoid any delays in
publication, please be sure to follow these instructions. Most journals
require that final submissions be uploaded through IEEE ScholarOne Manuscripts,
although some may still accept final submissions via email. Final
submissions should include source files of your accepted manuscript, high
quality graphic files, and a formatted pdf file. If you have any questions
regarding the final submission process, please contact the administrative
contact for the journal.

In addition to this, upload a file with complete contact information for all
authors. Include full mailing addresses, telephone numbers, fax numbers, and
e-mail addresses. Designate the author who submitted the manuscript on
IEEE ScholarOne Manuscripts as the ``corresponding author.'' This is the only
author to whom proofs of the paper will be sent.

\subsection{Copyright Form}
Authors must submit an electronic IEEE Copyright Form (eCF) upon submitting 
their final manuscript files. You can access the eCF system through your 
manuscript submission system or through the Author Gateway. You are 
responsible for obtaining any necessary approvals and/or security 
clearances. For additional information on intellectual property rights, 
visit the IEEE Intellectual Property Rights department web page at 
\underline{http://www.ieee.org/publications\_standards/publications/rights/}\discretionary{}{}{}\underline{index.html}.

\section*{IEEE Publishing Policy}
The general IEEE policy requires that authors should only submit original 
work that has neither appeared elsewhere for publication, nor is under 
review for another refereed publication. The submitting author must disclose 
all prior publication(s) and current submissions when submitting a 
manuscript. Do not publish ``preliminary'' data or results. The submitting 
author is responsible for obtaining agreement of all coauthors and any 
consent required from employers or sponsors before submitting an article. 
The IEEE Transactions and Journals Department strongly discourages courtesy 
authorship; it is the obligation of the authors to cite only relevant prior 
work.

The IEEE Transactions and Journals Department does not publish conference 
records or proceedings, but can publish articles related to conferences that 
have undergone rigorous peer review. Minimally, two reviews are required for 
every article submitted for peer review.

\section*{Acknowledgment}

The preferred spelling of the word ``acknowledgment'' in American English is 
without an ``e'' after the ``g.'' Use the singular heading even if you have 
many acknowledgments. Avoid expressions such as ``One of us (S.B.A.) would 
like to thank $\ldots$ .'' Instead, write ``F. A. Author thanks $\ldots$ .'' In most 
cases, sponsor and financial support acknowledgments are placed in the 
unnumbered footnote on the first page, not here.

\section*{References}

\def\refname{\vadjust{\vspace*{-2.5em}}} %Please don't do this in a real paper.

\noindent\textit{Basic format for books:}

\noindent J. K. Author, ``Title of chapter in the book,'' in {\it Title of His Published Book, x}th ed. City of Publisher,
(only U.S. State), Country: Abbrev. of Publisher, year, ch. $x$, sec. $x$,\break pp.~{\it xxx--xxx.}

{\it Examples:}{\vadjust{\vspace*{-2.5em}}}
\begin{thebibliography}{00}
\bibitem{bib1} G. O. Young, ``Synthetic structure of industrial plastics,'' in {\it Plastics,}2$^{\mathrm{nd}}$ ed., vol. 3, J. Peters, Ed. New York, NY, USA: McGraw-Hill, 1964, pp. 15--64.
\bibitem{bib2} W.-K. Chen, {\it Linear Networks and Systems.}Belmont, CA, USA: Wadsworth, 1993, pp. 123--135.
\end{thebibliography}

\noindent {\it Basic format for periodicals:}

\noindent J. K. Author, ``Name of paper,'' {\it Abbrev. Title of Periodical}, vol. {\it x, no}. $x, $pp{\it . xxx-xxx,}Abbrev. Month, year, DOI.
 {10.1109.} {{\it XXX}} {.123456}.

{\it Examples:}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib3} J. U. Duncombe, ``Infrared navigation---Part I: An assessment of feasibility,'' {\it IEEE Trans. Electron Devices}, vol. ED-11, no. 1, pp. 34--39, Jan. 1959, doi:.  {10.1109/TED.2016.2628402}.
\bibitem{bib4} E. P. Wigner, ``Theory of traveling-wave optical laser,''
{\it Phys. Rev}., vol. 134, pp. A635--A646, Dec. 1965, doi:  {10.1109.} {{\it XXX}} {.123456}.
\bibitem{bib5} E. H. Miller, ``A note on reflector arrays,'' {\it IEEE Trans. Antennas Propagat}., to be published.
\end{thebibliography}

\noindent {\it Basic format for reports:}

\noindent J. K. Author, ``Title of report,'' Abbrev. Name of Co., City of Co., Abbrev.
State, Country, Rep. {\it xxx}, year.

{\it Examples:}
\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib6} E. E. Reber, R. L. Michell, and C. J. Carter, ``Oxygen absorption in the earth's atmosphere,'' Aerospace Corp., Los Angeles, CA, USA, Tech. Rep. TR-0200 (4230-46)-3, Nov. 1988.
\bibitem{bib7} J. H. Davis and J. R. Cogdell, ``Calibration program for the 16-foot antenna,'' Elect. Eng. Res. Lab., Univ. Texas, Austin, TX, USA, Tech. Memo. NGL-006-69-3, Nov. 15, 1987.
\end{thebibliography}

\noindent {\it Basic format for handbooks:}

\noindent {\it Name of Manual/Handbook, x} ed., Abbrev. Name of Co., City of Co., Abbrev. State, Country, year, pp.
{\it xxx-xxx.}

{\it Examples:}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib8} {\it Transmission Systems for Communications}, 3rd ed., Western Electric Co., Winston-Salem, NC, USA, 1985, pp. 44--60.
\bibitem{bib9} {\it Motorola Semiconductor Data Manual}, Motorola Semiconductor Products Inc., Phoenix, AZ, USA, 1989.
\end{thebibliography}

\noindent {\it Basic format for books (when available online):}

\noindent J. K. Author, ``Title of chapter in the book,'' in {\it Title of Published Book}, $x$th ed. City of
Publisher, State, Country: Abbrev. of Publisher, year, ch.$x$, sec. $x$, pp.
{\it xxx--xxx}. [Online]. Available:  {http://www.web.com}




{\it Examples:}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib10} G. O. Young, ``Synthetic structure of industrial plastics,'' in Plastics, vol. 3, Polymers of Hexadromicon, J. Peters, Ed., 2nd ed. New York, NY, USA: McGraw-Hill, 1964, pp. 15-64. [Online]. Available:  {http://www.bookref.com}.
\bibitem{bib11} {\it The Founders' Constitution}, Philip B. Kurland and Ralph Lerner, eds., Chicago, IL, USA: Univ. Chicago Press, 1987. [Online]. Available:  {http://press-pubs.uchicago.edu/founders/}
\bibitem{bib12} The Terahertz Wave eBook. ZOmega Terahertz Corp., 2014. [Online]. Available:  {http://dl.z-thz.com/eBook/zomega\_ebook\_pdf\_1206\_sr.pdf}. Accessed on: May 19, 2014.
\bibitem{bib13} Philip B. Kurland and Ralph Lerner, eds., {\it The Founders' Constitution.}Chicago, IL, USA: Univ. of Chicago Press, 1987, Accessed on: Feb. 28, 2010, [Online] Available:  {http://press-pubs.uchicago.edu/founders/}
\end{thebibliography}

\noindent {\it Basic format for journals (when available online):}

\noindent J. K. Author, ``Name of paper,'' {\it Abbrev. Title of Periodical}, vol. $x$, no. $x$, pp. {\it xxx-xxx}, Abbrev. Month, year.
Accessed on: Month, Day, year, doi:  {10.1109.} {{\it
XXX}} {.123456}, [Online].

{\it Examples:}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib14} J. S. Turner, ``New directions in communications,'' {\it IEEE J. Sel. Areas Commun}., vol. 13, no. 1, pp. 11-23, Jan. 1995. DOI.  {10.1109.} {{\it XXX}} {.123456}.
\bibitem{bib15} W. P. Risk, G. S. Kino, and H. J. Shaw, ``Fiber-optic frequency shifter using a surface acoustic wave incident at an oblique angle,'' {\it Opt. Lett.}, vol. 11, no. 2, pp. 115--117, Feb. 1986, doi: {10.1109.} {{\it XXX}} {.123456}.
\bibitem{bib16} P. Kopyt {\it \textit{et al.}, ``}Electric properties of graphene-based conductive layers from DC up to terahertz range,'' {\it IEEE THz Sci. Technol.,}to be published, doi:  {10.1109/TTHZ.2016.2544142}.
\end{thebibliography}

\noindent {\it Basic format for papers presented at conferences (when available online):}

\noindent J.K. Author. (year, month). Title. presented at abbrev. conference title.
[Type of Medium]. Available: site/path/file
\\
\\
\\
{\it Example:}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib17} PROCESS Corporation, Boston, MA, USA. Intranets: Internet technologies deployed behind the firewall for corporate productivity. Presented at INET96 Annual Meeting. [Online]. Available:  {http://home.process.com/Intranets/wp2.htp}
\end{thebibliography}

\noindent {\it Basic format for reports and handbooks (when available online):}

\noindent J. K. Author. ``Title of report,'' Company. City, State, Country. Rep. no.,
(optional: vol./issue), Date. [Online] Available:\\
{site/path/file}\\

{\it Examples:}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib18} R. J. Hijmans and J. van Etten, ``Raster: Geographic analysis and modeling with raster data,'' R Package Version 2.0-12, Jan. 12, 2012. [Online]. Available:  {http://CRAN.R-project.org/package$=$raster} {}
\bibitem{bib19} Teralyzer. Lytera UG, Kirchhain, Germany [Online]. Available: http://www.lytera.de/Terahertz\_THz\_Spectroscopy.php?id$=$home, Accessed on: Jun. 5, 2014
\end{thebibliography}

\noindent {\it Basic format for computer programs and electronic documents (when available online):}

\noindent Legislative body. Number of Congress, Session. (year, month day). {\it Number of bill or resolution}, {\it Title}. [Type
of medium]. Available: site/path/file

\noindent {\bf {\it NOTE:}} ISO recommends that capitalization follow the accepted
practice for the language or script in which the information is given.



{\it Example:}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib20} U.S. House. 102nd Congress, 1st Session. (1991, Jan. 11). {\it H. Con. Res. 1, Sense of the Congress on Approval of Military Action}. [Online]. Available: LEXIS Library: GENFED File: BILLS
\end{thebibliography}

\noindent {\it Basic format for patents (when available online):}

\noindent Name of the invention, by inventor's name. (year, month day). Patent Number  [Type
of medium]. Available:  {site/path/file}

{\it Example:}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib21} Musical toothbrush with mirror, by L.M.R. Brooks. (1992, May 19). Patent D 326 189 [Online]. Available: NEXIS Library: LEXPAT File: DES
\end{thebibliography}


\noindent {\it Basic format for conference proceedings (published):}

\noindent J. K. Author, ``Title of paper,'' in {\it Abbreviated Name of Conf.}, City of Conf., Abbrev. State (if
given), Country, year, pp. {\it xxxxxx.}

{\it Example:}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib22} D. B. Payne and J. R. Stern, ``Wavelength-switched passively coupled single-mode optical network,'' in {\it Proc. IOOC-ECOC,}Boston, MA, USA,  1985,
pp. 585--590, doi:  {10.1109.} {{\it XXX}} {.123456}.
\end{thebibliography}

{\it Example for papers presented at conferences (unpublished):}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib23} D. Ebehard and E. Voges, ``Digital single sideband detection for interferometric sensors,'' presented at the {\it 2nd Int. Conf. Optical Fiber Sensors,} Stuttgart, Germany, Jan. 2-5, 1984.
\end{thebibliography}

\noindent {\it Basic format for patents:}

\noindent J. K. Author, ``Title of patent,'' U.S. Patent {\it x xxx xxx}, Abbrev. Month, day, year.

{\it Example:}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib24} G. Brandli and M. Dick, ``Alternating current fed power supply,'' U.S. Patent 4 084 217, Nov. 4, 1978.
\end{thebibliography}

\noindent {\it Basic format} {\it for theses (M.S.) and dissertations (Ph.D.):}

\noindent a) J. K. Author, ``Title of thesis,'' M.S. thesis, Abbrev. Dept., Abbrev.
Univ., City of Univ., Abbrev. State, year.

\noindent b) J. K. Author, ``Title of dissertation,'' Ph.D. dissertation, Abbrev.
Dept., Abbrev. Univ., City of Univ., Abbrev. State, year.

{\it Examples:}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib25} J. O. Williams, ``Narrow-band analyzer,'' Ph.D. dissertation, Dept. Elect. Eng., Harvard Univ., Cambridge, MA, USA, 1993.
\bibitem{bib26} N. Kawasaki, ``Parametric study of thermal and chemical nonequilibrium nozzle flow,'' M.S. thesis, Dept. Electron. Eng., Osaka Univ., Osaka, Japan, 1993.
\end{thebibliography}

\noindent {\it Basic format for the most common types of unpublished references:}

\noindent a) J. K. Author, private communication, Abbrev. Month, year.

\noindent b) J. K. Author, ``Title of paper,'' unpublished.

\noindent c) J. K. Author, ``Title of paper,'' to be published.

{\it Examples:}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib27} A. Harrison, private communication, May 1995.
\bibitem{bib28} B. Smith, ``An approach to graphs of linear forms,'' unpublished.
\bibitem{bib29} A. Brahms, ``Representation error for real numbers in binary computer arithmetic,'' IEEE Computer Group Repository, Paper R-67-85.
\end{thebibliography}

\noindent {\it Basic formats for standards:}

\noindent a) {\it Title of Standard}, Standard number, date.

\noindent b) {\it Title of Standard}, Standard number, Corporate author, location, date.

{\it Examples:}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib30} IEEE Criteria for Class IE Electric Systems, IEEE Standard 308, 1969.
\bibitem{bib31} Letter Symbols for Quantities, ANSI Standard Y10.5-1968.
\end{thebibliography}

{\it Article number in~reference examples:}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib32} R. Fardel, M. Nagel, F. Nuesch, T. Lippert, and A. Wokaun, ``Fabrication of organic light emitting diode pixels by laser-assisted forward transfer,'' {\it Appl. Phys. Lett.}, vol. 91, no. 6, Aug. 2007, Art. no. 061103, doi:  {10.1109.} {{\it XXX}} {.123456}.
\bibitem{bib33} J. Zhang and N. Tansu, ``Optical gain and laser characteristics of InGaN quantum wells on ternary InGaN substrates,'' {\it IEEE Photon. J.}, vol. 5, no. 2, Apr. 2013, Art. no. 2600111, doi:  {10.1109.} {{\it XXX}} {.123456}.
\end{thebibliography}

{\it Example when using \textit{et al.}:}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib34} S. Azodolmolky~{{et al.}}, Experimental demonstration of an impairment aware network planning and operation tool for transparent/translucent optical networks,''~{\it J. Lightw. Technol.}, vol. 29, no. 4, pp. 439--448, Sep. 2011,doi:  {10.1109.} {{\it XXX}} {.123456}.
\end{thebibliography}

\noindent {\it Basic format for datasets:}

\noindent Author,  Date, Year. ``Title of Dataset,'' distributed by Publisher/Distributor, http://url.com (or if DOI is used, end with a period)

{\it Example:}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib35} U.S. Department of Health and Human Services, Aug. 2013, ``Treatment Episode Dataset: Discharges (TEDS-D): Concatenated, 2006 to 2009,'' U.S. Department of Health and Human Services, Substance Abuse and Mental Health Services Administration, Office of Applied Studies, doi: 10.3886/ICPSR30122.v2.
\end{thebibliography}

\noindent {\it Basic format for code:}

\noindent Author,  Date published or disseminated, Year. ``Complete title, including ed./vers.\#,'' distributed by Publisher/Distributor, http://url.com (or if DOI is used, end with a period)

{\it Example:}{\vadjust{\vspace*{-2.5em}}}

\begin{thebibliography}{00}\leftskip1pc
\bibitem{bib36} T. D'Martin and S. Soares, 2019, ``Code for Assessment of Markov Decision Processes in Long-Term Hydrothermal Scheduling of Single-Reservoir Systems (Version 1.0),'' Code Ocean, doi: \_1.24433/CO.7212286.v1
\end{thebibliography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{a1.png}}]{First A. Author} (Fellow, IEEE) and all authors may include 
biographies. Biographies are
often not included in conference-related papers.
This author is an IEEE Fellow. The first paragraph
may contain a place and/or date of birth (list
place, then date). Next, the author’s educational
background is listed. The degrees should be listed
with type of degree in what field, which institution,
city, state, and country, and year the degree was
earned. The author’s major field of study should
be lower-cased.

The second paragraph uses the pronoun of the person (he or she) and
not the author’s last name. It lists military and work experience, including
summer and fellowship jobs. Job titles are capitalized. The current job must
have a location; previous positions may be listed without one. Information
concerning previous publications may be included. Try not to list more than
three books or published articles. The format for listing publishers of a book
within the biography is: title of book (publisher name, year) similar to a
reference. Current and previous research interests end the paragraph.

The third paragraph begins with the author’s title and last name (e.g.,
Dr. Smith, Prof. Jones, Mr. Kajor, Ms. Hunter). List any memberships in
professional societies other than the IEEE. Finally, list any awards and work
for IEEE committees and publications. If a photograph is provided, it should
be of good quality, and professional-looking.
\end{IEEEbiography}

\begin{IEEEbiographynophoto}{Second B. Author,} photograph and biography not available at the
time of publication.
\end{IEEEbiographynophoto}

\begin{IEEEbiographynophoto}{Third C. Author Jr.} (Member, IEEE), photograph and biography not available at the
time of publication.
\end{IEEEbiographynophoto}

\section*{Figures and Tables for Results}
\begin{table*}[!t]
\caption{Ablation Experiments on the Feature Clustering Block on Brain Tumor Dataset}
\label{table3}
\centering
\setlength{\tabcolsep}{12pt}
\renewcommand{\arraystretch}{1.8}
\begin{tabular}{c c c c c c}
\hline
\multirow{2}{*}{\textbf{No.}} &
\multicolumn{3}{c}{\textbf{settings}} &
\multirow{2}{*}{\textbf{DSC(\%)}} &
\multirow{2}{*}{\textbf{mNOC\%90}} \\
\cline{2-4}
 & position encoding & similarity measurement & multi-head & & \\
\hline
1 & None/Absolute Positional Embedding & Euclidean Distance + Gaussian KRF      & $\times$     & --                 & -- \\
2 & Actual Euclidean Distance          & Cosine                                 & $\times$     & $88.6\pm1.9^{*}$   & Unable \\
3 & Actual Euclidean Distance          & Euclidean Distance + Gaussian KRF      & $\times$     & $91.6\pm3.1^{*}$   & 7.1 \\
4 & Actual Euclidean Distance          & Euclidean Distance + Gaussian KRF      & $\checkmark$ & $93.4\pm2.2$       & 3.7 \\
\hline
\multicolumn{6}{l}{\footnotesize ${}^{*}$Indicates a t-test significance level of $p{<}0.05$ compared to the best performance.}
\end{tabular}
\end{table*}







\begin{table*}[t]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.8}
\caption{Comparison of CAISeg Against Other Approaches Across Brain Tumor and Colon Cancer Segmentation Tasks in Terms of Various Metrics}
\label{tab:main}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l l c c c c c c c c c c}
\hline
\multicolumn{2}{c}{} & \multicolumn{5}{c}{\textbf{``Brain Tumor'' from MRI}} & \multicolumn{5}{c}{\textbf{``Colon Cancer'' from CT}}\\
\cline{3-7}\cline{8-12}
Method & Interaction & Dice(\%) & mNOC@90 & HD$_{95}$(mm) & ASD(mm) & Jaccard & Dice(\%) & mNOC@75 & HD$_{95}$(mm) & ASD(mm) & Jaccard\\
\hline
Graph Cuts\textsubscript{2001} & points, bounding boxes & 79.2$\pm$8.5 & $>$20 & 8.49$\pm$4.21 & 3.49$\pm$1.82 & 0.66$\pm$0.16 & 53.4$\pm$16.9 & $>$20 & 20.02$\pm$10.89 & 3.19$\pm$1.75 & 0.36$\pm$0.15\\
Random Walker\textsubscript{2006} & points, bounding boxes & 80.1$\pm$9.4 & $>$20 & 8.61$\pm$4.19 & 3.75$\pm$2.26 & 0.67$\pm$0.18 & 58.6$\pm$12.4 & $>$20 & 17.35$\pm$11.42 & 2.74$\pm$1.94 & 0.41$\pm$0.12\\
MIDeepSeg\textsubscript{2021} & points & 90.3$\pm$6.4 & 15.7 & 4.50$\pm$3.67 & 1.41$\pm$1.19 & 0.82$\pm$0.12 & 77.9$\pm$4.7 & 10.4 & 3.98$\pm$0.95 & 0.75$\pm$0.09 & 0.64$\pm$0.08\\
DiNS\textsubscript{2022} & points & 91.2$\pm$1.9 & 10.3 & 3.35$\pm$1.21 & 1.09$\pm$0.21 & 0.84$\pm$0.05 & 70.9$\pm$6.2 & $>$20 & 4.56$\pm$0.22 & 0.72$\pm$0.28 & 0.55$\pm$0.09\\
3D RITM\textsubscript{2022} & points & 91.3$\pm$1.6 & 9.7 & 3.18$\pm$1.18 & 0.97$\pm$0.19 & 0.84$\pm$0.04 & 71.3$\pm$5.8 & $>$20 & 5.31$\pm$1.20 & 0.84$\pm$0.11 & 0.55$\pm$0.08\\
VMN\textsubscript{2023} & points & \underline{92.6$\pm$1.4} & 4 & 2.59$\pm$0.85 & \underline{0.99$\pm$0.25} & \underline{0.86$\pm$0.04} & \underline{80.2$\pm$5.1} & 11.1 & 3.91$\pm$0.98 & \textbf{0.41$\pm$0.08}$^\dagger$ & \underline{0.67$\pm$0.10}\\
SAM\text{-}Med3D\textsubscript{2023} & points & 92.1$\pm$2.1 & 4.8 & \underline{2.47$\pm$0.98} & 0.86$\pm$0.43 & 0.85$\pm$0.06 & 79.5$\pm$7.7 & \underline{8.2} & \underline{3.76$\pm$1.33} & 0.68$\pm$0.29 & 0.66$\pm$0.15\\
MedLSAM\textsubscript{2023} & bounding boxes & 90.4$\pm$1.0 & -- & 2.66$\pm$0.44 & 1.37$\pm$0.28 & 0.82$\pm$0.03 & 68.7$\pm$2.9 & -- & 15.78$\pm$3.55 & 1.83$\pm$0.84 & 0.52$\pm$0.04\\
MedSAM\textsubscript{2024} & bounding boxes & 90.2$\pm$1.3 & -- & 3.02$\pm$0.41 & 1.19$\pm$0.37 & 0.82$\pm$0.04 & 69.5$\pm$4.2 & -- & 14.96$\pm$4.83 & 1.52$\pm$0.76 & 0.53$\pm$0.06\\
\hline
\textbf{CAISeg(Ours)\textsubscript{2024}} & points & \textbf{93.4$\pm$2.2}$^\dagger$ & \textbf{2.7} & \textbf{2.12$\pm$1.23} & \textbf{0.80$\pm$0.45} & \textbf{0.88$\pm$0.06}$^\dagger$ & \textbf{81.1$\pm$7.3} & \textbf{4.6} & \textbf{3.12$\pm$0.81} & \underline{0.53$\pm$0.13} & \textbf{0.68$\pm$0.15}\\
\hline
\end{tabular}%
} % end resizebox

\vspace{3pt}
\parbox{0.96\textwidth}{\footnotesize
Here, the gray font indicates fully automated segmentation methods. $^\dagger$~indicates that CAISeg outperforms the best-performing method among other approaches for the corresponding metric with a \emph{t}-test significance level of $p{<}0.05$.
}
\end{table*}




\begin{table*}[t]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.8}
\caption{Comparison of Our Dual-Illumination Cross-Attention Fusion Against Other Approaches Across Plasma, Buffy Coat and Red Blood Cells Segmentation Tasks in Terms of Various Metrics}
\label{tab:main}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c c c c}
\hline
\multicolumn{2}{c}{} & \multicolumn{4}{c}{\textbf{Plasma + Red Blood Cells Segmentation@65}} & \multicolumn{4}{c}{\textbf{Buffy Coat Segmentation@65}}\\
\cline{3-6}\cline{7-10}
Method & Dual-backbone & Detection Rate(\%) $\uparrow$ & IOU $\uparrow$ & Diff$_{up}$(px) $\downarrow$ & Diff$_{low}$(px) $\downarrow$ & Detection Rate(\%) $\uparrow$ & IOU $\uparrow$ & Diff$_{up}$(px) $\downarrow$ & Diff$_{low}$(px) $\downarrow$\\
\hline
Yolo11-Blue & $\times$ & 99.72 & \underline{0.98$\pm$0.01} & 4.1$\pm$3.2 & \underline{3.6$\pm$2.6} & 97.17 & \underline{0.78$\pm$0.07} & 4.3$\pm$2.9 & \underline{2.5$\pm$1.9}\\
Yolo11-White & $\times$ & \underline{100.00} & 0.97$\pm$0.01 & \textbf{3.9$\pm$3.8} & 3.7$\pm$2.9 & \underline{98.61} & 0.75$\pm$0.07 & 4.6$\pm$3.3 & 2.5$\pm$1.9\\
Dual Yolo Concat & $\checkmark$ & 99.72 & 0.97$\pm$0.01 & 4.4$\pm$3.1 & 4.3$\pm$4.2 & 68.89 & 0.77$\pm$0.06 & \textbf{3.8$\pm$2.2} & 2.6$\pm$1.8\\
Dual Yolo Weighted & $\checkmark$ & 99.17 & 0.97$\pm$0.02 & 4.5$\pm$14.5 & 3.6$\pm$2.8 & 94.72 & 0.76$\pm$0.07 & \underline{4.0$\pm$2.3} & 2.7$\pm$1.9\\
Dual Yolo CrossAttn & $\checkmark$ & 97.50 & 0.97$\pm$0.01 & \underline{4.1$\pm$4.3} & 5.4$\pm$7.0 & 96.11 & 0.77$\pm$0.06 & 4.2$\pm$2.5 & \textbf{2.4$\pm$1.2}\\
\hline
\textbf{Dual Yolo CrossAttnPrecise (Our best)} & $\checkmark$ & \textbf{100.00} & \textbf{0.98$\pm$0.01} & 4.7$\pm$4.5 & \textbf{3.6$\pm$2.8} & \textbf{99.17} & \textbf{0.80$\pm$0.02} & 4.6$\pm$3.0 & 2.6$\pm$2.0\\
\hline

\end{tabular}%
} % end resizebox
\end{table*}



\begin{table*}[t]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.8}
\caption{Comparison of Our Dual-Illumination Cross-Attention Fusion Against Other Approaches Across Plasma, Buffy Coat and Red Blood Cells Segmentation Tasks in Terms of Various Metrics}
\label{tab:main}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c c c c}
\hline
\multicolumn{2}{c}{} & \multicolumn{4}{c}{\textbf{Plasma + Red Blood Cells Segmentation@70}} & \multicolumn{4}{c}{\textbf{Buffy Coat Segmentation@70}}\\
\cline{3-6}\cline{7-10}
Method & Dual-backbone & Detection Rate(\%) $\uparrow$ & IOU $\uparrow$ & Diff$_{up}$(px) $\downarrow$ & Diff$_{low}$(px) $\downarrow$ & Detection Rate(\%) $\uparrow$ & IOU $\uparrow$ & Diff$_{up}$(px) $\downarrow$ & Diff$_{low}$(px) $\downarrow$\\
\hline
Yolo11-Blue & $\times$ & 99.72 & \underline{0.98$\pm$0.01} & 4.1$\pm$3.2 & 3.6$\pm$2.6 & \textbf{97.50} & 0.77$\pm$0.07 & 4.3$\pm$2.9 & 2.5$\pm$1.9\\
Yolo11-White & $\times$ & \underline{100.00} & 0.97$\pm$0.01 & \textbf{3.9$\pm$3.8} & 3.7$\pm$2.9 & \underline{95.56} & 0.75$\pm$0.07 & 4.6$\pm$3.3 & 2.5$\pm$1.9\\
Dual Yolo Concat & $\checkmark$ & 99.44 & 0.97$\pm$0.01 & 4.4$\pm$3.1 & 4.3$\pm$4.0 & 54.72 & \underline{0.78$\pm$0.06} & \underline{3.7$\pm$2.2} & \underline{2.5$\pm$1.7}\\
Dual Yolo Weighted & $\checkmark$ & 98.89 & 0.97$\pm$0.01 & \underline{4.0$\pm$2.9} & \textbf{3.5$\pm$2.6} & 90.00 & 0.76$\pm$0.07 & 4.0$\pm$2.4 & 2.7$\pm$1.9\\
Dual Yolo CrossAttn & $\checkmark$ & 97.22 & 0.97$\pm$0.01 & 4.1$\pm$4.3 & 5.4$\pm$7.0 & 0.28 & \textbf{0.79} & 6.0 & 3.6\\
Dual Yolo CrossAttn (30 Epochs) & $\checkmark$ & \textbf{100.00} & \textbf{0.98$\pm$0.01} & 4.7$\pm$4.5 & \underline{3.6$\pm$2.8} & 94.72 & 0.76$\pm$0.07 & 4.7$\pm$3.1 & 2.6$\pm$2.0\\
\hline
\textbf{Dual Yolo (Our Best)} & $\checkmark$ & 90.83 & 0.97$\pm$0.01 & 4.8$\pm$4.4 & 3.9$\pm$5.1 & 0.00 & 0.00 & \textbf{0.0} & \textbf{0.0}\\
\hline

\end{tabular}%
}
\end{table*}




\begin{table*}[t]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.8}
\caption{Comparison of Our Dual-Illumination Cross-Attention Fusion Against Other Approaches Across Plasma, Buffy Coat and Red Blood Cells Segmentation Tasks in Terms of Various Metrics}
\label{tab:main}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c c c c}
\hline
\multicolumn{2}{c}{} & \multicolumn{4}{c}{\textbf{Plasma + Red Blood Cells Segmentation@75}} & \multicolumn{4}{c}{\textbf{Buffy Coat Segmentation@75}}\\
\cline{3-6}\cline{7-10}
Method & Dual-backbone & Detection Rate(\%) $\uparrow$ & IOU $\uparrow$ & Diff$_{up}$(px) $\downarrow$ & Diff$_{low}$(px) $\downarrow$ & Detection Rate(\%) $\uparrow$ & IOU $\uparrow$ & Diff$_{up}$(px) $\downarrow$ & Diff$_{low}$(px) $\downarrow$\\
\hline
Yolo11-Blue & $\times$ & 99.72 & \underline{0.98$\pm$0.01} & 4.1$\pm$3.2 & 3.6$\pm$2.6 & \textbf{93.61} & \underline{0.78$\pm$0.06} & 4.2$\pm$2.7 & 2.6$\pm$1.9\\
Yolo11-White & $\times$ & \underline{100.00} & 0.97$\pm$0.01 & \textbf{3.9$\pm$3.8} & 3.7$\pm$2.9 & \underline{88.89} & 0.76$\pm$0.07 & 4.6$\pm$3.4 & 2.6$\pm$2.0\\
Dual Yolo Concat & $\checkmark$ & 99.44 & 0.97$\pm$0.01 & 4.4$\pm$3.1 & 4.3$\pm$4.0 & 31.39 & \textbf{0.78$\pm$0.05} & 3.7$\pm$2.2 & 2.6$\pm$1.7\\
Dual Yolo Weighted & $\checkmark$ & 98.89 & 0.97$\pm$0.01 & \underline{4.0$\pm$2.9} & \textbf{3.5$\pm$2.6} & 76.39 & 0.77$\pm$0.06 & 4.0$\pm$2.3 & 2.7$\pm$1.9\\
Dual Yolo CrossAttn & $\checkmark$ & 95.00 & 0.97$\pm$0.01 & 4.2$\pm$4.3 & 5.4$\pm$7.0 & 0.00 & 0.00 & \textbf{0.0} & \textbf{0.0}\\
Dual Yolo CrossAttn (30 Epochs) & $\checkmark$ & \textbf{100.00} & \textbf{0.98$\pm$0.01} & 4.7$\pm$4.5 & \underline{3.6$\pm$2.8} & 86.39 & 0.77$\pm$0.07 & 4.8$\pm$3.1 & 2.7$\pm$2.1\\
\hline
\textbf{Dual Yolo (Our Best)} & $\checkmark$ & 88.89 & 0.97$\pm$0.01 & 4.9$\pm$4.4 & 3.9$\pm$5.1 & 0.00 & 0.00 & \underline{0.0} & \underline{0.0}\\
\hline

\end{tabular}%
} % end resizebox
\end{table*}




\begin{table*}[!t]
\caption{Ablation Experiments on the Blood Fractionation Component Segmentation Task@70}
\label{table3}
\centering
\setlength{\tabcolsep}{12pt}
\renewcommand{\arraystretch}{1.8}
\begin{tabular}{c c c c c c}
\hline
\multirow{2}{*}{\textbf{No.}} &
\multicolumn{3}{c}{\textbf{Settings}} &
\multirow{2}{*}{\textbf{Detection Rate(\%) $\uparrow$}} &
\multirow{2}{*}{\textbf{IOU $\uparrow$}} \\
\cline{2-4}
 & Dual-backbone & Dual-illumination fusion method & Pre-training & \\
\hline
1 & $\times$     & None                         & From Scratch      & 91.67 & 0.73$\pm$0.22 \\
2 & $\checkmark$ & Concatenation + Compression  & From Scratch      & --    & -- \\
3 & $\checkmark$ & Concatenation + Compression  & Transfer Learning & 86.11 & 0.73$\pm$0.22 \\
4 & $\checkmark$ & Adaptive Weighted Fusion     & From Scratch      & --    & -- \\
5 & $\checkmark$ & Adaptive Weighted Fusion     & Transfer Learning & 9.44  & 0.77$\pm$0.17 \\
6 & $\checkmark$ & Cross Modal Attention Fusion & From Scratch      & --    & -- \\
7 & $\checkmark$ & Cross Modal Attention Fusion & Transfer Learning & \textbf{97.78} & 0.73$\pm$0.22 \\
\hline
\end{tabular}
\end{table*}




\end{document}