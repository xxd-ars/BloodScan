\section{Introduction}
\label{sec:introduction}
\IEEEPARstart {B}LOOD component stratification following centrifugation is a critical process in clinical laboratory workflows. Centrifuged blood tubes exhibit distinct layers—serum or plasma, buffy coat, and erythrocyte fractions—whose accurate localization is essential for automated liquid handling and downstream assays. Precise layer identification also enables preliminary quality assessment by revealing sample integrity and physiological conditions. However, conventional stratification methods rely predominantly on manual visual inspection or basic measurement tools, which are constrained by operator subjectivity, labor intensity, and inter-operator variability. As laboratory automation advances and testing volumes increase, intelligent systems capable of accurate layer localization are needed to ensure reliable, high-throughput clinical diagnostics.

Automated blood stratification presents multiple technical challenges. First, inter-layer boundaries exhibit intrinsically low optical contrast. The buffy coat, typically 0.5–1.0 mm thick and comprising less than 1\% of blood volume, is particularly difficult to distinguish from adjacent layers under standard imaging conditions. Second, single-illumination imaging fails to capture the full range of discriminative features. White-light images provide structural context but poor buffy coat contrast, while blue-light illumination enhances buffy coat fluorescence but may obscure other boundaries. Third, cross-spectral registration is challenging due to parallax, pose variation, and temporal mismatch during dual-modality acquisition. Direct feature fusion often introduces spatial misalignment and semantic inconsistency, degrading detection accuracy.

\begin{figure}[!t]
\centering

% ==== 参数定义 ====
\newlength{\sepM}\setlength{\sepM}{10pt}      % 中间间距
\newlength{\colw}\setlength{\colw}{0.45\linewidth} % 每个子图宽度
\newlength{\imgheight}

% ==== 左图 ====
\begin{minipage}[t]{\colw}
  \centering
  \includegraphics[width=\linewidth]{img/blue_annotated.jpg}%
  \settoheight{\imgheight}{\includegraphics[width=\linewidth]{img/blue_annotated.jpg}}\\
  \vspace{3pt}
\end{minipage}
% ==== 右图 ====
\begin{minipage}[t]{\colw}
  \centering
  \includegraphics[width=\linewidth]{img/normal_annotated.jpg}\\
  \vspace{3pt}
\end{minipage}

\vspace{5pt}
\caption{\textbf{Annotated visualization of blood component stratification under white-light and blue-light illumination.}
Red markers denote manually selected layer boundary points by professional laboratory personnel: points 1–4 enclose the plasma region, points 3–6 define the buffy coat (white membrane layer), and points 5–7 correspond to the erythrocyte fraction. 
Under white-light illumination (left), inter-layer boundaries show low optical contrast, making the buffy coat difficult to distinguish. 
Under blue-light illumination (right), the buffy coat fluorescence enhances layer separability, though specular reflections on the tube surface and similar chromatic appearance between plasma and erythrocytes may introduce visual ambiguity.}
\label{fig:annotated images}
\end{figure}

Despite recent advances, existing blood stratification methods remain limited. Prior approaches have focused on one-dimensional liquid-level regression [1], which can identify single interfaces but not complex multi-layer structures. Deep networks like Inception-ResNet-V2 [2] achieve serum quality classification but provide only categorical assessment without spatial localization. Commercial systems such as PerkinElmer's JANUS Blood iQ [3] utilize dual-illumination imaging based on traditional image processing and clustering algorithm for automated layer detection, but technical details remain unpublished. In broader medical imaging, Transformer-based methods like MicFormer [4] and A2FSeg [5] have introduced cross-modal attention for CT–MRI fusion, yet these approaches target segmentation neglecting detection capabilities. Multispectral detection methods such as YOLO-Phantom [6] and MAF-YOLO combine visible and infrared imaging through simple concatenation, lacking refined spatial alignment mechanisms. Current methods thus fail to address fine-grained cross-modal alignment and complementary information extraction for stratified blood component targets.

To address these challenges, this paper proposes DIUA-YOLO (Dual-Illumination Unidirectional Attention YOLO), a dual-spectral detection framework for precise blood component stratification. Our principal contributions are:

\begin{itemize}
    \item A dual-backbone architecture that separately processes blue-light and white-light images is designed and developed, enabling multi-scale feature extraction and fusion to leverage complementary optical information. 
    \item We introduce a cross-modal attention mechanism employing unidirectional queries—where blue-light features query white-light features—combined with localized token-level attention. This design achieves effective modality alignment and information fusion while reducing computational complexity by approximately 91\% compared to bidirectional global attention. 
    \item We establish a rigorous clinical evaluation framework requiring each layer to be detected exactly once, demanding zero false positives and negatives. On our dual-illumination blood tube dataset, DIUA-YOLO achieves 98.89\% detection success rate, substantially outperforming single-modality YOLO11 baselines. 
\end{itemize}

The remainder of this paper is organized as follows. Section II details the DIUA-YOLO architecture and key technical modules. Section III describes the experimental design and evaluation methodology. Section IV presents results and performance analysis. Section V concludes and discusses future directions.