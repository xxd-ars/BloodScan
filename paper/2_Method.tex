\section{Methodology}
\label{sec:methodology}

\subsection{Automated Acquisition System}

The proposed DIUA-YOLO system comprises two integrated components: a hardware acquisition platform for dual-modality image collection and a deep learning network for blood layer detection. This section describes the automated platform design and operational workflow.

\subsubsection{Mechanical Configuration and Control Architecture} As in Figure~\ref{fig:mechanics}, the acquisition platform employs a three-degree-of-freedom linear motion system (X-axis horizontal, Y-axis anterior-posterior, Z-axis vertical) coupled with a single-degree-of-freedom rotational gripper. The X and Y axes coordinate to traverse all positions in a standard 96-well tube rack, while the Z axis provides vertical displacement between rack height and imaging focal plane. The rotational gripper delivers controllable radial clamping force and precise angular positioning, ensuring tube stability during transport and imaging.

The motion control system adopts a layered architecture. At the communication layer, RS485 serial protocol governs linear motion stepper motors, while Modbus TCP Ethernet protocol controls rotational servo motors. The application layer encapsulates atomic motor commands into composite detection routines, with thread event synchronization coordinating precise timing between mechanical motion and image acquisition. A data management layer provides SQLite database support, recording tube barcode, type, detection results, batch number, and timestamps for full traceability.

\begin{figure}[!t]
\centerline{\includegraphics[width=\columnwidth]{img/mechanics.jpg}}
\caption{\textbf{Mechanical configuration and prototype of the automated acquisition platform.} 
Left: Three-degree-of-freedom (X–Y–Z) linear motion system coupled with a single-degree-of-freedom rotational gripper, designed for precise positioning, tube handling, and imaging. 
Right: Physical implementation of the system, including the translational-rotational manipulator, tube rack, imaging module, and control interface.}
\label{fig:mechanics}
\end{figure}

\subsubsection{Dual-Illumination Imaging System} The imaging system employs a single industrial camera integrated with dual illumination sources. A blue LED array (center wavelength ~465 nm) enhances the optical contrast of the buffy coat through increased light scattering and weak intrinsic autofluorescence, while a white LED source (color temperature ~6500 K) provides structural and color information of the overall sample. A custom mechanical fixture rigidly maintains the relative geometry among the camera, light sources, and tube gripper, thereby minimizing parallax and viewpoint variation during acquisition. Illumination control is managed by a microcontroller that drives programmable LED arrays, with the host computer synchronizing illumination switching, motor actuation, and image capture via serial communication protocols.

\subsubsection{Automated Detection Workflow} A single-tube detection sequence comprises a strictly ordered set of operations: (1) The robotic arm descends along the Z-axis to the rack height, while the X-axis and the Y-axis move synchronously to position above the target test tube well; (2) The gripper closes to clamp tube, and the Z-axis ascends to lift it; (3) The X-axis translates the tube to the camera’s field of view, and the Z-axis lowers it to the imaging focal plane; (4) Once arrvied, sequential dual-illumination imaging is performed: the white-light and blue-light sources are activated in turn to capture two aligned frames, while during such process, the camera–tube geometry remains mechanically fixed; (5) The rotational executor performs a 180 \degree rotation, the same dual-illumination imaging takes place to capture the full image of the sample; (6) The acquired images are processed by the cross-spectral detection model for layer localization and analysis; (7) After imaging, the Z-axis lifts, and the X-axis returns the tube to its original position; (8) The Z-axis then descends to rack height, the gripper releases the tube, and the arm resets to its initial position. Within the motion control system, event blocking at each critical node enforces sequential execution, ensuring completion of one action before the next begins, thereby eliminating potential motion conflicts. 

Batch detection traverses all rack positions via nested loops: the outer loop controls column selection, the inner loop controls row selection. Upon completing all tubes, the system automatically homes and increments the batch counter. This design realizes a fully automated "grasp-image-assess-release" closed loop, requiring operators only to load racks and initiate commands after homing. Upon completing the final tube, the system automatically returns to the home position and increments the batch counter. This design achieves a fully automated closed-loop workflow encompassing tube loading, imaging, analysis, and return, requiring operator intervention only for rack placement and command initialization.

\begin{figure*}[!t]
\centering
\includegraphics[width=\textwidth]{img/arch.png}
\caption{\textbf{The proposed DIUA-YOLO framework,} comprising dual-illumination input, dual-backbone feature extraction, cross-spectral feature fusion, and YOLO11 segmentation modules. 
White-light and blue-light images are independently processed through modality-specific backbones to capture complementary structural and contrast information. 
Fused multi-scale representations are decoded through neck and segmentation heads to accurately localize plasma, buffy coat, and erythrocyte layers.}
\label{fig:architecture}
\end{figure*}

\subsection{DIUA-YOLO Network Architecture}

Figure~\ref{fig:architecture} illustrates the overall architecture of the proposed DIUA-YOLO framework. The network processes dual-illumination inputs through independent feature extraction backbones, fuses multi-scale representations via cross-modal feature fusion blocks, and outputs segmentation results through YOLO segmentation heads.

\subsubsection{Dual-Backbone Feature Extraction} White-light and blue-light images enter two structurally identical but parameter-independent convolutional neural network backbones based on the YOLO11 architecture. This dual-backbone design allows each modality's feature extractor to specialize in learning modality-specific representations: the white-light backbone network excels at extracting overall hierarchical structures, three-material color gradients, and texture details, while the blue-light backbone network captures the optical properties and boundary features of the buffy coat layer layer under blue illumination.

The backbone architecture employs YOLO11's C3k2 block, an efficient Cross Stage Partial (CSP) design that reduces computational overhead while preserving feature extraction capability. In each backbone stage, the C3k2 block employs three consecutive convolutional layers with residual connections to capture hierarchical feature representations at varying semantic depths. This streamlined architecture achieves approximately 40\% parameter reduction compared to prior YOLO variants while maintaining detection accuracy, thereby enabling efficient dual-backbone training for our dual-illumination framework without excessive computational burden.

Each backbone outputs a multi-scale feature pyramid at three levels—P3, P4, and P5—corresponding to different spatial resolutions and semantic depths. The P3 layer, with higher spatial resolution, suits detection of thin-layer structures such as the buffy coat. The P5 layer, with larger receptive fields, accommodates detection of thick layers such as serum, plasma and erythrocyte. This multi-scale design effectively captures blood layer information across varying thicknesses.

Let $I_w \in \mathbb{R}^{H \times W \times 3}$ and $I_b \in \mathbb{R}^{H \times W \times 3}$ denote the white-light and blue-light input images, respectively, where $H$ and $W$ represent height and width. The dual backbones extract feature pyramids:

\begin{equation}
\{F_w^{P3}, F_w^{P4}, F_w^{P5}\} = \text{Backbone}_w(I_w)
\end{equation}
\begin{equation}
\{F_b^{P3}, F_b^{P4}, F_b^{P5}\} = \text{Backbone}_b(I_b)
\end{equation}

where each feature $F \in \mathbb{R}^{h \times w \times c}$ has spatial dimensions $(h, w)$ and channel dimension $c$. These features then feed into fusion modules at each pyramid level.

\subsubsection{Cross-Modal Attention Fusion}

To effectively integrate complementary information from dual modalities while maintaining computational efficiency, we propose a cross-modal attention mechanism employing unidirectional queries and localized spatial matching. This fusion strategy operates independently at each pyramid level $s \in \{P3, P4, P5\}$ to align and enhance blue-light features using white-light contextual information.

\textbf{Unidirectional Query Design.} Unlike conventional bidirectional attention that computes mutual interactions, our method adopts an asymmetric design: blue-light features serve as queries $Q$ to retrieve information from white-light features (keys $K$ and values $V$), while the reverse direction is omitted. Formally, for feature pair $(F_b^s, F_w^s)$ at pyramid level $s$, we compute:

\begin{equation}
Q^s = \text{Conv}(F_b^s), \quad K_s = \text{Conv}(F_w^s), \quad V^s = \text{Conv}(F_w^s)
\end{equation}

where $Q_s, K_s, V_s \in \mathbb{R}^{B \times C \times H_s \times W_s}$, with $B$ denoting batch size, $C$ channel dimension, and $(H_s, W_s)$ spatial dimensions at level $s$. This unidirectional strategy exploits the blue-light modality's advantage in buffy coat detection to guide feature fusion, reducing attention computation by approximately 50\% compared to bidirectional schemes while maintaining detection performance.

\textbf{Token-Based Spatial Partitioning.} To reduce computational complexity from $O(N^2)$ (where $N = H_s \times W_s$) to tractable levels, we partition feature maps into non-overlapping tokens of size $t \times t$ pixels. The feature map is reshaped into a token grid:

\begin{equation}
Q^s_{tok} = \text{Tokenize}(Q^s) \in \mathbb{R}^{B \times n_h \times n_w \times C \times t^2}
\end{equation}

where $\text{Tokenize}(\cdot)$ denotes the combined spatial decomposition, dimension rearrangement, and flattening operation that partitions the feature map into non-overlapping $t \times t$ tokens, $n_h = \lceil H_s / t \rceil$ and $n_w = \lceil W_s / t \rceil$ define the token grid dimensions, and $t^2$ represents the flattened spatial dimension within each token. Similar transformations apply to $K^s_{tok}$ and $V^s_{tok}$. For our implementation, token size $t$ varies across pyramid levels, e.g., $t=2$ for P3 (preserving fine spatial detail), $t=3$ for P4, and $t=4$ for P5 (accommodating larger receptive fields).

\textbf{Localized Attention Computation.} Rather than computing global attention across all tokens, each query token attends only to a local neighborhood of key tokens. For a query token at grid position $(i, j)$, we extract a $k \times k$ neighborhood of key and value tokens centered at $(i, j)$:

\begin{equation}
\begin{aligned}
\mathcal{N}_{ij} = \{&K^s_{tok}[i', j'], V^s_{tok}[i', j'] \mid \\
&|i'-i| \leq \lfloor k/2 \rfloor, |j'-j| \leq \lfloor k/2 \rfloor\}
\end{aligned}
\end{equation}

where $k$ is the neighborhood size parameter. This localization is efficiently implemented via PyTorch's \texttt{unfold} operation with padding. The attention weights are then computed as:

\begin{equation}
\alpha_{ij} = \text{softmax}\left(\frac{Q^s_{tok}[i,j] \cdot K^s_{tok}[\mathcal{N}_{ij}]^\top}{\sqrt{t^2}}\right) \in \mathbb{R}^{C \times k^2}
\end{equation}

where $\sqrt{t^2}$ is the scaling factor (corresponding to the token dimension), and $\cdot$ denotes batched matrix multiplication across channels. The enhanced token $\widetilde{Q}^s_{tok}[i,j]$ is obtained through weighted aggregation of values from the local neighborhood:

\begin{equation}
\widetilde{Q}^s_{tok}[i,j] = \sum_{(i',j') \in \mathcal{N}_{ij}} \alpha_{ij}[i',j'] \cdot V^s_{tok}[i',j'] \in \mathbb{R}^{C \times t^2}
\end{equation}

The neighborhood size $k$ is adapted per pyramid level: $k=3$ for P3, $k=5$ for P4, and $k=7$ for P5. These token size and neighborhood size hyperparameters were determined through grid search optimization on validation set, systematically evaluating candidate configurations $t \in \{2, 3, 4\}$ and $k \in \{3, 5, 7\}$ to maximize detection success rate while maintaining computational efficiency. This hierarchical design balances local precision with contextual coverage. By constraining attention to local neighborhoods, complexity reduces from $O(N^2)$ to $O(N \cdot k^2)$, achieving approximately 91\% reduction at level of P5 with input image size of $1504 \times 1504$.

\textbf{Feature Reconstruction} After computing enhanced tokens for all grid positions, we collect them into a complete token grid:

\begin{equation}
\begin{aligned}
\widetilde{Q}^s_{tok} &= \{\widetilde{Q}^s_{tok}[i,j] \mid 1 \leq i \leq n_h, 1 \leq j \leq n_w\} \\
&\quad \in \mathbb{R}^{B \times n_h \times n_w \times C \times t^2}
\end{aligned}
\end{equation}

We then reverse the tokenization process through spatial rearrangement and reshaping operations to reconstruct the feature map:

\begin{equation}
\widetilde{F}_b^s = \text{Detokenize}(\widetilde{Q}^s_{tok}) \in \mathbb{R}^{B \times C \times H_s \times W_s}
\end{equation}

where $\text{Detokenize}(\cdot)$ performs dimension permutation and reshaping to convert the token grid back to spatial layout. The final fused representation $F_{f}^s$ employs a residual connection to preserve original blue-light information:

\begin{equation}
F_{f}^s = \text{Proj}(\widetilde{F}_b^s + F_b^s)
\end{equation}

where $F_b^s$ denotes the original blue-light features and $\text{Proj}(\cdot)$ denotes a $1 \times 1$ convolution for channel projection. This residual formulation ensures that the network learns additive refinements rather than replacing the base features, facilitating gradient flow and training stability.

\subsubsection{Channel Concatenation Fusion}

As a commonly used fusion strategy, we implement channel-wise concatenation followed by dimensionality reduction. For feature pair $(F_b^s, F_w^s)$ at pyramid level $s$, this method directly concatenates along the channel dimension and applies $1 \times 1$ convolution for compression:

\begin{equation}
F_{f}^s = \text{Conv}(\text{Concat}(F_b^s, F_w^s)) \in \mathbb{R}^{B \times C \times H_s \times W_s}
\end{equation}

where the $1 \times 1$ convolutional layer reduces channel count from $2C$ back to $C$ through learned linear combinations. While computationally efficient, this approach lacks explicit modeling of cross-modal semantic correspondences and spatial alignment, limiting its ability to resolve inter-modality registration errors or extract complementary information selectively.

\subsubsection{Adaptive Weighted Fusion}

This method attempts to learn modality-specific importance through adaptive weighting. The fusion combines spatial attention, global context, and learnable temperature parameters. Features are first normalized via layer normalization, then concatenated for joint weight prediction:

\begin{equation}
\begin{split}
\tilde{F}_s = \text{Concat}(&\text{LayerNorm}(F_b^s), \\
&\text{LayerNorm}(F^w_s)) \in \mathbb{R}^{B \times 2C \times H_s \times W_s}
\end{split}
\end{equation}

Two parallel pathways predict importance weights. The spatial pathway generates pixel-wise weights through cascaded convolutions:

\begin{equation}
w^s_{sp} = \sigma(\text{Conv}(\text{Conv}_{3\times3}(\text{Conv}_{3\times3}(\tilde{F}^s))))
\end{equation}

where $\sigma$ denotes the sigmoid activation. The global pathway computes context-aware weights via adaptive average pooling (which reduces spatial dimensions to $1 \times 1$ through global averaging) followed by convolutional projections:

\begin{equation}
w^s_{gl} = \sigma(\text{Conv}(\text{Conv}(\text{GAP}(\tilde{F}^s))))
\end{equation}

where $\text{GAP}(\cdot)$ denotes global average pooling. The final weight combines both pathways with a learnable temperature $\tau$, constrained to $[0.1, 0.9]$ via clamping to prevent extreme values:

\begin{equation}
w_s = \max(0.1, \min(w_s^{sp} \cdot w_s^{gl} \cdot \tau, 0.9))
\end{equation}

The fused feature $F_{f}^s$ applies this weight with additional blue-light residual to leverage its stronger buffy coat detection capability:

\begin{equation}
F_{f}^s = (\alpha + w_s) \cdot \hat{F}_b^s + (1 - w_s) \cdot \hat{F}_w^s
\end{equation}

where $\hat{F}_b^s$ and $\hat{F}_w^s$ denote the normalized features, and $\alpha$ is a fixed coefficient emphasizing blue-light contribution.

\subsection{Implementation Details}

Our implementation extends Ultralytics YOLO through minimally invasive modifications that enable dual-backbone architectures while preserving full compatibility with existing workflows. The key principle is deep integration: dual-modality support is achieved by extending—rather than replacing—YOLO's core components, allowing users to leverage the entire ecosystem with only data format adjustments.

The model parser in \textit{ultralytics/nn/tasks.py} was extended to interpret three-section YAML configurations comprising \textit{backbone\_b}, \textit{backbone\_w}, and \textit{head}. The \textit{parse\_model} function maintains separate channel tracking lists (\textit{ch\_b}, \textit{ch\_w}) and iterates through concatenated layer definitions (\textit{backbone\_b + backbone\_w + head}), automatically inferring output channels via index-based branching. During forward propagation, \textit{\_predict\_once} routes features through modality-specific backbones using conditional logic, merging streams at designated fusion points. This architecture enables direct reuse of all existing YOLO modules (Conv, C3k2, SPPF) without modification, transforming YOLO from single-modality to dual-modality detection through parser-level extensions alone.

Fusion modules are implemented as standalone PyTorch classes in \textit{ultralytics/nn/modules/fusion.py}, each accepting feature pairs $(F_b^s, F_w^s)$ and returning fused representations. To eliminate manual hyperparameter specification, we extended \textit{parse\_model} with specialized handling logic that automatically infers channel dimensions from backbone outputs. For example, YAML specification \textit{[[4, 15], 1, CrossModalAttention, [2, 3]]} triggers retrieval of channel counts from layers 4 and 15, followed by instantiation with token size 2 and neighbor size 3. This configuration-driven design enables switching fusion strategies by modifying a single YAML line—no source code changes required.

Data flow adopts a unified 6-channel tensor format concatenating RGB from both modalities. The \textit{split\_6ch\_tensor} function separates inputs for independent backbone processing, while \textit{BaseDataset} was extended to accept \textit{channels=6} parameter. The \textit{load\_image} method automatically detects pre-concatenated NumPy arrays (\textit{.npy}) when available, falling back to paired loading from \textit{images\_b/} and \textit{images\_w/} directories otherwise. Users need only adjust dataset YAML metadata (\textit{channels: 6}) and organize data accordingly—all other YOLO functionalities (augmentation, validation, export) operate unchanged.

This modular architecture achieves deep coupling with YOLO's ecosystem: training scripts, validation routines, and export utilities require no modification. The complete implementation will be open-sourced to support multi-modal detection research.