\section{Conclusion}
\label{sec:conclusion}
This paper presents DIUA-YOLO, a dual-illumination detection and segmentation framework addressing the critical challenge of automated blood component stratification following centrifugation. Conventional approaches relying on single-modality imaging or manual inspection fail to reliably localize the optically thin buffy coat layer while maintaining high throughput. We address these limitations through a dual-backbone architecture that independently processes white-light and blue-light images, enabling complementary feature extraction tailored to each modality's optical characteristics. The key innovation lies in our cross-modal attention mechanism, which employs unidirectional queries and token-based localized attention to achieve effective feature fusion while reducing computational complexity by approximately 91\% compared to global attention. This design fundamentally addresses spatial misalignment between modalities through implicit local feature correspondence, enabling robust boundary detection even under imperfect registration, a critical advantage over rigid pixel-wise fusion methods.

Comprehensive evaluation demonstrates that DIUA-YOLO achieves 98.80\% overall detection success rate under stringent clinical criteria requiring exact detection of all three blood layers without false positives or negatives. The framework achieves near-perfect performance for plasma and erythrocyte layers, while critically improving buffy coat detection to 96.39\%, representing transformative advancement for white blood cell localization. Statistical analysis confirms all architectural improvements are highly significant (p$<$0.001), validating the necessity of dual-backbone design, superiority of attention-based fusion, and benefits of hierarchical transfer learning. The integrated automated platform processes complete tubes in 8 seconds, delivering 3.75$\times$ speedup over manual workflows while dramatically reducing labor requirements. Sub-5-pixel boundary localization accuracy ensures volume estimation errors below 0.5\% for clinical applications.

Beyond blood tube analysis, this work contributes methodological insights demonstrating that explicit correspondence modeling through localized attention outperforms rigid fusion approaches for thin-layer detection under spatial misalignment. Primary limitations include dataset scale (500 base sample pairs) and restriction to pre-centrifuged tubes. Future work should expand to rare pathological conditions, explore simultaneous dual-spectrum imaging, and integrate cell-level analysis capabilities. The proposed architecture demonstrates strong potential for extension to other dual-modality medical imaging tasks, contributing to the advancement of automated diagnostic systems in personalized medicine and laboratory informatics.