\section{Results}
\label{sec:results}

\begin{table*}[t]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.8}
\caption{Comparison of Our Dual-Illumination Cross-Attention Fusion Against Other Approaches Across Plasma, Buffy Coat and Erythrocyte Segmentation Tasks in Terms of Various Metrics at conf=0.25}
\label{tab:main_conf25}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c c c c c c c c}
\hline
\multicolumn{2}{c}{} & \multicolumn{4}{c}{\textbf{Plasma Segmentation}} & \multicolumn{4}{c}{\textbf{Buffy Coat Segmentation}} & \multicolumn{4}{c}{\textbf{Erythrocyte Segmentation}}\\
\cline{3-6}\cline{7-10}\cline{11-14}
Method & Dual-backbone & \textbf{DSR(\%)} $\uparrow$ & IOU $\uparrow$ & Diff$_{up}$(px) $\downarrow$ & Diff$_{low}$(px) $\downarrow$ & \textbf{DSR(\%)} $\uparrow$ & IOU $\uparrow$ & Diff$_{up}$(px) $\downarrow$ & Diff$_{low}$(px) $\downarrow$ & \textbf{DSR(\%)} $\uparrow$ & IOU $\uparrow$ & Diff$_{up}$(px) $\downarrow$ & Diff$_{low}$(px) $\downarrow$\\
\hline
Yolo11-White & $\times$ & 52.50 & 0.81$\pm$0.18 & 5.8$\pm$4.8 & 97.2$\pm$119.3 & 8.06 & 0.69$\pm$0.08 & 5.9$\pm$3.9 & 2.5$\pm$1.1 & 51.39 & 0.88$\pm$0.15 & 93.2$\pm$116.7 & \underline{3.9$\pm$6.6}\\
Yolo11-Blue & $\times$ & 24.72 & 0.94$\pm$0.03 & 9.7$\pm$6.1 & \underline{3.1$\pm$3.6} & 38.06 & 0.75$\pm$0.07 & \underline{4.0$\pm$2.3} & \textbf{1.9$\pm$1.4} & 2.22 & 0.96$\pm$0.01 & 6.4$\pm$2.6 & 6.0$\pm$3.7\\
Dual Yolo Concat & $\checkmark$ & 86.94 & 0.97$\pm$0.03 & \textbf{5.2$\pm$3.7} & 5.9$\pm$19.2 & 67.78 & \underline{0.75$\pm$0.07} & 4.1$\pm$2.7 & 2.5$\pm$1.8 & 49.72 & \textbf{0.98$\pm$0.01} & \underline{4.9$\pm$5.2} & 5.2$\pm$2.4\\
Dual Yolo Weighted & $\checkmark$ & \underline{99.72} & \textbf{0.97$\pm$0.01} & \underline{5.3$\pm$6.9} & 4.4$\pm$6.4 & \underline{75.56} & 0.73$\pm$0.07 & 4.6$\pm$3.0 & 2.7$\pm$2.3 & \underline{99.44} & \textbf{0.98$\pm$0.01} & 5.3$\pm$3.2 & 6.3$\pm$7.3\\
\hline
\textbf{Dual Yolo CrossAttn} & $\checkmark$ & \textbf{100.00} & \textbf{0.97$\pm$0.01} & 5.4$\pm$3.4 & \textbf{3.6$\pm$5.6} & \textbf{96.39} & \textbf{0.76$\pm$0.06} & \textbf{3.7$\pm$2.2} & \underline{2.4$\pm$1.8} & \textbf{100.00} & \textbf{0.98$\pm$0.01} & \underline{3.4$\pm$2.5} & \textbf{3.1$\pm$2.4}\\
\hline
\end{tabular}%
} % end resizebox
\end{table*}


\begin{table*}[t]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.8}
\caption{Comparison of Our Dual-Illumination Cross-Attention Fusion Against Other Approaches Across Plasma, Buffy Coat and Erythrocyte Segmentation Tasks in Terms of Various Metrics at conf=0.5}
\label{tab:main_conf50}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c c c c c c c c}
\hline
\multicolumn{2}{c}{} & \multicolumn{4}{c}{\textbf{Plasma Segmentation}} & \multicolumn{4}{c}{\textbf{Buffy Coat Segmentation}} & \multicolumn{4}{c}{\textbf{Erythrocyte Segmentation}}\\
\cline{3-6}\cline{7-10}\cline{11-14}
Method & Dual-backbone & \textbf{DSR(\%)} $\uparrow$ & IOU $\uparrow$ & Diff$_{up}$(px) $\downarrow$ & Diff$_{low}$(px) $\downarrow$ & \textbf{DSR(\%)} $\uparrow$ & IOU $\uparrow$ & Diff$_{up}$(px) $\downarrow$ & Diff$_{low}$(px) $\downarrow$ & \textbf{DSR(\%)} $\uparrow$ & IOU $\uparrow$ & Diff$_{up}$(px) $\downarrow$ & Diff$_{low}$(px) $\downarrow$\\
\hline
Yolo11-White & $\times$ & 56.67 & 0.81$\pm$0.18 & 5.7$\pm$4.7 & 96.7$\pm$118.7 & 4.72 & 0.72$\pm$0.06 & 5.0$\pm$3.5 & 1.5$\pm$0.8 & 47.50 & 0.89$\pm$0.14 & 80.7$\pm$111.5 & \underline{4.1$\pm$7.3}\\
Yolo11-Blue & $\times$ & 72.78 & 0.94$\pm$0.03 & 9.6$\pm$7.8 & \underline{3.8$\pm$5.6} & \underline{66.67} & 0.75$\pm$0.07 & 4.1$\pm$2.4 & \textbf{1.9$\pm$1.4} & 10.28 & 0.96$\pm$0.01 & 4.9$\pm$2.9 & 7.7$\pm$9.5\\
Dual Yolo Concat & $\checkmark$ & 93.06 & 0.96$\pm$0.04 & \underline{5.1$\pm$3.7} & 7.0$\pm$25.2 & 39.17 & \textbf{0.77$\pm$0.05} & \underline{3.8$\pm$2.8} & 2.4$\pm$1.5 & 96.67 & \textbf{0.98$\pm$0.01} & 4.7$\pm$5.1 & 5.2$\pm$2.5\\
Dual Yolo Weighted & $\checkmark$ & \underline{98.61} & \textbf{0.97$\pm$0.01} & \textbf{5.0$\pm$5.0} & 4.4$\pm$6.4 & 43.89 & 0.74$\pm$0.07 & 4.2$\pm$2.8 & 2.7$\pm$2.2 & \textbf{100.00} & \textbf{0.98$\pm$0.01} & \textbf{3.3$\pm$3.2} & 6.4$\pm$7.4\\
\hline
\textbf{Dual Yolo CrossAttn} & $\checkmark$ & \textbf{100.00} & \textbf{0.97$\pm$0.01} & 5.4$\pm$3.4 & \textbf{3.6$\pm$5.6} & \textbf{90.28} & \underline{0.77$\pm$0.06} & \textbf{3.7$\pm$2.2} & \underline{2.3$\pm$1.8} & \textbf{100.00} & \textbf{0.98$\pm$0.01} & \underline{3.4$\pm$2.5} & \textbf{3.2$\pm$2.4}\\
\hline
\end{tabular}%
}
\end{table*}


This section presents comprehensive evaluation of the DIUA-YOLO framework across multiple dimensions: quantitative performance metrics, ablation studies, qualitative visualization, and attention mechanism analysis. We evaluate our method against single-modality baselines and alternative fusion strategies under the stringent Detection Success Rate (DSR) criterion, which mandates exact detection of all three blood layers without false positives or negatives.


\subsection{Single-Modality vs. Dual-Modality Performance}

Table~\ref{tab:main_conf25} presents the primary evaluation results at confidence threshold 0.25, comparing single-modality baselines (Yolo11-White, Yolo11-Blue) against dual-modality fusion variants. The results demonstrate substantial performance gains from dual-illumination architectures while revealing complementary strengths of each modality.

Single-modality methods exhibit distinct performance characteristics reflecting their illumination-specific advantages. Yolo11-Blue achieves 24.72\% DSR for plasma segmentation with high IoU (0.94$\pm$0.03), benefiting from blue-light enhancement of layer boundaries. Conversely, its erythrocyte layer DSR remains critically low (2.22\%), as blue illumination provides limited structural information for red blood cell visualization. Yolo11-White demonstrates inverse behavior: 52.50\% plasma DSR but only 8.06\% buffy coat DSR, confirming that white-light imaging struggles with the optically thin buffy coat layer despite adequate overall structural representation.

Dual-modality architectures consistently outperform single-modality baselines across all metrics, validating our hypothesis that cross-spectral fusion addresses complementary deficiencies. Among dual-backbone methods, our Dual Yolo CrossAttn method achieves optimal performance with 100\% DSR for both plasma and erythrocyte segmentation, and 96.39\% DSR for the challenging buffy coat layer. This represents absolute improvements of +75.28\% (plasma), +96.39\% (buffy coat), and +97.78\% (erythrocyte) over the better-performing single-modality baseline for each respective class.

Geometric precision metrics corroborate detection success rates. Cross-attention fusion maintains IoU of 0.97$\pm$0.01 for plasma and 0.98$\pm$0.01 for erythrocyte layers, matching or exceeding single-modality performance while achieving substantially higher detection completeness. Boundary displacement metrics reveal sub-5-pixel accuracy: Diff$_{up}$ = 5.4$\pm$3.4 px and Diff$_{low}$ = 3.6$\pm$5.6 px for plasma; Diff$_{up}$ = 3.4$\pm$2.5 px and Diff$_{low}$ = 3.1$\pm$2.4 px for erythrocyte. For the buffy coat layer—whose thickness typically spans 20--40 pixels—achieving Diff$_{up}$ = 3.7$\pm$2.2 px demonstrates clinically acceptable localization precision.

Table~\ref{tab:main_conf50} presents results at higher confidence threshold (0.5), where the performance gap between single-modality and dual-modality methods becomes more pronounced. Cross-attention fusion maintains 100\% DSR for plasma and erythrocyte layers, while buffy coat DSR decreases marginally to 90.28\%. This threshold-dependent behavior reflects the inherent detection difficulty of thin buffy coat structures, yet dual-modality methods remain substantially more robust than single-modality alternatives.

\begin{table*}[t]
\centering
\scriptsize
\renewcommand{\arraystretch}{1.8}
\caption{Segmentation Metrics Across Plasma, Buffy Coat and Erythrocyte Segmentation Tasks (conf=0.001, IoU@0.5)}
\label{tab:academic_metrics}
\resizebox{\textwidth}{!}{%
\begin{tabular}{l c c c c c c c c c c c c c c c c c c c c}
\hline
\multicolumn{2}{c}{} & \multicolumn{6}{c}{\textbf{Plasma Segmentation}} & \multicolumn{6}{c}{\textbf{Buffy Coat Segmentation}} & \multicolumn{6}{c}{\textbf{Plasma Segmentation}}\\
\cline{3-8}\cline{9-14}\cline{15-20}
Method & Dual-backbone & mAP@[0.5:0.95] & AP@0.75 & AP@0.50 & F1 & Prec & Recall & mAP@[0.5:0.95] & AP@0.75 & AP@0.50 & F1 & Prec & Recall & mAP@[0.5:0.95] & A@0.P75 & AP@0.50 & F1 & Prec & Recall\\
\hline
Yolo11-White & $\times$ & 48.17 & 47.56 & 65.85 & 62.55 & 56.35 & 70.28 & 7.15 & 1.84 & 17.18 & 18.82 & 44.39 & 11.94 & 55.75 & 50.49 & 73.35 & 71.68 & 84.53 & 62.23\\
Yolo11-Blue & $\times$ & 87.52 & 97.33 & 96.90 & 96.20 & 94.87 & 97.57 & 53.17 & 47.35 & \underline{97.38} & \underline{90.34} & \underline{95.69} & 85.56 & 93.24 & 98.57 & \textbf{99.50} & 73.09 & 57.60 & \textbf{100.00}\\
Dual Yolo Concat & $\checkmark$ & 98.80 & 99.65 & 99.39 & 94.88 & 90.71 & 99.44 & \underline{55.22} & \textbf{57.77} & 90.86 & 84.17 & 87.79 & 80.83 & 98.52 & 98.53 & 98.49 & 91.69 & 86.09 & 98.06\\
Dual Yolo Weighted & $\checkmark$ & \underline{99.36} & \underline{99.95} & \textbf{99.50} & \textbf{99.96} & \textbf{100.00} & \underline{99.92} & 50.31 & 38.62 & 95.06 & 88.83 & 87.70 & \underline{90.00} & \textbf{99.68} & \textbf{99.50} & \textbf{99.50} & \underline{99.39} & \underline{98.78} & \textbf{100.00}\\
\hline
\textbf{Dual Yolo CrossAttn} & $\checkmark$ & \textbf{99.57} & \textbf{100.00} & \textbf{99.50} & \underline{99.63} & \underline{99.25} & \textbf{100.00} & \textbf{55.71} & \underline{54.53} & \textbf{99.50} & \textbf{98.67} & \textbf{100.00} & \textbf{97.37} & \underline{99.50} & \textbf{99.50} & \textbf{99.50} & \textbf{99.89} & \textbf{99.78} & \textbf{100.00}\\
\hline

\end{tabular}%
} % end resizebox
\end{table*}

\subsection{mAP Metrics and Segmentation Quality}

Table~\ref{tab:academic_metrics} presents traditional detection metrics evaluated at low confidence threshold (conf=0.001, IoU@0.5) to comprehensively assess segmentation quality across the precision-recall spectrum. These metrics provide complementary insights to DSR by evaluating model performance across varying confidence levels.

For plasma segmentation, Dual Yolo CrossAttn achieves mAP@[0.5:0.95] of 99.57\%, AP@0.75 of 100\%, and AP@0.50 of 99.50\%, surpassing all baselines. The F1 score of 99.63\% with precision 99.25\% and recall 100\% indicates near-perfect segmentation of this relatively straightforward class. Single-modality methods show substantial performance degradation: Yolo11-White achieves only 48.17\% mAP@[0.5:0.95], while Yolo11-Blue reaches 87.52\%, confirming the superiority of blue illumination for structural boundary detection.

Buffy coat segmentation—the most challenging task—reveals the critical advantage of cross-modal attention. Our method achieves 55.71\% mAP@[0.5:0.95], 99.50\% AP@0.50, and 98.67\% F1 score with 100\% precision and 97.37\% recall. This represents a dramatic improvement over Yolo11-White (7.15\% mAP@[0.5:0.95], 17.18\% AP@0.50) and substantial gains over Yolo11-Blue (53.17\% mAP@[0.5:0.95], 97.38\% AP@0.50). The perfect precision (100\%) combined with high recall (97.37\%) demonstrates the model's ability to reliably localize buffy coat boundaries without generating false positives—essential for automated clinical deployment.

For erythrocyte segmentation, all methods achieve high AP@0.50 ($\geq$98.49\%), reflecting the relative ease of detecting this thick, visually distinct layer. Nevertheless, Dual Yolo CrossAttn maintains the highest F1 score and joint-best recall, ensuring complete layer coverage.

Figure~\ref{fig mechanics} visualizes average performance across all methods, illustrating that dual-modality approaches consistently dominate across DSR, IoU, and academic metrics. The cross-attention method exhibits the most balanced profile, achieving near-ceiling performance across all metrics without the instability observed in weighted fusion or the modest performance plateau of channel concatenation.


\begin{figure}[!t]
\centerline{\includegraphics[width=\columnwidth]{img/metrics.png}}
\caption{Comparison of average detection and segmentation metrics across all methods. Detection sucess rate and IoU are from conf=0.25.}
\label{fig mechanics}
\end{figure}


\subsection{Fusion Strategy Comparison}

Among dual-modality architectures, fusion mechanism design critically determines performance. We compare three strategies of increasing sophistication: channel concatenation, adaptive weighted fusion, and cross-modal attention.

\textbf{Channel Concatenation} (Dual Yolo Concat) provides baseline dual-modality fusion by directly concatenating feature channels followed by convolutional compression. This approach achieves 86.94\% plasma DSR and 67.78\% buffy coat DSR at conf=0.25 (Table~\ref{tab:main_conf25}), substantially outperforming single-modality methods but falling short of more advanced fusion mechanisms. The limited performance reflects the method's inability to model explicit cross-modal correspondences or adaptively weight modality-specific contributions, relying instead on learned convolutional filters to implicitly extract complementary information.

\textbf{Adaptive Weighted Fusion} (Dual Yolo Weighted) employs learnable spatial and global attention weights to modulate modality contributions. This method achieves 99.72\% plasma DSR and 99.44\% erythrocyte DSR—approaching cross-attention performance for these classes. However, buffy coat DSR reaches only 75.56\%, revealing the method's limitations in handling optically challenging structures. Training stability also poses concerns: the weighted fusion mechanism exhibited convergence difficulties during early epochs, requiring careful training hyperparameter tuning and sometimes producing degenerate solutions where one modality dominates.

\textbf{Cross-Modal Attention} (Dual Yolo CrossAttn) implements unidirectional token-level attention with localized spatial matching. This design achieves the highest DSR across all classes: 100\% for serum/plasma, 96.39\% for buffy coat, and 100\% for erythrocyte at conf=0.25. The substantial buffy coat improvement (+20.83\% over weighted fusion, +28.61\% over concatenation) demonstrates the value of explicit semantic correspondence modeling. By allowing blue-light features to query relevant white-light context within local neighborhoods, the attention mechanism effectively resolves ambiguities in thin-layer localization while maintaining computational efficiency through token-based spatial partitioning.

Geometric precision metrics show consistent trends: cross-attention achieves the most balanced upper/lower boundary errors across all classes, with Diff$_{up}$ and Diff$_{low}$ typically within 3--6 pixels. This uniformity suggests the attention mechanism successfully integrates complementary spatial information rather than simply favoring one modality.

\begin{table*}[!t]
\caption{Ablation Experiments on the Blood Fractionation Component Segmentation Task at conf=0.25}
\label{ablation}
\centering
\setlength{\tabcolsep}{12pt}
\renewcommand{\arraystretch}{1.8}
\begin{tabular}{c c c c c c}
\hline
\multirow{2}{*}{\textbf{No.}} &
\multicolumn{3}{c}{\textbf{Settings}} &
\multirow{2}{*}{\textbf{Detection Rate(\%) $\uparrow$}} &
\multirow{2}{*}{\textbf{IOU $\uparrow$}} \\
\cline{2-4}
 & Dual-backbone & Dual-illumination fusion method & Pre-training & \\
\hline
1 & $\times$     & None                     & From Scratch      & 21.67 & 0.88$\pm$0.22 \\
2 & $\checkmark$ & Channel Concatenation    & From Scratch      & 52.40 & 0.87$\pm$0.23 \\
3 & $\checkmark$ & Channel Concatenation    & Transfer Learning & 68.15 & \underline{}{0.90$\pm$0.21} \\
4 & $\checkmark$ & Adaptive Weighted        & From Scratch      & 83.20 & 0.86$\pm$0.20 \\
5 & $\checkmark$ & Adaptive Weighted        & Transfer Learning & 91.57 & 0.89$\pm$0.17 \\
6 & $\checkmark$ & Cross Modal Attention    & From Scratch      & \underline{95.10} & 0.89$\pm$0.18 \\
7 & $\checkmark$ & Cross Modal Attention    & Transfer Learning & \textbf{98.80} & \textbf{0.90$\pm$0.14} \\
\hline
\end{tabular}
\end{table*}

\subsection{Ablation Studies}

Table~\ref{ablation} presents systematic ablation experiments evaluating three critical design factors: dual-backbone architecture, fusion mechanism, and initialization strategy. All experiments were conducted at conf=0.25 under identical training protocols with 360 test samples. Statistical significance was assessed using two-proportion Z-tests to rigorously validate performance differences.

\textbf{Dual-Backbone Necessity.} Comparing experiments 1 (single-backbone) and 2--7 (dual-backbone variants) reveals the fundamental importance of independent modality-specific feature extraction. Single-backbone architecture achieves only 21.67\% detection rate on total three classes. Introducing dual backbones with even the simplest fusion (channel concatenation, experiment 2) improves detection rate to 52.40\%, representing a statistically significant +30.73 percentage point improvement (p$<$0.001, Z=-8.54), validating the core architectural principle.

\textbf{Fusion Mechanism Progression.} Experiments 2--3 (concatenation), 4--5 (weighted fusion), and 6--7 (cross-attention) demonstrate progressive performance gains with increasingly sophisticated fusion designs. Under scratch training, cross-modal attention (experiment 6) achieves 95.10\% detection rate and 0.89$\pm$0.18 IoU, representing +42.70\% and +11.90\% absolute improvements over concatenation (p$<$0.001, Z=-13.00) and weighted fusion (p$<$0.001, Z=-5.11) respectively. These highly significant differences validate our hypothesis that explicit semantic correspondence modeling through attention mechanisms better captures cross-modal complementarity than implicit feature blending.

\textbf{Transfer Learning Impact.} Comparing scratch (even-numbered experiments) versus pretrained (odd-numbered experiments) initialization reveals consistent benefits from hierarchical weight transfer. Transfer learning improves detection rates by +15.75\% (concatenation, p$<$0.001), +8.37\% (weighted fusion, p$<$0.001), and +3.70\% (cross-attention, p$<$0.01). The diminishing marginal benefit for more sophisticated fusion methods suggests that cross-attention's architectural advantages partially compensate for initialization quality. Nevertheless, all pretrain effects remain statistically significant, and the pretrained cross-attention model (experiment 7) achieves the best overall performance: 98.80\% detection rate and 0.90$\pm$0.14 IoU, confirming the value of combining advanced fusion mechanisms with domain-adapted initialization.

\begin{figure*}[!t]
\centering
\newlength{\sepSa}\setlength{\sepSa}{0.8pt}   % 小列间距
\newlength{\sepLa}\setlength{\sepLa}{10pt}    % 大列间距（用于 2↔3 与 6↔7）
\newlength{\sepYa}\setlength{\sepYa}{3pt}     % 行间距
\newlength{\colwa}\setlength{\colwa}{\dimexpr(\linewidth - 4\sepSa - 2\sepLa)/7\relax}
\newcommand{\Img}[1]{%
  \includegraphics[width=\colwa,trim=2pt 2pt 2pt 2pt,clip]{#1}%
}
\newcommand{\Row}[7]{%
  \noindent
  \Img{#1}\hspace{\sepSa}%
  \Img{#2}\hspace{\sepLa}% 2↔3 大间距
  \Img{#3}\hspace{\sepSa}%
  \Img{#4}\hspace{\sepSa}%
  \Img{#5}\hspace{\sepSa}%
  \Img{#6}\hspace{\sepLa}% 6↔7 大间距
  \Img{#7}\par
}
\Row{evaluation/2022-03-28_103204_17_T3_2410.jpg}{evaluation/2022-03-28_103204_17_T5_2412.jpg}{evaluation/2022-03-28_103204_17_T5_2412_0_no_detection_id.jpg}{evaluation/2022-03-28_103204_17_T5_2412_0_no_detection_id_white.jpg}{evaluation/2022-03-28_103204_17_T5_2412_0_evaluation_concat.jpg}{evaluation/2022-03-28_103204_17_T5_2412_0_no_detection_cross.jpg}{evaluation/2022-03-28_103204_17_T5_2412_0_evaluation_best.jpg}\vspace{\sepYa}
\Row{evaluation/2022-03-28_143344_64_T3_2432.jpg}{evaluation/2022-03-28_143344_64_T5_2434.jpg}{evaluation/2022-03-28_143344_64_T5_2434_7_no_detection_id.jpg}{evaluation/2022-03-28_143344_64_T5_2434_7_no_detection_id_white.jpg}{evaluation/2022-03-28_143344_64_T5_2434_7_no_detection_concat.jpg}{evaluation/2022-03-28_143344_64_T5_2434_7_no_detection_cross.jpg}{evaluation/2022-03-28_143344_64_T5_2434_7_evaluation_best.jpg}\vspace{\sepYa}
\Row{evaluation/2022-04-07_141933_73_T3_2520.jpg}{evaluation/2022-04-07_141933_73_T5_2522.jpg}{evaluation/2022-04-07_141933_73_T5_2522_0_no_detection_id.jpg}{evaluation/2022-04-07_141933_73_T5_2522_0_no_detection_id_white.jpg}{evaluation/2022-04-07_141933_73_T5_2522_0_no_detection_concat.jpg}{evaluation/2022-04-07_141933_73_T5_2522_0_no_detection_cross.jpg}{evaluation/2022-04-07_141933_73_T5_2522_0_evaluation_best.jpg}\vspace{\sepYa}
\Row{evaluation/2022-04-15_084806_41_T3_2436.jpg}{evaluation/2022-04-15_084806_41_T5_2438.jpg}{evaluation/2022-04-15_084806_41_T5_2438_4_no_detection_id.jpg}{evaluation/2022-04-15_084806_41_T5_2438_4_no_detection_id_white.jpg}{evaluation/2022-04-15_084806_41_T5_2438_4_evaluation_concat.jpg}{evaluation/2022-04-15_084806_41_T5_2438_4_evaluation_cross.jpg}{evaluation/2022-04-15_084806_41_T5_2438_4_evaluation_best.jpg}
\noindent
\makebox[\colwa][c]{\small Photo}\hspace{\sepSa}%
\makebox[\colwa][c]{\small Ground Truth}\hspace{\sepLa}%
\makebox[\colwa][c]{\small Yolo11-White}\hspace{\sepSa}%
\makebox[\colwa][c]{\small Yolo11-Blue}\hspace{\sepSa}%
\makebox[\colwa][c]{\parbox{\colwa}{\centering \small Dual Yolo\\Channel Concat}}%
\makebox[\colwa][c]{\parbox{\colwa}{\centering \small Dual Yolo\\Adaptive Weighted}}%
\makebox[\colwa][c]{\parbox{\colwa}{\centering \small Dual Yolo\\Cross Attention}}%
\caption{Demonstrations of segmentation results from different models for the Blood Fractionation Component Segmentation task. In the figure, from top to bottom, the components are plasma, buffy coat, and red blood cell, respectively. From the third column, yellow, green, and blue points represent the segmented regions for each class. The regions outlined by white contours are the boundaries formed by the respective methods. From left to right, the columns display the following: original photos for the testing tube, ground truth annotated by professional blood testing personnel, Yolo11-White result, Yolo11-Blue result, Dual Yolo Channel Concatenation Fusion result, Dual Yolo Adaptive Weighted Fusion result, and Dual Yolo Cross Attention result.}
\label{fig:evaluation-grid}
\end{figure*}

\subsection{Qualitative Visualization}

Figure~\ref{fig:evaluation-grid} presents representative segmentation results across four test samples, comparing ground truth annotations against predictions from all methods. The visualizations reveal both qualitative performance differences and failure modes.

\textbf{Plasma Segmentation.} All dual-modality methods produce accurate serum/plasma boundaries closely matching ground truth clinical annotation points (rows 1--4, columns 5--7). Yolo11-White occasionally generates irregular upper boundaries due to meniscus reflection artifacts (row 1, column 3), while Yolo11-Blue exhibits slight over-segmentation extending into the buffy coat region (row 2, column 4). Cross-attention fusion produces the most consistent boundaries, effectively suppressing both artifact types through complementary information integration.

\textbf{Buffy Coat Detection.} The thin buffy coat layer (green regions) presents the greatest challenge, with substantial inter-method variation. Yolo11-White frequently fails to detect this layer entirely (rows 1--4, column 3), consistent with its 8.06\% buffy coat DSR. Yolo11-Blue performs better but occasionally produces fragmented detections or merged boundaries with adjacent layers. Concatenation and weighted fusion methods show improved consistency but exhibit thickness estimation errors—either compressing the layer too thinly or expanding into neighboring regions.

Cross-attention fusion (column 7) demonstrates superior buffy coat localization across all samples, producing contiguous masks with accurate upper and lower boundaries. The white boundary contours closely align with ground truth annotations, with typical deviations under 5 pixels—sufficient for clinical volume estimation. This consistent performance across varying buffy coat thicknesses (ranging from $\sim$15 pixels to $\sim$35 pixels) validates the attention mechanism's ability to adaptively leverage blue-light contrast enhancement while consulting white-light structural context.

\textbf{Erythrocyte Segmentation.} All methods achieve accurate erythrocyte layer detection (blue regions) in most cases, reflecting this class's high visual distinctiveness. However, single-modality methods occasionally exhibit lower boundary estimation errors when tube bottoms create optical discontinuities. Dual-modality methods consistently produce stable lower boundaries by integrating complementary depth cues.

\section{Discussion}

\subsection{Clinical Significance}

The DIUA-YOLO framework achieves 98.80\% overall detection success rate at clinical operating threshold (conf=0.25), meeting the stringent requirement for automated blood tube analysis. The sub-5-pixel boundary localization accuracy translates to volume estimation errors below 0.5\% for typical 10mL tubes—well within acceptable clinical tolerance. Most critically, the 96.39\% buffy coat detection rate represents a transformative improvement for white blood cell quantification, as this layer contains leukocytes essential for immune system assessment and disease diagnosis.

The automated platform processes a complete tube (dual-illumination acquisition, inference, result reporting) in approximately 8 seconds—enabling throughput exceeding 450 tubes per hour. This represents a 3.75$\times$ speedup over manual visual inspection and layer annotation workflows (typically 30 seconds per tube), while eliminating inter-operator variability. For high-volume clinical laboratories processing thousands of samples daily, such automation reduces greatly labor requirements, translating to substantial cost savings while improving result consistency and throughput.

Integration with the mechanical gripper system enables full workflow automation from tube loading to result reporting. The SQLite database logging provides full traceability, while the platform's modular design allows deployment in standard laboratory environments without specialized infrastructure. However, comprehensive clinical field deployment and validation remain essential next steps. Future work should conduct prospective studies in operational clinical laboratories to assess system robustness under real-world conditions, including diverse operator workflows, environmental variations, and sustained high-throughput operation.

\subsection{Understanding Cross-Modal Fusion Performance}

The substantial performance gap between fusion strategies—particularly cross-attention's superiority over concatenation and weighted fusion—can be attributed to their differing abilities to handle spatial misalignment between dual-modality inputs. Despite mechanical fixture stabilization, residual misalignment inevitably arises from parallax effects, chromatic aberration, and temporal offset during sequential illumination switching. These subtle displacements, critically affect boundary-sensitive detection tasks where layer interfaces span only a few pixels.

Channel concatenation operates under a strict pixel-to-pixel correspondence assumption, directly merging features at identical spatial coordinates. This rigid alignment makes concatenation vulnerable to misregistration: a blue-light edge feature may correspond to slightly shifted white-light context, causing the subsequent convolutional layers to receive semantically inconsistent input. Adaptive weighted fusion partially mitigates this limitation through learned spatial attention, enabling the network to suppress contributions from misaligned regions. However, its pixel-wise weighting remains constrained to fixed spatial positions, preventing active search for correct correspondences.

Cross-modal attention fundamentally addresses misalignment through its token-based neighborhood mechanism. By partitioning features into tokens and allowing each blue-light query token to attend to a local neighborhood of white-light key tokens, the attention operation performs implicit local feature alignment. When a blue-light boundary token encounters spatial offset, it can retrieve semantically matching white-light features from neighboring positions within the $k \times k$ window rather than being forced to use the strictly co-located feature. This spatial flexibility is particularly critical for thin-layer boundaries such as the buffy coat, where minor pixel misalignment can shift edge responses beyond concatenation's receptive field but remain within attention's neighborhood range. The localized search mechanism thus enables robust cross-modal correspondence even under imperfect registration, explaining the dramatic performance improvements observed in ablation studies.

\subsection{Limitations and Future Directions}

Despite strong performance, several limitations warrant further investigation. First, the current system assumes pre-centrifuged tubes with established layer stratification. Fresh whole blood or partially centrifuged samples may not exhibit clear boundaries, potentially degrading detection performance. Future work should investigate time-series imaging during centrifugation to model dynamic layer formation.

Second, the dataset comprises 90 samples covering typical clinical scenarios but may not fully represent rare pathological conditions such as severe anemia (reduced erythrocyte layer), leukemia (expanded buffy coat), or hemolysis (layer disruption). Expanding the dataset to include rare disease phenotypes and conducting clinical validation studies across diverse patient populations remains essential for regulatory approval and widespread deployment.

Third, while dual-illumination imaging demonstrates clear advantages, the sequential white-light and blue-light acquisition requires 2--3 seconds, introducing potential motion artifacts if tubes are not mechanically stabilized. Simultaneous dual-spectrum imaging using beam-splitting optics or specialized sensors could reduce acquisition time and eliminate registration errors, though at increased hardware complexity and cost.

Fourth, the high-resolution dual-modality input (1504$\times$1504 pixels, 6 channels) demands substantial GPU memory during training, limiting batch size to 2 per RTX 3090 GPU. While the cross-modal attention mechanism itself is computationally efficient relative to global attention, the large spatial dimensions necessitate careful memory management. Future work could explore input resolution reduction strategies, gradient checkpointing, or mixed-precision training to increase batch sizes and potentially improve convergence. Additionally, model compression techniques such as knowledge distillation or quantization-aware training could enable deployment on edge devices for point-of-care settings.

Fifth, current evaluation focuses on layer detection and boundary localization but does not perform cell counting or morphological analysis within the buffy coat layer. Integrating microscopic imaging or cell classification sub-networks could provide complete blood count (CBC) equivalent information, enhancing clinical utility. Preliminary experiments with SAM-based cell segmentation within detected buffy coat regions show promise but require substantial annotation effort.

Finally, the system currently operates on vacuum blood collection tubes, which maintain consistent geometry. Extending to capillary tubes, micro-centrifuge tubes, or non-standard containers would require retraining and potentially redesigning the gripper mechanism. Transfer learning experiments on these alternative tube types represent important future validation.

\subsection{Broader Impact}

Beyond blood tube analysis, the dual-illumination cross-attention architecture demonstrates general applicability to multi-modal medical imaging tasks where complementary spectral information enhances diagnostic accuracy. Potential extensions include dual-modality endoscopy (white-light + narrow-band imaging for polyp detection), dermatological imaging (visible + UV for skin lesion analysis), and histopathology (H\&E + immunofluorescence for tissue classification).

The unidirectional attention design and token-based spatial partitioning offer computational advantages for high-resolution medical images ($\geq$1500$\times$1500 pixels), where global attention becomes prohibitively expensive. By constraining attention to local neighborhoods adaptive to pyramid level, the framework scales to large images while maintaining fine-grained feature interactions—addressing a key bottleneck in medical image analysis.

From a methodological perspective, this work contributes to the broader understanding of multi-modal fusion in deep learning. The comparative evaluation of concatenation, weighted fusion, and attention mechanisms provides empirical evidence that explicit correspondence modeling through attention yields superior performance compared to implicit fusion approaches, particularly for challenging detection tasks involving thin or low-contrast structures. These insights generalize beyond medical imaging to applications such as RGB-D fusion for robotics, multi-spectral satellite imagery analysis, and sensor fusion for autonomous systems.