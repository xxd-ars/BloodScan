# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license
"""Dual-modal fusion modules for YOLO."""

import torch
import torch.nn as nn
import torch.nn.functional as F

from .conv import Conv

__all__ = ("ConcatCompress", "WeightedFusion", "CrossModalAttention", "MultiLayerCrossModalAttention")


class ConcatCompress(nn.Module):
    """
    Concatenation followed by compression fusion module.
    
    This module concatenates features from two modalities and then compresses
    them back to the original channel dimension using 1x1 convolution.
    """

    def __init__(self, c1, c2=None):
        """
        Initialize ConcatCompress module.
        
        Args:
            c1 (int): Number of input channels (for each modality).
            c2 (int, optional): Number of output channels. If None, defaults to c1.
        """
        super().__init__()
        if c2 is None:
            c2 = c1
        self.c1 = c1
        self.c2 = c2
        # ä½¿ç”¨1x1å·ç§¯å°†é€šé“æ•°ä»c1*2å‹ç¼©åˆ°c2
        # c1*2: è¾“å…¥é€šé“æ•°(ä¸¤ä¸ªæ¨¡æ€ç‰¹å¾æ‹¼æ¥åçš„é€šé“æ•°)
        # c2: è¾“å‡ºé€šé“æ•°(å‹ç¼©åçš„é€šé“æ•°)
        # kernel_size=1: 1x1å·ç§¯
        # stride=1: æ­¥é•¿ä¸º1
        self.compress = Conv(c1 + c2, (c1 + c2)//2, 1, 1)
    
    def forward(self, x):
        """
        Forward pass of ConcatCompress.
        
        Args:
            x (list): List containing two feature tensors [blue_feat, white_feat].
            
        Returns:
            torch.Tensor: Fused feature tensor.
        """
        # Concatenate along channel dimension
        x = torch.cat(x, 1)
        # Compress back to original channels
        return self.compress(x)


class WeightedFusion(nn.Module):
    """
    Spatial and content-aware weighted fusion module.
    
    This module learns adaptive weights based on both spatial patterns and global
    content to fuse features from two modalities intelligently.
    """

    def __init__(self, c1, c2=None):
        """
        Initialize WeightedFusion module.
        
        Args:
            c1 (int): Number of input channels (for each modality).
            c2 (int, optional): Number of output channels. If None, defaults to c1.
        """
        super().__init__()
        if c2 is None:
            c2 = c1
        self.c1 = c1
        self.c2 = c2
        
        # Spatial weight prediction network using standard Conv modules
        self.spatial_conv1 = Conv(c1 * 2, c1 // 4, 3, 1)
        self.spatial_conv2 = Conv(c1 // 4, c1 // 8, 3, 1)
        self.spatial_out = nn.Sequential(
            nn.Conv2d(c1 // 8, 1, 1),
            nn.Sigmoid()
        )
        
        # å…¨å±€ä¸Šä¸‹æ–‡é¢„æµ‹å™¨ - ä½¿ç”¨è‡ªé€‚åº”å¹³å‡æ± åŒ–å°†ç‰¹å¾å›¾å‹ç¼©ä¸º1x1
        # AdaptiveAvgPool2d(1)ä¼šè‡ªåŠ¨è®¡ç®—æ± åŒ–çª—å£å¤§å°,å°†ä»»æ„è¾“å…¥å°ºå¯¸çš„ç‰¹å¾å›¾æ± åŒ–ä¸º1x1
        # è¿™æ ·å¯ä»¥è·å–å…¨å±€çš„ç‰¹å¾ä¿¡æ¯,ç”¨äºé¢„æµ‹å…¨å±€æƒé‡
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        
        # ä½¿ç”¨1x1å·ç§¯å¤„ç†å…¨å±€ç‰¹å¾,ä¸ä½¿ç”¨BNé¿å…è®­ç»ƒé—®é¢˜
        # è¾“å…¥é€šé“æ•°ä¸ºc1*2(æ‹¼æ¥çš„åŒæ¨¡æ€ç‰¹å¾),è¾“å‡ºé€šé“æ•°ä¸ºc1//4 
        self.global_conv = nn.Sequential(
            nn.Conv2d(c1 * 2, c1 // 4, 1),  # 1x1å·ç§¯é™ç»´
            nn.SiLU()  # SiLUæ¿€æ´»å‡½æ•°
        )
        
        # æœ€åè¾“å‡ºå•é€šé“çš„å…¨å±€æƒé‡,ä½¿ç”¨Sigmoidå½’ä¸€åŒ–åˆ°0-1
        self.global_out = nn.Sequential(
            nn.Conv2d(c1 // 4, 1, 1),  # 1x1å·ç§¯å¾—åˆ°æƒé‡
            nn.Sigmoid()  # Sigmoidå½’ä¸€åŒ–
        )
        
        # Learnable temperature parameter
        self.temperature = nn.Parameter(torch.ones(1))
        
        # Output projection if needed
        if c1 != c2:
            self.proj = Conv(c1, c2, 1, 1)
        else:
            self.proj = nn.Identity()
    
    def forward(self, x):
        """
        Forward pass of WeightedFusion.
        
        Args:
            x (list): List containing two feature tensors [blue_feat, white_feat].
            
        Returns:
            torch.Tensor: Fused feature tensor with adaptive weighting.
        """
        blue_feat, white_feat = x
        
        # Concatenate features for weight prediction
        concat_feat = torch.cat([blue_feat, white_feat], dim=1)
        
        # Predict spatial-aware weights
        spatial_weight = self.spatial_conv1(concat_feat)
        spatial_weight = self.spatial_conv2(spatial_weight)
        spatial_weight = self.spatial_out(spatial_weight)
        
        # Predict global content-aware weight
        global_feat = self.global_pool(concat_feat)
        global_weight = self.global_conv(global_feat)
        global_weight = self.global_out(global_weight)
        
        # Combine spatial and global weights with temperature scaling
        final_weight = spatial_weight * global_weight * self.temperature
        
        # Apply weighted fusion: w * blue + (1-w) * white
        fused_feat = final_weight * blue_feat + (1 - final_weight) * white_feat
        
        # Apply output projection
        return self.proj(fused_feat)


class CrossModalAttention(nn.Module):
    """
    è·¨æ¨¡æ€æ³¨æ„åŠ›æ¨¡å—ï¼šè“å…‰ç‰¹å¾æŸ¥è¯¢ç™½å…‰ç‰¹å¾è¿›è¡Œè‡ªæˆ‘å¢å¼º
    Cross-modal attention module for blue light querying white light features.
    
    æ ¸å¿ƒæ€æƒ³ï¼šè“å…‰ç‰¹å¾ä½œä¸º"é—®é¢˜"(Query)ï¼Œå»ç™½å…‰ç‰¹å¾ä¸­å¯»æ‰¾"ç­”æ¡ˆ"(Key-Value)ï¼Œ
    é€šè¿‡æ³¨æ„åŠ›æƒé‡æ¥é€‰æ‹©æ€§åœ°èåˆç™½å…‰ä¿¡æ¯ï¼Œä»è€Œå¢å¼ºè“å…‰ç‰¹å¾ã€‚
    Blue light features act as queries to enhance themselves using white light features.
    Uses token-based local attention for efficiency.
    
    ä½¿ç”¨åŸºäºTokençš„å±€éƒ¨æ³¨æ„åŠ›æ¥æé«˜è®¡ç®—æ•ˆç‡ã€‚
    """

    def __init__(self, c1, c2=None, token_size=4, neighbor_size=3):
        """
        Initialize CrossModalAttention module.
        
        Args:
            c1 (int): Number of input channels.
            c2 (int, optional): Number of output channels. If None, defaults to c1.
            token_size (int): Token size for spatial partitioning. (default 4x4 pixels)
            neighbor_size (int): Neighbor search range for local attention. (default 3x3 neighbors)
        """
        super().__init__()
        if c2 is None:
            c2 = c1
        self.c1 = c1
        self.c2 = c2
        self.token_size = token_size
        self.neighbor_size = neighbor_size
        
        # å¯è§†åŒ–ç›¸å…³å±æ€§
        self.enable_visualization = False
        self.attention_maps = None
        self.last_input_size = None
        
        # Projection layers: å°†ç‰¹å¾æŠ•å½±åˆ°Queryã€Keyã€Valueç©ºé—´
        self.q_proj = nn.Conv2d(c1, c1, 1) # Blue  light -> Query
        self.k_proj = nn.Conv2d(c1, c1, 1) # White light -> Key
        self.v_proj = nn.Conv2d(c1, c1, 1) # White light -> Value

        ## Scale factor
        # self.scale = (c1 // (token_size * token_size)) ** -0.5
        
        # Corrected scale factor
        self.scale = (token_size * token_size) ** -0.5
        
        # Output projection
        self.out_proj = Conv(c1, c2, 1, 1) if c1 != c2 else nn.Identity()

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­ï¼šå®ç°è·¨æ¨¡æ€æ³¨æ„åŠ›ç‰¹å¾å¢å¼º
        Forward pass of CrossModalAttention.
        
        Args:
            x (list): List containing [blue_feat, white_feat] tensors.
            
        Returns:
            torch.Tensor: Enhanced blue features.
        """
        blue_feat, white_feat = x
        B, C, H, W = blue_feat.shape
        
        # ä¿å­˜è¾“å…¥å°ºå¯¸ç”¨äºå¯è§†åŒ–
        if self.enable_visualization:
            self.last_input_size = (H, W)
        
        # ============================================================================
        # æ­¥éª¤1ï¼šç‰¹å¾æŠ•å½± - ç”ŸæˆQueryã€Keyã€Value
        # ============================================================================
        # Qï¼šè“å…‰ç‰¹å¾çš„"é—®é¢˜" - "æˆ‘éœ€è¦ä»€ä¹ˆæ ·çš„ä¿¡æ¯ï¼Ÿ"
        # Kï¼šç™½å…‰ç‰¹å¾çš„"ç´¢å¼•" - "æˆ‘æœ‰ä»€ä¹ˆæ ·çš„ä¿¡æ¯ï¼Ÿ"  
        # Vï¼šç™½å…‰ç‰¹å¾çš„"å†…å®¹" - "æˆ‘çš„å…·ä½“ä¿¡æ¯æ˜¯ä»€ä¹ˆï¼Ÿ"
        Q = self.q_proj(blue_feat)   # [B, C, H, W] - è“å…‰çš„éœ€æ±‚
        K = self.k_proj(white_feat)  # [B, C, H, W] - ç™½å…‰çš„ç´¢å¼•
        V = self.v_proj(white_feat)  # [B, C, H, W] - ç™½å…‰çš„å†…å®¹
        
        # ============================================================================
        # æ­¥éª¤2ï¼šå¡«å……å¤„ç† - ç¡®ä¿å°ºå¯¸èƒ½è¢«token_sizeæ•´é™¤
        # ============================================================================
        pad_h = (self.token_size - H % self.token_size) % self.token_size
        pad_w = (self.token_size - W % self.token_size) % self.token_size
        if pad_h > 0 or pad_w > 0:
            Q = F.pad(Q, (0, pad_w, 0, pad_h))
            K = F.pad(K, (0, pad_w, 0, pad_h))
            V = F.pad(V, (0, pad_w, 0, pad_h))
            H_pad, W_pad = Q.shape[2], Q.shape[3]
        else:
            H_pad, W_pad = H, W
        
        # ============================================================================
        # æ­¥éª¤3ï¼šç‰¹å¾TokenåŒ– - å°†ç‰¹å¾å›¾åˆ†å‰²æˆå°å—è¿›è¡Œå¤„ç†
        # ============================================================================
        # ç›®çš„ï¼šé™ä½è®¡ç®—å¤æ‚åº¦ï¼Œä»åƒç´ çº§åˆ«çš„O(N^2)é™åˆ°Tokençº§åˆ«çš„O(M^2)
        # å…¶ä¸­N=H*Wï¼ˆåƒç´ æ•°ï¼‰ï¼ŒM=num_tokensï¼ˆTokenæ•°ï¼‰ï¼Œé€šå¸¸M << N
        
        num_h, num_w = H_pad // self.token_size, W_pad // self.token_size  # Tokenç½‘æ ¼å¤§å°
        token_dim = self.token_size * self.token_size  # æ¯ä¸ªTokençš„ç‰¹å¾ç»´åº¦
        
        # é‡å¡‘æ“ä½œè¯¦è§£ï¼š[B, C, H, W] -> [B, num_h, num_w, C, token_dim]
        # 1. view: å°†Hç»´åº¦åˆ†å‰²ä¸º(num_h, token_size)ï¼ŒWç»´åº¦åˆ†å‰²ä¸º(num_w, token_size)
        # 2. permute: é‡æ–°æ’åˆ—ç»´åº¦ï¼Œå°†tokençš„ç©ºé—´ç»´åº¦ç§»åˆ°æœ€å
        # 3. reshape: å°†tokenå†…çš„ç©ºé—´ç»´åº¦å±•å¹³ä¸ºä¸€ç»´
        Q_tokens = Q.view(B, C, num_h, self.token_size, num_w, self.token_size).permute(0, 2, 4, 1, 3, 5).reshape(B, num_h, num_w, C, token_dim)
        K_tokens = K.view(B, C, num_h, self.token_size, num_w, self.token_size).permute(0, 2, 4, 1, 3, 5).reshape(B, num_h, num_w, C, token_dim)
        V_tokens = V.view(B, C, num_h, self.token_size, num_w, self.token_size).permute(0, 2, 4, 1, 3, 5).reshape(B, num_h, num_w, C, token_dim)

        # ============================================================================
        # æ­¥éª¤4ï¼šæ ¸å¿ƒæ³¨æ„åŠ›è®¡ç®— - è¿™é‡Œå®ç°ç‰¹å¾å¢å¼ºçš„å…³é”®é€»è¾‘
        # ============================================================================
        enhanced_tokens = self._local_attention_vectorized(Q_tokens, K_tokens, V_tokens)
        
        # ============================================================================
        # æ­¥éª¤5ï¼šç‰¹å¾é‡ç»„ - å°†å¢å¼ºåçš„Tokené‡æ–°ç»„è£…æˆç‰¹å¾å›¾
        # ============================================================================
        # é€†å‘æ“ä½œï¼š[B, num_h, num_w, C, token_dim] -> [B, C, H, W]
        
        # 5a. æ¢å¤Tokenå†…çš„ç©ºé—´ç»“æ„ï¼štoken_dim -> (token_size, token_size)
        enhanced_tokens = enhanced_tokens.reshape(B, num_h, num_w, C, self.token_size, self.token_size)
        
        # 5b. é‡æ–°æ’åˆ—ç»´åº¦ï¼šå°†é€šé“ç»´åº¦ç§»åˆ°å‰é¢ï¼Œä¸ºæœ€ç»ˆé‡å¡‘åšå‡†å¤‡
        enhanced_tokens = enhanced_tokens.permute(0, 3, 1, 4, 2, 5)  # [B, C, num_h, token_size, num_w, token_size]
        
        # 5c. åˆå¹¶Tokenç½‘æ ¼å’ŒTokenå†…éƒ¨ç©ºé—´ç»´åº¦ï¼Œé‡å»ºå®Œæ•´ç‰¹å¾å›¾
        enhanced_feat = enhanced_tokens.reshape(B, C, H_pad, W_pad)
        
        # 5d. å»é™¤ä¹‹å‰æ·»åŠ çš„å¡«å……
        if pad_h > 0 or pad_w > 0:
            enhanced_feat = enhanced_feat[:, :, :H, :W]
        
        # ============================================================================
        # æ­¥éª¤6ï¼šæ®‹å·®è¿æ¥ - ç‰¹å¾å¢å¼ºçš„æœ€ç»ˆå®ç°
        # ============================================================================
        # å…³é”®ï¼šæˆ‘ä»¬ä¸æ˜¯æ›¿æ¢åŸå§‹ç‰¹å¾ï¼Œè€Œæ˜¯åœ¨å…¶åŸºç¡€ä¸Šæ·»åŠ æ³¨æ„åŠ›å¢å¼ºçš„ä¿¡æ¯
        # enhanced_featï¼šä»ç™½å…‰ä¸­å­¦åˆ°çš„ã€ç»è¿‡æ³¨æ„åŠ›ç­›é€‰çš„æœ‰ç”¨ä¿¡æ¯
        # blue_featï¼šåŸå§‹è“å…‰ç‰¹å¾
        # ä¸¤è€…ç›¸åŠ ï¼šåŸå§‹ä¿¡æ¯ + è·¨æ¨¡æ€å¢å¼ºä¿¡æ¯ = å¢å¼ºåçš„ç‰¹å¾
        return self.out_proj(blue_feat + enhanced_feat)
    
    def _local_attention_vectorized(self, Q, K, V):
        """
        ä½¿ç”¨å‘é‡åŒ–å®ç°å±€éƒ¨æ³¨æ„åŠ›è®¡ç®—
        
        è¿™æ˜¯æ•´ä¸ªæ¨¡å—çš„æ ¸å¿ƒï¼šå®ç°"ç”¨æ³¨æ„åŠ›åŠ æƒçš„ç™½å…‰ä¿¡æ¯å¢å¼ºè“å…‰ç‰¹å¾"
        
        Args:
            Q: [B, num_h, num_w, C, token_dim] - è“å…‰Query tokens
            K: [B, num_h, num_w, C, token_dim] - ç™½å…‰Key tokens  
            V: [B, num_h, num_w, C, token_dim] - ç™½å…‰Value tokens
            
        Returns:
            [B, num_h, num_w, C, token_dim] - å¢å¼ºåçš„tokens
        """
        B, num_h, num_w, C, token_dim = Q.shape
        num_tokens = num_h * num_w
        
        # ========================================================================
        # å­æ­¥éª¤1ï¼šæå–å±€éƒ¨é‚»åŸŸ - å®ç°"å±€éƒ¨çº¦æŸ"çš„å…³é”®
        # ========================================================================
        # ä½¿ç”¨unfoldé«˜æ•ˆæå–æ¯ä¸ªä½ç½®çš„é‚»åŸŸä¿¡æ¯ï¼Œé¿å…Pythonå¾ªç¯
        kH = kW = self.neighbor_size  # é‚»åŸŸå¤§å°ï¼ˆå¦‚3x3ï¼‰
        padding = self.neighbor_size // 2  # è¾¹ç•Œå¡«å……
        
        # é‡å¡‘Kå’ŒVä¸ºé€‚åˆunfoldçš„æ ¼å¼ï¼š[B, C*token_dim, num_h, num_w]
        K_reshaped = K.permute(0, 3, 4, 1, 2).reshape(B, C * token_dim, num_h, num_w)
        V_reshaped = V.permute(0, 3, 4, 1, 2).reshape(B, C * token_dim, num_h, num_w)

        # unfoldæ“ä½œï¼šä¸ºæ¯ä¸ªä½ç½®æå–å…¶é‚»åŸŸçª—å£
        # ç»“æœå½¢çŠ¶ï¼š[B, C*token_dim*neighbor_size^2, num_tokens]
        K_unfolded = F.unfold(K_reshaped, kernel_size=(kH, kW), padding=padding)
        V_unfolded = F.unfold(V_reshaped, kernel_size=(kH, kW), padding=padding)
        
        # é‡å¡‘ä¸ºæ–¹ä¾¿åç»­è®¡ç®—çš„æ ¼å¼
        num_neighbors = kH * kW  # é‚»åŸŸå†…çš„tokenæ•°é‡ï¼ˆå¦‚3x3=9ï¼‰
        K_neighbors = K_unfolded.reshape(B, C, token_dim, num_neighbors, num_tokens).permute(0, 4, 3, 1, 2)
        V_neighbors = V_unfolded.reshape(B, C, token_dim, num_neighbors, num_tokens).permute(0, 4, 3, 1, 2)
        # æœ€ç»ˆå½¢çŠ¶ï¼š[B, num_tokens, num_neighbors, C, token_dim]
        
        # å°†Qé‡å¡‘ä¸ºä¸neighborsåŒ¹é…çš„æ ¼å¼
        Q_flat = Q.reshape(B, num_tokens, 1, C, token_dim)  # [B, num_tokens, 1, C, token_dim]
        
        # ========================================================================
        # å­æ­¥éª¤2ï¼šè®¡ç®—æ³¨æ„åŠ›ç›¸ä¼¼åº¦ - "è“å…‰é—®é¢˜"ä¸"ç™½å…‰ç´¢å¼•"çš„åŒ¹é…ç¨‹åº¦
        # ========================================================================
        # è¿™é‡Œå›ç­”æ‚¨çš„é—®é¢˜ï¼š"è®¡ç®—å¾—åˆ°ç›¸ä¼¼åº¦ä¹‹åæˆ‘ä»¬åšä»€ä¹ˆï¼Ÿ"
        
        # 2a. è®¡ç®—ç‚¹ç§¯ç›¸ä¼¼åº¦
        # einsumè§£é‡Šï¼šå¯¹äºæ¯ä¸ªè“å…‰tokençš„queryï¼Œè®¡ç®—å®ƒä¸æ‰€æœ‰é‚»è¿‘ç™½å…‰tokençš„keyçš„ç›¸ä¼¼åº¦
        # 'BLqCD, BLnCD -> BLCn' è¡¨ç¤ºï¼š
        # - B: batchç»´åº¦
        # - L: tokenä½ç½®ç»´åº¦  
        # - q: queryç»´åº¦(1)ï¼Œn: neighborç»´åº¦
        # - C: é€šé“ç»´åº¦ï¼ŒD: tokenç‰¹å¾ç»´åº¦
        # ç»“æœï¼šæ¯ä¸ªè“å…‰tokenå¯¹å…¶é‚»åŸŸå†…æ¯ä¸ªç™½å…‰tokençš„ç›¸ä¼¼åº¦åˆ†æ•°
        attn = torch.einsum('BLqCD, BLnCD -> BLCn', Q_flat, K_neighbors) * self.scale
        
        # 2b. Softmaxå½’ä¸€åŒ–ï¼šå°†ç›¸ä¼¼åº¦è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
        # å«ä¹‰ï¼šå¯¹äºæ¯ä¸ªè“å…‰tokenï¼Œå®ƒçš„é‚»åŸŸç™½å…‰tokensçš„é‡è¦æ€§æƒé‡æ€»å’Œä¸º1
        attn = F.softmax(attn, dim=-1)  # [B, num_tokens, C, num_neighbors]
        
        # ä¿å­˜æ³¨æ„åŠ›æƒé‡ç”¨äºå¯è§†åŒ–
        if self.enable_visualization:
            self.attention_maps = attn.detach().cpu()
        
        # ========================================================================
        # å­æ­¥éª¤3ï¼šåŠ æƒæ±‚å’Œ - å®ç°ç‰¹å¾å¢å¼ºçš„æ ¸å¿ƒæ“ä½œ
        # ========================================================================
        # è¿™ä¸€æ­¥å›ç­”äº†"å¦‚ä½•å¢å¼ºåŸæœ‰ç‰¹å¾"ï¼š
        
        # 3a. ç”¨æ³¨æ„åŠ›æƒé‡å¯¹ç™½å…‰Valueè¿›è¡ŒåŠ æƒæ±‚å’Œ
        # einsumè§£é‡Šï¼š
        # - attn[B,L,C,n]ï¼šæ¯ä¸ªä½ç½®Lã€é€šé“Cå¯¹é‚»åŸŸnçš„æ³¨æ„åŠ›æƒé‡
        # - V_neighbors[B,L,n,C,D]ï¼šé‚»åŸŸç™½å…‰tokençš„å…·ä½“ç‰¹å¾å†…å®¹
        # - ç»“æœ[B,L,C,D]ï¼šåŠ æƒèåˆåçš„ç‰¹å¾ï¼ŒåŒ…å«äº†æ¥è‡ªç™½å…‰çš„æœ‰ç”¨ä¿¡æ¯
        enhanced_token = torch.einsum('BLCn, BLnCD -> BLCD', attn, V_neighbors)
        
        # 3b. ç‰¹å¾å¢å¼ºçš„ç›´è§‚è§£é‡Šï¼š
        # - å¦‚æœæŸä¸ªç™½å…‰é‚»åŸŸtokenä¸å½“å‰è“å…‰tokenç›¸ä¼¼åº¦é«˜(attnå¤§)ï¼Œ
        #   é‚£ä¹ˆè¿™ä¸ªç™½å…‰tokençš„ç‰¹å¾(V)ä¼šåœ¨æœ€ç»ˆç»“æœä¸­å æ›´å¤§æ¯”é‡
        # - å¦‚æœç›¸ä¼¼åº¦ä½(attnå°)ï¼Œåˆ™è¯¥ç™½å…‰ç‰¹å¾çš„è´¡çŒ®å¾ˆå°
        # - æœ€ç»ˆç»“æœæ˜¯æ‰€æœ‰é‚»åŸŸç™½å…‰ç‰¹å¾çš„"æ™ºèƒ½æ··åˆ"ï¼Œæ··åˆæƒé‡ç”±ç›¸ä¼¼åº¦å†³å®š
        
        # é‡å¡‘å›åŸå§‹Tokenç½‘æ ¼æ ¼å¼
        return enhanced_token.reshape(B, num_h, num_w, C, token_dim)
    
    def get_attention_spatial_map(self, target_size=None):
        """
        å°†Tokençº§æ³¨æ„åŠ›æƒé‡æ˜ å°„åˆ°åŸå§‹å›¾åƒç©ºé—´
        
        Args:
            target_size (tuple): ç›®æ ‡å›¾åƒå°ºå¯¸ (H, W)ï¼Œé»˜è®¤ä½¿ç”¨last_input_size
            
        Returns:
            torch.Tensor: ç©ºé—´æ³¨æ„åŠ›å›¾ [B, H, W] æˆ– Noneï¼ˆå¦‚æœæœªå¯ç”¨å¯è§†åŒ–ï¼‰
        """
        if not self.enable_visualization or self.attention_maps is None:
            return None
        
        if target_size is None:
            target_size = self.last_input_size
        
        if target_size is None:
            return None
        
        H, W = target_size
        B, num_tokens, C, num_neighbors = self.attention_maps.shape
        
        # è®¡ç®—Tokenç½‘æ ¼å°ºå¯¸
        pad_h = (self.token_size - H % self.token_size) % self.token_size
        pad_w = (self.token_size - W % self.token_size) % self.token_size
        H_pad, W_pad = H + pad_h, W + pad_w
        num_h, num_w = H_pad // self.token_size, W_pad // self.token_size
        
        # å¯¹æ³¨æ„åŠ›æƒé‡è¿›è¡Œå¹³å‡èšåˆï¼š[B, num_tokens, C, num_neighbors] -> [B, num_tokens]
        # èšåˆç­–ç•¥ï¼šå¯¹é€šé“å’Œé‚»åŸŸç»´åº¦æ±‚å¹³å‡ï¼Œå¾—åˆ°æ¯ä¸ªtokenä½ç½®çš„æ•´ä½“æ³¨æ„åŠ›å¼ºåº¦
        attn_avg = self.attention_maps.mean(dim=(2, 3))  # [B, num_tokens]
        
        # é‡å¡‘ä¸ºTokenç½‘æ ¼ï¼š[B, num_tokens] -> [B, num_h, num_w]
        attn_grid = attn_avg.reshape(B, num_h, num_w)
        
        # ä¸Šé‡‡æ ·åˆ°åƒç´ ç©ºé—´ï¼š[B, num_h, num_w] -> [B, H_pad, W_pad]
        attn_spatial = F.interpolate(
            attn_grid.unsqueeze(1),  # æ·»åŠ é€šé“ç»´åº¦ [B, 1, num_h, num_w]
            size=(H_pad, W_pad),
            mode='bilinear',
            align_corners=False
        ).squeeze(1)  # ç§»é™¤é€šé“ç»´åº¦ [B, H_pad, W_pad]
        
        # è£å‰ªåˆ°åŸå§‹å°ºå¯¸
        if pad_h > 0 or pad_w > 0:
            attn_spatial = attn_spatial[:, :H, :W]
        
        return attn_spatial
    
    def enable_attention_visualization(self, enable=True):
        """å¯ç”¨/ç¦ç”¨æ³¨æ„åŠ›å¯è§†åŒ–"""
        self.enable_visualization = enable
        if not enable:
            self.attention_maps = None
            self.last_input_size = None


class MultiLayerCrossModalAttention(nn.Module):
    """
    å¤šå±‚è·¨æ¨¡æ€æ³¨æ„åŠ›æ¨¡å—ï¼šé€šè¿‡å¤šå±‚æ³¨æ„åŠ›å®ç°æ¸è¿›å¼ç‰¹å¾å¢å¼º
    Multi-layer cross-modal attention module for progressive feature enhancement.
    
    æ ¸å¿ƒè®¾è®¡ç†å¿µï¼š
    1. ç¬¬ä¸€å±‚ï¼šå»ºç«‹åŸºç¡€çš„è“å…‰-ç™½å…‰ç‰¹å¾å¯¹åº”å…³ç³»
    2. åç»­å±‚ï¼šåœ¨å‰ä¸€å±‚åŸºç¡€ä¸Šè¿›ä¸€æ­¥ç²¾åŒ–å’Œå¢å¼ºç‰¹å¾
    3. æ®‹å·®è¿æ¥ï¼šç¡®ä¿ä¿¡æ¯æµé€šå’Œæ¢¯åº¦ä¼ æ’­
    4. å±‚é—´ç‰¹å¾èåˆï¼šæ¯å±‚éƒ½èƒ½è·å¾—æ¥è‡ªç™½å…‰çš„æ–°ä¿¡æ¯
    
    ç›¸æ¯”å•å±‚æ³¨æ„åŠ›çš„ä¼˜åŠ¿ï¼š
    - æ¸è¿›å¼ç‰¹å¾ç²¾åŒ–ï¼šé€å±‚æå‡ç‰¹å¾è´¨é‡
    - æ›´å¼ºçš„è¡¨è¾¾èƒ½åŠ›ï¼šå¤šå±‚éçº¿æ€§å˜æ¢
    - å±‚æ¬¡åŒ–ä¿¡æ¯èåˆï¼šä¸åŒå±‚çº§æ•è·ä¸åŒå°ºåº¦çš„æ¨¡æ€å…³ç³»
    """

    def __init__(self, c1, c2=None, token_size=4, neighbor_size=3, num_layers=2):
        """
        Initialize MultiLayerCrossModalAttention module.
        
        Args:
            c1 (int): Number of input channels.
            c2 (int, optional): Number of output channels. If None, defaults to c1.
            token_size (int): Token size for spatial partitioning. (default 4x4 pixels)
            neighbor_size (int): Neighbor search range for local attention. (default 3x3 neighbors)
            num_layers (int): Number of attention layers. (default 2)
        """
        super().__init__()
        if c2 is None:
            c2 = c1
        self.c1 = c1
        self.c2 = c2
        self.token_size = token_size
        self.neighbor_size = neighbor_size
        self.num_layers = num_layers
        
        # åˆ›å»ºå¤šå±‚æ³¨æ„åŠ›æ¨¡å—
        self.attention_layers = nn.ModuleList()
        for i in range(num_layers):
            # æ¯ä¸€å±‚éƒ½æœ‰ç‹¬ç«‹çš„Qã€Kã€VæŠ•å½±
            layer = nn.ModuleDict({
                'q_proj': nn.Conv2d(c1, c1, 1),
                'k_proj': nn.Conv2d(c1, c1, 1), 
                'v_proj': nn.Conv2d(c1, c1, 1),
                'norm': nn.LayerNorm(c1),  # å±‚å½’ä¸€åŒ–ç¨³å®šè®­ç»ƒ
            })
            self.attention_layers.append(layer)
        
        # Scale factor
        self.scale = (token_size * token_size) ** -0.5
        
        # ä¸­é—´å±‚çš„ç‰¹å¾èåˆæƒé‡ï¼ˆå­¦ä¹ å¦‚ä½•ç»„åˆä¸åŒå±‚çš„ä¿¡æ¯ï¼‰
        if num_layers > 1:
            self.layer_weights = nn.Parameter(torch.ones(num_layers) / num_layers)
        
        # Output projection
        self.out_proj = Conv(c1, c2, 1, 1) if c1 != c2 else nn.Identity()

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­ï¼šå®ç°å¤šå±‚è·¨æ¨¡æ€æ³¨æ„åŠ›ç‰¹å¾å¢å¼º
        
        Args:
            x (list): List containing [blue_feat, white_feat] tensors.
            
        Returns:
            torch.Tensor: Enhanced blue features through multi-layer attention.
        """
        blue_feat, white_feat = x
        B, C, H, W = blue_feat.shape
        
        # è®°å½•æ¯ä¸€å±‚çš„è¾“å‡ºï¼Œç”¨äºæœ€ç»ˆçš„åŠ æƒèåˆ
        layer_outputs = []
        
        # å½“å‰å¤„ç†çš„è“å…‰ç‰¹å¾ï¼ˆä¼šåœ¨æ¯å±‚æ›´æ–°ï¼‰
        current_blue = blue_feat
        
        # é€å±‚å¤„ç†
        for layer_idx, layer_modules in enumerate(self.attention_layers):
            # ================================================================
            # ç¬¬ä¸€å±‚ï¼šä½¿ç”¨åŸå§‹è“å…‰å’Œç™½å…‰ç‰¹å¾
            # åç»­å±‚ï¼šä½¿ç”¨å¢å¼ºåçš„è“å…‰ç‰¹å¾æŸ¥è¯¢åŸå§‹ç™½å…‰ç‰¹å¾
            # ================================================================
            
            # ç‰¹å¾æŠ•å½±
            Q = layer_modules['q_proj'](current_blue)  # æŸ¥è¯¢ï¼šå½“å‰è“å…‰ç‰¹å¾
            K = layer_modules['k_proj'](white_feat)    # é”®ï¼šå§‹ç»ˆä½¿ç”¨åŸå§‹ç™½å…‰ç‰¹å¾
            V = layer_modules['v_proj'](white_feat)    # å€¼ï¼šå§‹ç»ˆä½¿ç”¨åŸå§‹ç™½å…‰ç‰¹å¾
            
            # æ‰§è¡Œå•å±‚æ³¨æ„åŠ›è®¡ç®—
            enhanced_feat = self._single_layer_attention(Q, K, V, H, W)
            
            # å±‚å½’ä¸€åŒ–ï¼ˆåœ¨é€šé“ç»´åº¦ä¸Šï¼‰
            # éœ€è¦å°†ç‰¹å¾reshapeä¸º [B, H*W, C] æ ¼å¼è¿›è¡ŒLayerNorm
            B_norm, C_norm, H_norm, W_norm = enhanced_feat.shape
            enhanced_feat_norm = enhanced_feat.permute(0, 2, 3, 1).reshape(B_norm, H_norm * W_norm, C_norm)
            enhanced_feat_norm = layer_modules['norm'](enhanced_feat_norm)
            enhanced_feat = enhanced_feat_norm.reshape(B_norm, H_norm, W_norm, C_norm).permute(0, 3, 1, 2)
            
            # æ®‹å·®è¿æ¥ï¼šå½“å‰è“å…‰ç‰¹å¾ + æœ¬å±‚å¢å¼ºç‰¹å¾
            current_blue = current_blue + enhanced_feat
            
            # ä¿å­˜æœ¬å±‚è¾“å‡ºç”¨äºæœ€ç»ˆèåˆ
            layer_outputs.append(enhanced_feat)
        
        # ================================================================
        # å¤šå±‚ä¿¡æ¯èåˆï¼šåŠ æƒç»„åˆä¸åŒå±‚çš„å¢å¼ºä¿¡æ¯
        # ================================================================
        if self.num_layers > 1:
            # ä½¿ç”¨å¯å­¦ä¹ æƒé‡èåˆä¸åŒå±‚çš„è¾“å‡º
            # ç†å¿µï¼šæ—©æœŸå±‚æ•è·åŸºç¡€å¯¹åº”ï¼Œæ·±å±‚æ•è·å¤æ‚å…³ç³»
            weighted_enhancement = torch.zeros_like(layer_outputs[0])
            for i, layer_output in enumerate(layer_outputs):
                weighted_enhancement += self.layer_weights[i] * layer_output
            
            # æœ€ç»ˆç»“æœï¼šåŸå§‹è“å…‰ + åŠ æƒèåˆçš„å¤šå±‚å¢å¼ºä¿¡æ¯
            final_output = blue_feat + weighted_enhancement
        else:
            # å•å±‚æƒ…å†µï¼šç›´æ¥ä½¿ç”¨å½“å‰è“å…‰ç‰¹å¾
            final_output = current_blue
        
        return self.out_proj(final_output)
    
    def _single_layer_attention(self, Q, K, V, H, W):
        """
        å•å±‚æ³¨æ„åŠ›è®¡ç®—ï¼ˆå¤ç”¨CrossModalAttentionçš„é€»è¾‘ï¼‰
        
        Args:
            Q: Query features [B, C, H, W]
            K: Key features [B, C, H, W]  
            V: Value features [B, C, H, W]
            H, W: Original height and width
            
        Returns:
            torch.Tensor: Enhanced features from this attention layer
        """
        B, C = Q.shape[:2]
        
        # å¡«å……å¤„ç†
        pad_h = (self.token_size - H % self.token_size) % self.token_size
        pad_w = (self.token_size - W % self.token_size) % self.token_size
        if pad_h > 0 or pad_w > 0:
            Q = F.pad(Q, (0, pad_w, 0, pad_h))
            K = F.pad(K, (0, pad_w, 0, pad_h))
            V = F.pad(V, (0, pad_w, 0, pad_h))
            H_pad, W_pad = Q.shape[2], Q.shape[3]
        else:
            H_pad, W_pad = H, W
        
        # TokenåŒ–
        num_h, num_w = H_pad // self.token_size, W_pad // self.token_size
        token_dim = self.token_size * self.token_size
        
        Q_tokens = Q.view(B, C, num_h, self.token_size, num_w, self.token_size).permute(0, 2, 4, 1, 3, 5).reshape(B, num_h, num_w, C, token_dim)
        K_tokens = K.view(B, C, num_h, self.token_size, num_w, self.token_size).permute(0, 2, 4, 1, 3, 5).reshape(B, num_h, num_w, C, token_dim)
        V_tokens = V.view(B, C, num_h, self.token_size, num_w, self.token_size).permute(0, 2, 4, 1, 3, 5).reshape(B, num_h, num_w, C, token_dim)

        # æ³¨æ„åŠ›è®¡ç®—
        enhanced_tokens = self._local_attention_vectorized(Q_tokens, K_tokens, V_tokens)
        
        # ç‰¹å¾é‡ç»„
        enhanced_tokens = enhanced_tokens.reshape(B, num_h, num_w, C, self.token_size, self.token_size)
        enhanced_tokens = enhanced_tokens.permute(0, 3, 1, 4, 2, 5)
        enhanced_feat = enhanced_tokens.reshape(B, C, H_pad, W_pad)
        
        # å»é™¤å¡«å……
        if pad_h > 0 or pad_w > 0:
            enhanced_feat = enhanced_feat[:, :, :H, :W]
        
        return enhanced_feat
    
    def _local_attention_vectorized(self, Q, K, V):
        """
        å±€éƒ¨æ³¨æ„åŠ›è®¡ç®—ï¼ˆä¸CrossModalAttentionç›¸åŒçš„å®ç°ï¼‰
        """
        B, num_h, num_w, C, token_dim = Q.shape
        num_tokens = num_h * num_w
        
        # æå–å±€éƒ¨é‚»åŸŸ
        kH = kW = self.neighbor_size
        padding = self.neighbor_size // 2
        
        K_reshaped = K.permute(0, 3, 4, 1, 2).reshape(B, C * token_dim, num_h, num_w)
        V_reshaped = V.permute(0, 3, 4, 1, 2).reshape(B, C * token_dim, num_h, num_w)

        K_unfolded = F.unfold(K_reshaped, kernel_size=(kH, kW), padding=padding)
        V_unfolded = F.unfold(V_reshaped, kernel_size=(kH, kW), padding=padding)
        
        num_neighbors = kH * kW
        K_neighbors = K_unfolded.reshape(B, C, token_dim, num_neighbors, num_tokens).permute(0, 4, 3, 1, 2)
        V_neighbors = V_unfolded.reshape(B, C, token_dim, num_neighbors, num_tokens).permute(0, 4, 3, 1, 2)
        
        Q_flat = Q.reshape(B, num_tokens, 1, C, token_dim)
        
        # è®¡ç®—æ³¨æ„åŠ›
        attn = torch.einsum('BLqCD, BLnCD -> BLCn', Q_flat, K_neighbors) * self.scale
        attn = F.softmax(attn, dim=-1)
        
        # åŠ æƒæ±‚å’Œ
        enhanced_token = torch.einsum('BLCn, BLnCD -> BLCD', attn, V_neighbors)
        
        return enhanced_token.reshape(B, num_h, num_w, C, token_dim) 