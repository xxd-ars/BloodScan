import re, os, time, random, datetime, asyncio, configparser, json, nest_asyncio
from openai import OpenAI
from src.utils.log import setup_logger
from typing import List
import types
import openai

logger = setup_logger(__name__)

config = configparser.ConfigParser()
config.read("./src/config/config.ini", encoding="utf-8")

OPENAI_API_KEY = "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"

pr_price_dict = {
    "gpt-4o-2024-11-20": 2.5,
    "gpt-4.1": 2,
    "gpt-4.1-mini": 0.4,
    "gpt-4.1-nano": 0.1,

    "deepseek-chat": 0.27,
    "deepseek-reasoner": 0.55,
}
rt_price_dict = {
    "gpt-4o-2024-11-20": 10,
    "gpt-4.1": 8,
    "gpt-4.1-mini": 1.6,
    "gpt-4.1-nano": 0.4,
    
    "deepseek-chat": 1.1,
    "deepseek-reasoner": 2.19,
}

def message_quailifer():
    return ["hello", "world"]

async def async_llm_reasoning(model, system_message, user_message, task_name,
                              temperature, max_tokens, max_retries = 5, 
                              LONG_MODE = False, STREAM_MODE = False):
    try:
        GEMINI_DOWN = False
        time_stamp  = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        prompt_path = f"results/app_prompts/{task_name}_{time_stamp}.txt"
        prompt  = f"""Prompt: \nsystem_message: {system_message},\nuser_message: {user_message}"""
        
        with open(prompt_path, "w", encoding="utf-8") as file:
            file.write(prompt)
        # logger.info(f"Model in use: {model}")
        logger.info(f"llm prompt saved to {prompt_path}")

        if LONG_MODE:
            client = anthropic.Anthropic(api_key=CLAUDE_API_KEY)
            with client.beta.messages.stream(
                model = model,
                max_tokens = max_tokens,
                thinking={
                    "type": "enabled",
                    "budget_tokens": 5000},
                system=system_message,
                messages=[{"role": "user", "content": user_message,}],
                betas=["token-efficient-tools-2025-02-19","output-128k-2025-02-19"]) as stream:
                for event in stream:
                    if event.type == "content_block_start":
                        logger.info(f"\nStarting {event.content_block.type} block...")
                    elif event.type == "content_block_delta":
                        if event.delta.type == "thinking_delta":
                            # logger.info(f"Thinking {event.delta.thinking}")
                            yield event.delta.thinking, ""
                            await asyncio.sleep(0)
                        elif event.delta.type == "text_delta":
                            yield "", event.delta.text
                            await asyncio.sleep(0)
                    elif event.type == "content_block_stop":
                        logger.info("\nBlock complete.")
        
        elif 'gemini' in model:
            retries = 0
            while retries < max_retries:
                try:
                    if 'thinking' in model:
                        client_gem = genai.Client(api_key=GEMINI_API_KEY)
                    else:
                        client_gem = genai.Client(api_key=GEMINI_API_KEY,
                                                  http_options={'api_version':'v1alpha'})
                    response = client_gem.models.generate_content(
                        model = model, 
                        contents = user_message,
                        config = {
                            'system_instruction': system_message,
                            'temperature': temperature,
                            'max_output_tokens': max_tokens,
                        }
                    )
                    result = response.text
                    if result == None:
                        GEMINI_DOWN = True
                    break
                except Exception as e:
                    retries += 1
                    logger.warning(f"Geminiè°ƒç”¨, é”™è¯¯: {e}, ç¬¬ {retries} æ¬¡é‡è¯•")
                    logger.warning(f"Geminiè¿”å›, {response}")
                    if retries < max_retries:
                        time.sleep(10)
            if retries == max_retries:
                GEMINI_DOWN = True
                model = "gpt-4o-2024-11-20"

        elif 'claude' in model:
            client = anthropic.Anthropic(api_key=CLAUDE_API_KEY)
            response = client.messages.create(
                model = model, 
                max_tokens=max_tokens, 
                temperature=temperature, 
                stream=STREAM_MODE,
                system = system_message,
                messages = [{"role": "user", "content": user_message,}])
            if STREAM_MODE:
                for event in response:
                    if event.type == "content_block_delta" and event.delta.type == "text_delta":
                        yield event.delta.text
                        await asyncio.sleep(0)
            else:
                result = response.content[0].text
                yield result

        elif 'ep' in model:
            client = OpenAI(
                base_url="https://ark.cn-beijing.volces.com/api/v3/",
                api_key=DEEPSK_API_KEY)
            response = client.chat.completions.create(
                 model = model,
                 max_tokens = max_tokens, 
                 temperature = temperature, 
                 stream = STREAM_MODE,
                 messages = [
                     {"role": "system", "content": system_message},
                     {"role": "user", "content": user_message}])
            if STREAM_MODE:
                yield "Thinking..."
                await asyncio.sleep(0)
                for chunk in response:
                    if chunk.choices[0].delta.content is not None:
                        yield chunk.choices[0].delta.content
                        await asyncio.sleep(0)
            else:
                result = response.choices[0].message.content
                yield result
            
        elif 'gpt' in model or GEMINI_DOWN:
            logger.info(f"GEMINI_DOWN is {GEMINI_DOWN}")
            client = OpenAI(api_key = OPENAI_API_KEY)
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_message},
                    {"role": "user", "content": user_message}
                ],
                temperature=temperature,
                max_tokens=max_tokens,
                stream = STREAM_MODE,
            )
            if STREAM_MODE:
                for chunk in response:
                    if chunk.choices[0].delta.content is not None:
                        yield chunk.choices[0].delta.content
                        await asyncio.sleep(0)
            else:
                result = response.choices[0].message.content
                yield result
        else:
            logger.error(f"wrong model input of {model}")
    except Exception as e:
        logger.error(f"{str(e)}")
        yield f"Error: {str(e)}"

async def llm_multiple_responses(model, system_message, user_message, task_name,
                                 temperature, max_tokens, n_responses):
    async def single_response(idx):
        """å•ä¸ªå“åº”çš„ç”Ÿæˆå™¨ï¼Œå¸¦ç¼–å·"""
        full_response = ""
        async for chunk in async_llm_reasoning(
            model, system_message, user_message, task_name,
            temperature, max_tokens, STREAM_MODE=True
        ):
            full_response += chunk or ""
            yield idx, chunk  # è¿”å›ç¼–å·å’Œ chunk

    generators = [single_response(i) for i in range(n_responses)]
    tasks = [asyncio.create_task(anext(gen)) for gen in generators]

    while tasks:
        done, pending = await asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED)
        new_tasks = []

        for task in done:
            try:
                idx, chunk = task.result()
                yield idx, chunk
                gen = generators[idx]
                new_tasks.append(asyncio.create_task(anext(gen)))
            except StopAsyncIteration:
                # å½“å‰ generator å·²å®Œæˆ
                pass

        tasks = list(pending) + new_tasks

async def get_chat_answers_batch(folder, api_key, messages, 
                                 model="gpt-4o-2024-11-20", 
                                 temperature=0,
                                 top_p=0.9,
                                 frequency_penalty=0,
                                 presence_penalty=0,
                                 response_format="text", 
                                 base_url="https://api.openai.com/v1",
                                 batch_wait_time=60):
    token_pr_price = pr_price_dict[model]/2
    token_rt_price = rt_price_dict[model]/2
    # åˆ›å»ºclientæ—¶è®¾ç½®è¶…æ—¶æ—¶é—´
    client = openai.OpenAI(
        api_key=api_key, 
        base_url=base_url,
        timeout=60.0,  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°60ç§’
        max_retries=3  # æ·»åŠ é‡è¯•æ¬¡æ•°
    )
    # 1. æ–°å»º JSONL æ–‡ä»¶
    batch_file_path = folder + "/batch_requests.jsonl"
    with open(batch_file_path, "w", encoding="utf-8") as f:
        for id, msg in enumerate(messages):
            request = {
                "custom_id": f"request-{id}",
                "method": "POST",
                "url": "/v1/chat/completions",
                "body": {
                    "model": model,
                    "messages": msg,
                    "temperature": temperature,
                    "max_tokens": 8000,
                    "response_format": {"type": response_format},
                    "top_p": top_p,
                    "frequency_penalty": frequency_penalty,
                    "presence_penalty": presence_penalty,
                }
            }
            f.write(json.dumps(request, ensure_ascii=False) + "\n")

    # 2. ä¸Šä¼  JSONL æ–‡ä»¶
    with open(batch_file_path, "rb") as file:
        upload_response = client.files.create(file=file, purpose="batch")
    file_id = upload_response.id
    logger.info(f"âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {file_id}")

    # 3. åˆ›å»º Batch ä»»åŠ¡
    batch_response = client.batches.create(
        input_file_id=file_id,
        endpoint="/v1/chat/completions",
        completion_window = "24h",
    )
    batch_id = batch_response.id
    logger.info(f"âœ… Batch ä»»åŠ¡åˆ›å»ºæˆåŠŸ: {batch_id}")

    # 4. è½®è¯¢ä»»åŠ¡çŠ¶æ€
    while True:
        try:
            batch_status = client.batches.retrieve(batch_id)
            status = batch_status.status
            
            if status == "in_progress" and hasattr(batch_status, "request_counts"):
                counts = batch_status.request_counts
 
                total = counts.total if hasattr(counts, "total") else 0
                completed = counts.completed if hasattr(counts, "completed") else 0
                failed = counts.failed if hasattr(counts, "failed") else 0 
                pending = total - completed - failed
                
                logger.info(f"â³ Batch ä»»åŠ¡çŠ¶æ€: {status}: {completed} å®Œæˆ - {failed} å¤±è´¥ - {pending} ç­‰å¾…ä¸­ - {total} æ€»æ•°")
            else:
                logger.info(f"â³ Batch ä»»åŠ¡çŠ¶æ€: {status}")

            if status in ["completed", "failed"]:
                break
                
        except openai.APITimeoutError:
            logger.warning(f"â— è·å–æ‰¹å¤„ç†çŠ¶æ€æ—¶è¶…æ—¶ï¼Œç­‰å¾…{batch_wait_time}ç§’åé‡è¯•")
        except Exception as e:
            logger.warning(f"â— è·å–æ‰¹å¤„ç†çŠ¶æ€æ—¶å‡ºé”™: {e}, ç­‰å¾…{batch_wait_time}ç§’åé‡è¯•")
        
        await asyncio.sleep(batch_wait_time)

    # 5. å¤„ç†ç»“æœ
    if status == "completed":
        try:
            output_file_id = batch_status.output_file_id
            output_file = client.files.content(output_file_id)
            logger.info(f"âœ… ä»»åŠ¡å®Œæˆ, ä¸‹è½½ç»“æœ: {output_file_id}")

            response = [json.loads(line)["response"]["body"]["choices"][0]["message"]["content"] for line in output_file.text.strip().split("\n")]
            token_pr_usage = sum([json.loads(line)["response"]["body"]["usage"]["prompt_tokens"] for line in output_file.text.strip().split("\n")])
            token_rt_usage = sum([json.loads(line)["response"]["body"]["usage"]["completion_tokens"] for line in output_file.text.strip().split("\n")])
            token_usd_usage= token_pr_usage/1000000*token_pr_price + token_rt_usage/1000000*token_rt_price
            logger.info(f"ğŸª™ ä»»åŠ¡æ¶ˆè€— {token_pr_usage} + {token_rt_usage} Token")
            logger.info(f"ğŸ’µ ä»»åŠ¡æ¶ˆè€— {token_usd_usage} USD")
            return response
        except Exception as e:
            logger.error(f"âŒ ä¸‹è½½æˆ–å¤„ç†ç»“æœæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            # å°è¯•æ¢å¤ï¼šé€šè¿‡batch_idæŸ¥è¯¢å¤„ç†ç»“æœ
            logger.info("ğŸ”„ å°è¯•é€šè¿‡å…¶ä»–æ–¹å¼è·å–æ‰¹å¤„ç†ç»“æœ...")
            # è¿™é‡Œå¯ä»¥æ·»åŠ å¤‡ç”¨è·å–ç»“æœçš„æ–¹æ³•
    
    logger.error("âŒ Batch ä»»åŠ¡å¤±è´¥")
    return []

def test(model, 
         api_key, 
         api_key_pool, 
         base_url = None, 
         temperature = 0.7, top_p = 1, frequency_penalty = 0.0, presence_penalty = 0.0,
         folder = "", BATCH_MODE = False):
    
    messages = list(message_quailifer())
    
    if BATCH_MODE:
        # âœ… è°ƒç”¨ batch ç‰ˆæœ¬
        nest_asyncio.apply()
        answers = asyncio.run(get_chat_answers_batch(
            folder = folder,
            api_key=api_key,  # å–ç¬¬ä¸€ä¸ª API Key
            messages=messages,
            model=model,
            temperature=temperature,
            top_p=top_p,
            frequency_penalty=frequency_penalty,
            presence_penalty=presence_penalty,
            base_url=base_url
        ))
    else:
        nest_asyncio.apply()
        # get_chat_answersç°åœ¨è¿”å›(contents, ask_result_dict)è€Œä¸æ˜¯(results, ask_result_dict)
        answers, _ = asyncio.run(get_chat_answers(
            api_key_pool=api_key_pool,
            base_url=base_url,
            model=model,
            messages=messages,
            temperature=temperature,
            top_p=top_p,
            frequency_penalty=frequency_penalty,
            presence_penalty=presence_penalty
        ))
    # # parse
    # results = []
    # for id, result in enumerate(answers):
    #     # print(result)
    #     try:
    #         extraction_result = json.loads(result)
    #         keywords = extraction_result.get('keywords', [])
    #         entities = extraction_result.get('entities', [])

    #         keywords_list = [item['keyword'] for item in keywords if item['relevance'] == 'é«˜']
    #         entities_list = [item['entity_name'] for item in entities]

    #         # print(f"æˆåŠŸæå–å…³é”®è¯: {keywords_list}")
    #         # print(f"æˆåŠŸæå–å®ä½“: {entities_list}")
    #         results.append(keywords_list + entities_list)
    #     except Exception as e:
    #         logger.error(f"LLM returns extractions error: {e}")
    #         results.append([])
    
    # return 
    return answers
    
def get_embedding_gemini(self, texts: List[str], title: str = None) -> List[List[float]]:
    """æ‰¹é‡è·å–Geminiæ¨¡å‹çš„æ–‡æœ¬åµŒå…¥å‘é‡
    
    Args:
        texts: è¾“å…¥æ–‡æœ¬åˆ—è¡¨
        title: æ–‡æ¡£æ ‡é¢˜
    Returns:
        åµŒå…¥å‘é‡åˆ—è¡¨ï¼Œæ¯ä¸ªæ–‡æœ¬å¯¹åº”ä¸€ä¸ªå‘é‡
    """
    if not self.gemini_client:
        raise ValueError("Geminiå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
    
    max_tokens = 8192  # æ¯ä¸ªchunkçš„æœ€å¤§æ€»é•¿åº¦é™åˆ¶
    max_text_length = max_tokens   # å•ä¸ªæ–‡æœ¬çš„æœ€å¤§é•¿åº¦é™åˆ¶
    max_texts_per_chunk = 5  # æ¯ä¸ªchunkæœ€å¤šåŒ…å«çš„æ–‡æœ¬æ•°
    rest_interval = 3       # æ¯å¤„ç†ä¸€ä¸ªchunkåçš„ä¼‘æ¯æ—¶é—´ï¼ˆç§’ï¼‰
    
    # APIå¯†é’¥æ± é…ç½®
    api_keys = self.api_keys
    if not api_keys or not api_keys[0]:
        raise ValueError("æœªé…ç½®æœ‰æ•ˆçš„APIå¯†é’¥")
    
    current_api_index = 0
    current_api_key = api_keys[current_api_index]
    
    # å°†æ–‡æœ¬åˆ†æˆå¤šä¸ªchunk
    chunks = []
    current_chunk = []
    current_chunk_length = 0
    
    for text in texts:
        # å¦‚æœå•ä¸ªæ–‡æœ¬è¶…è¿‡é™åˆ¶ï¼Œéœ€è¦æˆªæ–­
        if len(text) > max_text_length:
            logger.warning(f"å•ä¸ªæ–‡æœ¬é•¿åº¦({len(text)})è¶…è¿‡æœ€å¤§é™åˆ¶({max_text_length})ï¼Œå°†è¢«æˆªæ–­")
            text = text[:max_text_length]
        
        text_length = len(text)
        
        # å¦‚æœå½“å‰chunkå·²æ»¡ï¼ˆæ–‡æœ¬æ•°è¾¾åˆ°ä¸Šé™æˆ–æ€»é•¿åº¦è¶…è¿‡é™åˆ¶ï¼‰ï¼Œåˆ›å»ºæ–°chunk
        if (len(current_chunk) >= max_texts_per_chunk or 
            (current_chunk_length + text_length > max_tokens and current_chunk)):
            chunks.append(current_chunk)
            current_chunk = []
            current_chunk_length = 0
        
        # æ·»åŠ æ–‡æœ¬åˆ°å½“å‰chunk
        current_chunk.append(text)
        current_chunk_length += text_length
    
    # æ·»åŠ æœ€åä¸€ä¸ªchunkï¼ˆå¦‚æœæœ‰å†…å®¹ï¼‰
    if current_chunk:
        chunks.append(current_chunk)
    
    logger.info(f"å°†{len(texts)}ä¸ªæ–‡æœ¬åˆ†æˆ{len(chunks)}ä¸ªå—è¿›è¡Œå¤„ç†")
    
    # å¤„ç†æ¯ä¸ªchunk
    all_results = []
    for i, chunk in enumerate(chunks):
        logger.info(f"å¤„ç†ç¬¬{i+1}/{len(chunks)}ä¸ªå—ï¼ŒåŒ…å«{len(chunk)}ä¸ªæ–‡æœ¬ï¼Œæ€»é•¿åº¦çº¦{sum(len(t) for t in chunk)}å­—ç¬¦")
        
        # æ ‡è®°æ˜¯å¦æˆåŠŸå¤„ç†å½“å‰chunk
        chunk_processed = False
        api_attempts = 0
        
        # å°è¯•ä½¿ç”¨ä¸åŒçš„APIå¯†é’¥å¤„ç†å½“å‰chunk
        while not chunk_processed and api_attempts < len(api_keys):
            try:
                # æ›´æ–°å®¢æˆ·ç«¯ä½¿ç”¨å½“å‰APIå¯†é’¥
                self.gemini_client = genai.Client(api_key=current_api_key)
                logger.info(f"ä½¿ç”¨APIå¯†é’¥ #{current_api_index+1} å¤„ç†ç¬¬{i+1}ä¸ªå—")
                
                result = self.gemini_client.models.embed_content(
                    model=self.model_name,
                    contents=chunk,
                    config=types.EmbedContentConfig(
                        title=title,
                        task_type=self.task_type,
                        output_dimensionality=N_DIM)
                )
                
                # æå–åµŒå…¥å‘é‡
                chunk_embeddings = [embedding.values for embedding in result.embeddings]
                all_results.extend(chunk_embeddings)
                logger.info(f"æˆåŠŸå¤„ç†ç¬¬{i+1}ä¸ªå—ï¼Œè·å–äº†{len(chunk_embeddings)}ä¸ªåµŒå…¥å‘é‡")
                
                # æ ‡è®°æˆåŠŸå¤„ç†
                chunk_processed = True
                
            except Exception as e:
                # è®°å½•é”™è¯¯å¹¶åˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªAPIå¯†é’¥
                logger.warning(f"ä½¿ç”¨APIå¯†é’¥ #{current_api_index+1} å¤„ç†ç¬¬{i+1}ä¸ªå—æ—¶å‡ºé”™: {str(e)}")
                api_attempts += 1
                
                # æ›´æ–°APIå¯†é’¥ç´¢å¼•ï¼Œå¦‚æœå·²ç»ç”¨å®Œæ‰€æœ‰å¯†é’¥ï¼Œä»å¤´å¼€å§‹
                current_api_index = (current_api_index + 1) % len(api_keys)
                current_api_key = api_keys[current_api_index]
        
        # æ£€æŸ¥æ˜¯å¦æˆåŠŸå¤„ç†
        if not chunk_processed:
            error_msg = f"æ‰€æœ‰APIå¯†é’¥éƒ½æ— æ³•å¤„ç†ç¬¬{i+1}ä¸ªå—"
            logger.error(error_msg)
            raise Exception(error_msg)
        
        # å¤„ç†å®Œä¸€ä¸ªchunkåä¼‘æ¯
        if i < len(chunks) - 1:  # å¦‚æœä¸æ˜¯æœ€åä¸€ä¸ªchunk
            # logger.info(f"ä¼‘æ¯{rest_interval}ç§’åç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªå—")
            time.sleep(rest_interval)
    
    return all_results